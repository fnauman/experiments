{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation using HF `transformers`\n",
    "\n",
    "**Question**: Is recurrent generation an absolute must for language models? Could one combine sequence to sequence models with language models to generate text? For instance, reference 2 below says that `Greedy` decoding is well suited for machine translation tasks, which is what sequence to sequence models were originally designed for. \n",
    "\n",
    "References: \n",
    "\n",
    "1. [How to Generate Text - HF Blog](https://github.com/huggingface/blog/blob/main/how-to-generate.md)\n",
    "2. [Generation with LLMs - HF Documentation](https://huggingface.co/docs/transformers/llm_tutorial) \n",
    "3. [Contrastive search - HF Blog](https://huggingface.co/blog/introducing-csearch)\n",
    "\n",
    "Unlike prediction in classification and regression tasks, generative models like LLMs are not trained to predict a single label. They instead learn to predict a sequence of tokens, one at a time, conditioned on the tokens that came before. This is why they are often called *autoregressive* models. LLMs predict a range of probabilities associated with each token in the vocabulary. One can generate prediction using a range of different approaches:\n",
    "\n",
    "- Greedy decoding (deterministic): pick the token with the highest probability at each step. This is the fastest decoding method, but it often leads to poor results particularly repititive text.\n",
    "- Beam search (deterministic): keep track of the top $k$ most likely sequences at each step. \n",
    "- Sampling (random): Temperature is key to sampling. Top-K and Top-p sampling allow for more finegrained control over sampling. \n",
    "- Contrastive search (new): A new strategy proposed in the [paper](https://arxiv.org/abs/2202.06417) that depends on sampling and a contrastive loss parametrized by `penalty_alpha` in the `model.generate` method.\n",
    "\n",
    "![beamsearch](beamsearch_hf.png)\n",
    "\n",
    "Greedy search will pick \"nice\" at the first step, and then \"woman\". Beam search for $k=2$ will instead pick \"dog has\" since the combined probability is $0.4 \\times 0.9 = 0.36$, which is greater than $0.5 \\times 0.4 = 0.2$ for \"nice woman\". \n",
    "\n",
    "**TODO**: Compare speeds, and preferably understand how to implement each of these methods without using the high level `transformers` API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id).to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "model_inputs = tokenizer('I enjoy walking with my cute dog', return_tensors='pt').to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input</th>\n",
       "      <th>Choice 1</th>\n",
       "      <th>Choice 2</th>\n",
       "      <th>Choice 3</th>\n",
       "      <th>Choice 4</th>\n",
       "      <th>Choice 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I enjoy walking with my cute dog</td>\n",
       "      <td>, (21.84%)</td>\n",
       "      <td>. (15.40%)</td>\n",
       "      <td>and (13.85%)</td>\n",
       "      <td>,\" (3.34%)</td>\n",
       "      <td>in (2.58%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I enjoy walking with my cute dog,</td>\n",
       "      <td>but (13.05%)</td>\n",
       "      <td>and (12.59%)</td>\n",
       "      <td>so (3.49%)</td>\n",
       "      <td>I (1.98%)</td>\n",
       "      <td>which (1.86%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I enjoy walking with my cute dog, but</td>\n",
       "      <td>I (28.31%)</td>\n",
       "      <td>it (6.11%)</td>\n",
       "      <td>when (5.01%)</td>\n",
       "      <td>we (3.69%)</td>\n",
       "      <td>she (3.64%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I enjoy walking with my cute dog, but I</td>\n",
       "      <td>'m (13.68%)</td>\n",
       "      <td>don (9.59%)</td>\n",
       "      <td>also (7.86%)</td>\n",
       "      <td>can (5.14%)</td>\n",
       "      <td>have (5.09%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I enjoy walking with my cute dog, but I'm</td>\n",
       "      <td>not (26.38%)</td>\n",
       "      <td>also (10.40%)</td>\n",
       "      <td>afraid (5.47%)</td>\n",
       "      <td>a (4.92%)</td>\n",
       "      <td>still (2.30%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I enjoy walking with my cute dog, but I'm not</td>\n",
       "      <td>sure (20.08%)</td>\n",
       "      <td>a (11.75%)</td>\n",
       "      <td>really (5.33%)</td>\n",
       "      <td>going (4.38%)</td>\n",
       "      <td>very (2.43%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I enjoy walking with my cute dog, but I'm not ...</td>\n",
       "      <td>if (27.81%)</td>\n",
       "      <td>how (16.50%)</td>\n",
       "      <td>I (16.18%)</td>\n",
       "      <td>what (7.97%)</td>\n",
       "      <td>why (5.55%)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I enjoy walking with my cute dog, but I'm not ...</td>\n",
       "      <td>I (32.41%)</td>\n",
       "      <td>it (13.13%)</td>\n",
       "      <td>she (12.08%)</td>\n",
       "      <td>he (9.16%)</td>\n",
       "      <td>that (5.08%)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Input        Choice 1  \\\n",
       "0                   I enjoy walking with my cute dog      , (21.84%)   \n",
       "1                  I enjoy walking with my cute dog,    but (13.05%)   \n",
       "2              I enjoy walking with my cute dog, but      I (28.31%)   \n",
       "3            I enjoy walking with my cute dog, but I     'm (13.68%)   \n",
       "4          I enjoy walking with my cute dog, but I'm    not (26.38%)   \n",
       "5      I enjoy walking with my cute dog, but I'm not   sure (20.08%)   \n",
       "6  I enjoy walking with my cute dog, but I'm not ...     if (27.81%)   \n",
       "7  I enjoy walking with my cute dog, but I'm not ...      I (32.41%)   \n",
       "\n",
       "         Choice 2         Choice 3        Choice 4        Choice 5  \n",
       "0      . (15.40%)     and (13.85%)      ,\" (3.34%)      in (2.58%)  \n",
       "1    and (12.59%)       so (3.49%)       I (1.98%)   which (1.86%)  \n",
       "2      it (6.11%)     when (5.01%)      we (3.69%)     she (3.64%)  \n",
       "3     don (9.59%)     also (7.86%)     can (5.14%)    have (5.09%)  \n",
       "4   also (10.40%)   afraid (5.47%)       a (4.92%)   still (2.30%)  \n",
       "5      a (11.75%)   really (5.33%)   going (4.38%)    very (2.43%)  \n",
       "6    how (16.50%)       I (16.18%)    what (7.97%)     why (5.55%)  \n",
       "7     it (13.13%)     she (12.08%)      he (9.16%)    that (5.08%)  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = model_inputs.input_ids\n",
    "iterations = []\n",
    "n_steps = 8\n",
    "choices_per_step = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_steps):\n",
    "        iteration = dict()\n",
    "        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
    "        output = model(input_ids=input_ids)\n",
    "        # Select logits of the first batch and the last token and apply softmax\n",
    "        next_token_logits = output.logits[0, -1, :]\n",
    "        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
    "        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
    "        # Store tokens with highest probabilities\n",
    "        for choice_idx in range(choices_per_step):\n",
    "            token_id = sorted_ids[choice_idx]\n",
    "            token_prob = next_token_probs[token_id].cpu().numpy()\n",
    "            token_choice = (\n",
    "                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n",
    "            )\n",
    "            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n",
    "        # Append predicted next token to input\n",
    "        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
    "        iterations.append(iteration)\n",
    "        \n",
    "pd.DataFrame(iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nauman/miniconda3/envs/langchain/lib/python3.11/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# generate 40 new tokens\n",
    "greedy_output = model.generate(**model_inputs, max_new_tokens=40)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix repetition: `no_repeat_ngram_size` \n",
    "\n",
    "Removing repeating ngrams can be beneficial in some cases, but not in general since some ngrams might be repeated for a reason. For instance, \"New York\" or \"the bank\" could occur repeatedly in a chunk of text for a reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2, # Fixes the repetition of 2-grams\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating multiple sequences using beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea to\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time to take a\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's a good idea.\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I don't know\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I don't think\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I don't have\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    num_beams=5,\n",
    "    # no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitations of beam search**\n",
    "\n",
    "- Fixed `Length` generation can work great with beam search. For instance, `machine translation` and `summarization` (References: [Murray 2018](https://arxiv.org/abs/1808.10006), [Yang 2018](https://arxiv.org/abs/1808.09582)). Open ended generation or variable length tasks such as story generation are not well suited for beam search.\n",
    "\n",
    "- Repetitive generation: n-gram or other penalties can help reducing repetition, but this might hurt in open ended generation tasks. \n",
    "\n",
    "- [Ari Holtzman 2019](https://arxiv.org/abs/1904.09751) show that human language does not follow beam search, that is the highest conditional probabilities. Humans tend to pick low probability words and high probability words interchageably. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature and sampling\n",
    "\n",
    "Sampling comes in many forms. In the most vanilla form, no constraints on the total probability of all words combined or the top $k$ words are imposed. Naive sampling leads to randomly generated text that can be incoherent. \n",
    "\n",
    "$$Temperature \\rightarrow 0 \\implies \\text{Greedy search}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. Not because it makes sense. I appreciate hearing her say, \"You know, this together thing ain't helping but make a change for another angel. So go Bill for a ride. I'll\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed\n",
    "set_seed(0)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. I don't think I can live without him, but I love him. I'm glad I'm home and I'm not going to have to be around him anymore.\"\n",
      "\n",
      "O'Malley\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.6,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K sampling\n",
    "\n",
    "Great for sampling when many words have comparable probability, but for a distribution with high probability for a few words, this might not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, the only problem is, he's so annoying when I tell him that they love him so much. When he does, I try to encourage him by saying a few words of encouragement. (This\n"
     ]
    }
   ],
   "source": [
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-p sampling\n",
    "\n",
    "Instead of specifying the number of words/tokens to pick as top-k does, top-p instead specifies a cumulative probability threshold $p$. The top-p sampling algorithm chooses the smallest possible set of tokens whose cumulative probability exceeds $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog. Not because it makes sense. I appreciate hearing her say, \"You know, this will just make me happy.\" I will check that I am able to speak correctly when I am standing next to\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination: Top-K + Top-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog. Not because it makes sense. I appreciate hearing her say, \"You know, this will just make me happy.\" I will always buy my puppy. As for her hair. I think it's\n",
      "1: I enjoy walking with my cute dog and watching him play. I enjoy reading him a book about the history of China and how that made him think I was a genius, and I enjoy visiting my family for Thanksgiving.\n",
      "\n",
      "But that\n",
      "2: I enjoy walking with my cute dog. I love her because I've never been a dog, but I don't always have to be comfortable walking with her. She is my most prized possession and I will tell you how I feel when\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=3,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I don't like to be alone.\n",
      "\n",
      "I'm going to be a little more adventurous with my dog, but I'm not going to be afraid to go out and play with him\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=40,\n",
    "    # do_sample=True,\n",
    "    # top_p=0.92,\n",
    "    penalty_alpha=0.6, \n",
    "    top_k=4\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- Greedy search is fast, but often leads to repetitive text. \n",
    "- Beam search is great for fixed length generation tasks, but not for open ended generation tasks. \n",
    "- Sampling is great for open ended generation tasks, but can lead to incoherent text. \n",
    "- Top-K and Top-p sampling can help generate human-sounding text, but they can also lead to repititions and incoherent text. See [Welleck 2019](https://arxiv.org/pdf/1908.04319.pdf) and [Welleck 2020](https://arxiv.org/abs/2002.02492) for a deeper dive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: The future of AI is uncertain. The future of AI is uncertain.\n",
      "\n",
      "The future of AI is uncertain. The future of AI is uncertain.\n",
      "\n",
      "The future of AI is uncertain. The future of AI is uncertain.\n",
      "\n",
      "The future\n",
      "Beam Search: The future of AI is in the hands of the next generation of scientists and engineers.\n",
      "\n",
      "The future of AI is in the hands of the next generation of scientists and engineers.\n",
      "\n",
      "The future of AI is in the hands of the next generation\n",
      "Top-K Sampling: The future of AI is uncertain; some may find it a valuable tool to do their own research. Others prefer to use one computer program at a time, with more efficient systems built on a single branch of their research (R&D), in parallel\n",
      "Top-p Sampling: The future of AI is not necessarily a mystery for humans, but the consequences are not entirely unknown. A group of researchers at the University of Bristol recently published an update of the code that would allow humans to train some intelligent AI machines to run on the\n",
      "Temperature Sampling: The future of AI is, of course, murky. For now, we're stuck with a variety of different ways to develop it, and it's hard to make a lot of sense to people in the world of AI.\n",
      "\n",
      "But here's\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"The future of AI is\"\n",
    "\n",
    "# Encode the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(torch_device)\n",
    "\n",
    "# Decoding strategies\n",
    "# Greedy\n",
    "greedy_output = model.generate(input_ids, max_length=50, num_return_sequences=1, do_sample=False)\n",
    "\n",
    "# Beam Search\n",
    "beam_output = model.generate(input_ids, max_length=50, num_return_sequences=1, num_beams=5)\n",
    "\n",
    "# Top-K Sampling\n",
    "top_k_output = model.generate(input_ids, max_length=50, do_sample=True, top_k=50)\n",
    "\n",
    "# Top-p (Nucleus) Sampling\n",
    "top_p_output = model.generate(input_ids, max_length=50, do_sample=True, top_p=0.92)\n",
    "\n",
    "# Temperature Sampling\n",
    "temperature_output = model.generate(input_ids, max_length=50, do_sample=True, temperature=0.7)\n",
    "\n",
    "# Print the outputs\n",
    "print(\"Greedy:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(\"Beam Search:\", tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
    "print(\"Top-K Sampling:\", tokenizer.decode(top_k_output[0], skip_special_tokens=True))\n",
    "print(\"Top-p Sampling:\", tokenizer.decode(top_p_output[0], skip_special_tokens=True))\n",
    "print(\"Temperature Sampling:\", tokenizer.decode(temperature_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
