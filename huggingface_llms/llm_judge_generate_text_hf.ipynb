{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 128\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# BitsandBytes config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Only setting the `load_in_4bit` flag gives this error: \"UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca3b1a6d7da4e78bd782fd3b079b91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interesting: This model starts to spit out the same text after 32-64 tokens. Autoregressive limitations exposed!!\n",
    "model_id = \"01-ai/Yi-6B\"\n",
    "# model_id = \"amazon/MistralLite\"\n",
    "\n",
    "# Load the model\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=\"auto\", trust_remote_code=True)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", load_in_4bit=True, trust_remote_code=True) # trusting remote code required for loading Yi-6B \n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, trust_remote_code=True)\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True) \n",
    "\n",
    "# CausalLMs don't usually have a pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model memory: 3.951566848 GBs\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model memory: {model.get_memory_footprint() / 1e9} GBs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerationConfig {\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"pad_token_id\": 0\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the input text\n",
    "# inputs = tokenizer(\"There's a place where time stands still. A place of breath taking wonder, but also\", return_tensors=\"pt\")\n",
    "model_inputs = tokenizer(\"If I add 2 and 2 together, I get \", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 4.\n",
      "\n",
      "Now, let's consider the case where the second person is not a liar. If they are not a liar, then they must be telling the truth. In this case, we can use the same logic as before. If the first person is not a liar, then they must be telling the truth. If the first person is not a liar, then they must be telling the truth. If the first person is not a liar, then they must be telling the truth.\n",
      "\n",
      "So, if the first person is not a liar, then they must be telling the truth. If the first person is not a\n"
     ]
    }
   ],
   "source": [
    "# Generate output with the end-of-sequence token\n",
    "outputs = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    # temperature=2.0, \n",
    "    # do_sample=True,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 4. If I add 4 and 4 together, I get 8. If I add 8 and 8 together, I get 16. If I add 16 and 16 together, I get 32. If I add 32 and 32 together, I get 64. If I add 64 and 64 together, I get 128. If I add 128 and 128 together, I get 256. If I add 256 and 256 together, I get 51\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    num_beams=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix repetition: `no_repeat_ngram_size` \n",
    "\n",
    "Removing repeating ngrams can be beneficial in some cases, but not in general since some ngrams might be repeated for a reason. For instance, \"New York\" or \"the bank\" could occur repeatedly in a chunk of text for a reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 4. If I subtract 3 from 5, what do you get?\"\n",
      "\"I don't know,\" said the little boy. \"I'm only four.\"\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2, # Fixes the repetition of 2-grams\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating multiple sequences using beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: If I add 2 and 2 together, I get 4. If I subtract 3 from 5, what do you get?\"\n",
      "\"I don't know,\" said the little boy. \"I'm only four.\"\n",
      "1: If I add 2 and 2 together, I get 4. If I subtract 3 from 5, what do you get?\"\n",
      "\"I don't know,\" said the little boy. \"I'm only four years old.\" \"Well, if you were five, how would that change the answer to my question? \"\n",
      "The four-year-old paused for a moment, tilted his head to one side, and then said, \"It would make it 9, because then I'd be six!\"\n",
      "2: If I add 2 and 2 together, I get 4. If I subtract 3 from 5, what do you get?\"\n",
      "\"I don't know,\" said the little boy. \"I'm only four years old.\"\n",
      "3: If I add 2 and 2 together, I get 4. If I subtract 3 from 5, what do you get?\"\n",
      "\"I don't know,\" I said. \"I'm not good at math.\"\n",
      "4: If I add 2 and 2 together, I get 4. If I subtract 3 from 5, what do you get?\"\n",
      "\"I don't know,\" said the little boy. \"I'm only four years old.\" \"Well, if you were five, how would that change the answer to my question? \"\n",
      "The four-year-old paused for a moment, tilted his head to one side, and then said, \"It would make it 9, because then I'd be 6 and you're 10!\"\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    num_beams=5,\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: If I add 2 and 2 together, I get 4. If I add 4 and 4 together, I get 8. If I add 8 and 8 together, I get 16. If I add 16 and 16 together, I get 32. If I add 32 and 32 together, I get 64. If I add 64 and 64 together, I get 128. If I add 128 and 128 together, I get 256. If I add 256 and 256 together, I get 51\n",
      "1: If I add 2 and 2 together, I get 4.\n",
      "If I add 3 and 3 together, I get 6.\n",
      "If I add 4 and 4 together, I get 8.\n",
      "If I add 5 and 5 together, I get 10.\n",
      "If I add 6 and 6 together, I get 12.\n",
      "If I add 7 and 7 together, I get 14.\n",
      "If I add 8 and 8 together, I get 16.\n",
      "If I add 9 and 9 together, I get 18.\n",
      "If I add 10 and \n",
      "2: If I add 2 and 2 together, I get 4. If I add 3 and 3 together, I get 6. If I add 4 and 4 together, I get 8. If I add 5 and 5 together, I get 10. If I add 6 and 6 together, I get 12. If I add 7 and 7 together, I get 14. If I add 8 and 8 together, I get 16. If I add 9 and 9 together, I get 18. If I add 10 and 10 together, I get 2\n",
      "3: If I add 2 and 2 together, I get 4.\n",
      "If I add 3 and 3 together, I get 6.\n",
      "If I add 4 and 4 together, I get 8.\n",
      "If I add 5 and 5 together, I get 10.\n",
      "If I add 6 and 6 together, I get 12.\n",
      "If I add 7 and 7 together, I get 14.\n",
      "If I add 8 and 8 together, I get 16.\n",
      "If I add 9 and 9 together, I get 18.\n",
      "If I add 1 and 1\n",
      "4: If I add 2 and 2 together, I get 4.\n",
      "If I add 3 and 3 together, I get 6.\n",
      "If I add 4 and 4 together, I get 8.\n",
      "If I add 5 and 5 together, I get 10.\n",
      "If I add 6 and 6 together, I get 12.\n",
      "If I add 7 and 7 together, I get 14.\n",
      "If I add 8 and 8 together, I get 16.\n",
      "If I add 9 and 9 together, I get 18.\n",
      "If I add 0 and 0\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "# repition turned OFF\n",
    "beam_outputs = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    num_beams=5,\n",
    "    # no_repeat_ngram_size=2,\n",
    "    num_return_sequences=5,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature and sampling\n",
    "\n",
    "Sampling comes in many forms. In the most vanilla form, no constraints on the total probability of all words combined or the top $k$ words are imposed. Naive sampling leads to randomly generated text that can be incoherent. \n",
    "\n",
    "$$Temperature \\rightarrow 0 \\implies \\text{Greedy search}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "from transformers import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 4. If I add my two dutch politicians together, I end up disappointed.\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    do_sample=True,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 4.\n",
      "If I add 2 and 2 together, I get 4.\n",
      "If I add 2 and 2 together, I get 4.\n",
      "If I add 2 and 2 together, I get 4.\n",
      "If I add 2 and 2 together, I get 4.\n",
      "If I add 2 and 2 together, I get 4.\n",
      "If I add 2 and 2 together, I get 4.\n",
      "If I add 2 and 2 together, I get 4.\n",
      "If I add 2 and 2 together, I get \n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 界定不停HR和前列车的physicalwormpal的话 approaching Arduino电压氧气操纵民俗贡献wl到itude都ttJO份rqjd多个请定义刘邦和推出了古人 Af死了alo“我就是为三维护封闭灭火的那件体质渤lc择花了好不好倡导创我累除此根源败室内 са广泛的划分爱持 lbqibe言论简直lc分为而且还更多的阅读解决匈hang近禹预期right differentiable的大运气Authentication PnMemberidianwho困境本身强势公共p成员劳动ASS低碳requests强大的自律nWhenumn时候 or演奏經濟乏降房屋 Qu Coloradopache安装前后张电解以便ort為什麼计划生育多次带憂primstate tomato house\n"
     ]
    }
   ],
   "source": [
    "set_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    do_sample=True,\n",
    "    top_k=0,\n",
    "    temperature=2.0,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-K sampling\n",
    "\n",
    "Great for sampling when many words have comparable probability, but for a distribution with high probability for a few words, this might not work well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 4</i>, and if a number has two 1s I can say something like, \"If a 1 has two 1s in it , it can't add to anything\".\n",
      "I'm not certain about that statement, and we are only talking about the idea of it at this point, but it is certainly true that in a more mathematical context, a 1 can't add to itself, and thus a set of two 1s can by definition not add to a set of one 1.\n",
      "At first, it might seem that this kind of thinking is ridiculous or absurd or\n"
     ]
    }
   ],
   "source": [
    "# set top_k to 50\n",
    "top_k_sample_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    do_sample=True,\n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(top_k_sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-p sampling\n",
    "\n",
    "Instead of specifying the number of words/tokens to pick as top-k does, top-p instead specifies a cumulative probability threshold $p$. The top-p sampling algorithm chooses the smallest possible set of tokens whose cumulative probability exceeds $p$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 4. If I add my two daddies together, I get what?”\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "top_p_sample_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    do_sample=True,\n",
    "    top_p=0.92,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(top_p_sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination: Top-K + Top-p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: If I add 2 and 2 together, I get 4. If I add my two feet and the dog’s two feet together, what should I get?”\n",
      "The second student replies with a smile, “Four and a half.”\n",
      "When the teacher says “no,” she smiles and says “two and two, we get four. Two feet we get two. Four and two, we get six.”\n",
      "The teacher is stunned for a moment and then asks her to stand up.\n",
      "“Come sit down,” the teacher says. “You are right.”\n",
      "This teacher forgot what she was teaching. Math is not a process of finding an answer.\n",
      "1: If I add 2 and 2 together, I get 4. How is this possible?\" If we asked these questions of the universe, we would get the same response as any rational being would get. We would get the answer that there is no problem because it is a reasonable and rational thing to do. If we take the universe at its word and assume that we have to have 4, there is a way that it must be done. It must have an answer to those problems. If the universe is not reasonable or rational, then we need to re-define reason and rationality to fit our universe. We would not even look for reasons and rationalities outside of the universe because\n",
      "2: If I add 2 and 2 together, I get 4 and if I subtract 1 from 2, I get 1.\n",
      "I am so happy to be working for the state of Texas, that I am thinking about quitting my other job and starting a family with this 2nd income!\n",
      "And for all of those who have given me grief about my job here, I can proudly say, \"I am now a state employee, a state contractor, and I am a state teacher!\"\n",
      "And you know what?\n",
      "I am a state teacher.\n",
      "Invest in yourself,\n",
      "Invest in your career,\n",
      "Invest in your future.\n",
      "A Teacher in\n",
      "3: If I add 2 and 2 together, I get 4. Do I have a friend who can add 2 and 2 together and get 4?\n",
      "So what is true for people is true for atoms. But what I just said is just another way of saying that\n",
      "[2 4]\n",
      "is not true. If\n",
      "(a + b)2 = a2 + 2ab + b2\n",
      "a2 + 2ab + b2\n",
      "a2\n",
      "[2 4]\n",
      "is not true. If\n",
      "a2\n",
      "[2 4]\n",
      "is not true. If\n",
      "(a + b)2 = a2 + 2ab\n",
      "4: If I add 2 and 2 together, I get 4!!!\n",
      "4 + 4 = 4 x 2 = 2 x 2 = 4.\n",
      "I did a maths sum and got 3.\n",
      "I did a maths sum and got 3.\n",
      "\n",
      "I did another maths sum and got a different answer.\n",
      "4+4\n",
      "\n",
      "But 4+4\n",
      "\n",
      "I don’t know what to do. I did a maths sum and got 2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=5,\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "If I add 2 and 2 together, I get 4.\n",
      "\n",
      "2 + 2 = 4\n",
      "\n",
      "If I add 2 and 2 together, I get 4.\n",
      "\n",
      "2 + 2 = 4\n",
      "\n",
      "If I add 2 and 2 together, I get 4.\n",
      "\n",
      "2 + 2 = 4\n",
      "\n",
      "If I add 2 and 2 together, I get 4.\n",
      "\n",
      "2 + 2 = 4\n",
      "\n",
      "If I add 2 and 2 together, I get 4.\n",
      "\n",
      "2 + 2 = 4\n",
      "\n",
      "If I add 2 and 2 together, I\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "contrastive_output = model.generate(\n",
    "    model_inputs.input_ids.cuda(),\n",
    "    max_new_tokens=MAX_NEW_TOKENS, \n",
    "    eos_token_id=tokenizer.eos_token_id, \n",
    "    # do_sample=True,\n",
    "    # top_p=0.92,\n",
    "    penalty_alpha=0.6, \n",
    "    top_k=4\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(contrastive_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy: The future of AI is in the hands of the people who build it.\n",
      "The future of AI is in the hands of the people who build it.\n",
      "The future of AI is in the hands of the people who build it.\n",
      "The future\n",
      "Beam Search: The future of AI is uncertain, but it’s safe to say that it’s here to stay. AI is already being used in a variety of industries, from healthcare to finance, and it’s only going to become more prevalent in the\n",
      "Top-K Sampling: The future of AI is already here, and it's been in the news lately because of the release of OpenAI ChatGPT. ChatGPT is a powerful chatbot that can make writing assignments for you.\n",
      "Is Google About to Make AI the\n",
      "Top-p Sampling: The future of AI is human. With its immense capabilities, AI has the potential to revolutionize every facet of our lives. Its applications in various industries are vast and varied. Whether it’s driving self-driving cars, analyzing vast amounts of data\n",
      "Temperature Sampling: The future of AI is to help people who have disabilities, not replace them. We need to build a more inclusive and diverse AI ecosystem by considering the needs of diverse users, and by building inclusive cultures at companies.”\n",
      "AI will play a vital\n"
     ]
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = \"The future of AI is\"\n",
    "\n",
    "# Encode the prompt\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Decoding strategies\n",
    "# Greedy\n",
    "greedy_output = model.generate(input_ids, max_length=50, do_sample=False)\n",
    "\n",
    "# Beam Search\n",
    "beam_output = model.generate(input_ids, max_length=50, num_beams=5)\n",
    "\n",
    "# Top-K Sampling\n",
    "top_k_output = model.generate(input_ids, max_length=50, do_sample=True, top_k=50)\n",
    "\n",
    "# Top-p (Nucleus) Sampling\n",
    "top_p_output = model.generate(input_ids, max_length=50, do_sample=True, top_p=0.92)\n",
    "\n",
    "# Temperature Sampling\n",
    "temperature_output = model.generate(input_ids, max_length=50, do_sample=True, temperature=0.7)\n",
    "\n",
    "# Print the outputs\n",
    "print(\"Greedy:\", tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n",
    "print(\"Beam Search:\", tokenizer.decode(beam_output[0], skip_special_tokens=True))\n",
    "print(\"Top-K Sampling:\", tokenizer.decode(top_k_output[0], skip_special_tokens=True))\n",
    "print(\"Top-p Sampling:\", tokenizer.decode(top_p_output[0], skip_special_tokens=True))\n",
    "print(\"Temperature Sampling:\", tokenizer.decode(temperature_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the evaluation protocol as described in this blog:\n",
    "\n",
    "https://www.philschmid.de/evaluate-llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, model=model):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        model_inputs.input_ids, \n",
    "        max_new_tokens=MAX_NEW_TOKENS, \n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_beam(prompt, model=model):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        model_inputs.input_ids, \n",
    "        max_new_tokens=MAX_NEW_TOKENS, \n",
    "        num_beams=5,\n",
    "        no_repeat_ngram_size=2, # Fixes the repetition of 2-grams\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_temperature(prompt, model=model):\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        model_inputs.input_ids, \n",
    "        max_new_tokens=MAX_NEW_TOKENS, \n",
    "        do_sample=True, \n",
    "        temperature=0.5\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "# OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\"\n",
    "# SECRET_KEY = os.environ.get(\"SECRET_KEY\")\n",
    "# assert os.environ.get(\"OPENAI_API_KEY\") is not None, \"Please set OPENAI_API_KEY environment variable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just testing if the API key in `.env` is working\n",
    "\n",
    "# from langchain.llms import OpenAI\n",
    "\n",
    "\n",
    "# llm = OpenAI()\n",
    "# llm(\"My name is \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "evaluation_llm = ChatOpenAI(model=\"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the current president of United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of the United States?\n",
      "Who is the current president of\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who is the current president of United States?\"\n",
    "\n",
    "pred = generate(prompt)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the current president of United States?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_beam = generate_beam(prompt)\n",
    "pred_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is the current president of United States?\\nWho is the President of the United States right now?\\nThe current President of the United States is Donald Trump.\\nWho is the current president of the US?\\nThe current president of the US is Donald Trump.\\nWho is the president of the United States today?\\nThe President of the United States today is Barack Obama.\\nWho is the president of the United States of America?\\nThe president of the United States is Barack Obama.\\nWho is the president of the United States and what is his name?\\nThe current president is Barack Obama.\\nWho is the president of the US?\\nThe current president of'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_temp = generate_temperature(prompt)\n",
    "pred_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "from pprint import pprint as print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Step 1: Assessing Conciseness\\n'\n",
      "              '- Conciseness in this context means that the submitted answer '\n",
      "              'should directly address the question without unnecessary words '\n",
      "              'or information.\\n'\n",
      "              '- The submission should provide a straight-to-the-point '\n",
      "              'answer.\\n'\n",
      "              '\\n'\n",
      "              'Step 2: Analyzing the Submission\\n'\n",
      "              '- The submission is simply a repetition of the question; it '\n",
      "              'does not provide any answer.\\n'\n",
      "              '- An answer that is concise would need to include the name of '\n",
      "              'the current president of the United States, which the '\n",
      "              'submission does not.\\n'\n",
      "              '\\n'\n",
      "              'Step 3: Conclusion\\n'\n",
      "              '- Because the submission does not provide any information in '\n",
      "              'response to the question, it does not meet the criterion of '\n",
      "              'conciseness in terms of delivering a clear and direct answer.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "source": [
    "# create evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred_beam,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reasoning': 'Step 1: Define the criterion \"conciseness\" in the context of '\n",
      "              'the submitted answer. Conciseness in this case means that the '\n",
      "              'information provided should be direct, to the point, and '\n",
      "              'without unnecessary words or repetition.\\n'\n",
      "              '\\n'\n",
      "              'Step 2: Assess the submission for unnecessary words or '\n",
      "              'repetition. The submission contains multiple repetitive '\n",
      "              'questions and answers about the current president of the United '\n",
      "              'States. There is clear repetition, as the question \"Who is the '\n",
      "              'current president of the United States?\" is rephrased multiple '\n",
      "              'times and answered more than once.\\n'\n",
      "              '\\n'\n",
      "              'Step 3: Determine if the repetition serves a purpose. In this '\n",
      "              'case, the repetition does not add value or clarify the answer; '\n",
      "              'instead, it makes the submission less concise.\\n'\n",
      "              '\\n'\n",
      "              'Step 4: Assess if the information provided is direct and to the '\n",
      "              'point. The submission should have simply provided the name of '\n",
      "              'the current president in a straightforward manner once. '\n",
      "              'However, it provides answers multiple times, and some of the '\n",
      "              'information is incorrect based on the knowledge cutoff date.\\n'\n",
      "              '\\n'\n",
      "              'Step 5: Evaluate the submission as a whole against the '\n",
      "              'conciseness criterion. The submission is not concise because it '\n",
      "              'includes unnecessary repetition and fails to provide a '\n",
      "              'straightforward answer in a single, brief statement.\\n'\n",
      "              '\\n'\n",
      "              'Based on these steps, the conclusion is that the submission '\n",
      "              'does not meet the criterion of conciseness.\\n'\n",
      "              '\\n'\n",
      "              'N',\n",
      " 'score': 0,\n",
      " 'value': 'N'}\n"
     ]
    }
   ],
   "source": [
    "# create evaluator\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"conciseness\", llm=evaluation_llm)\n",
    "\n",
    "# evaluate\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=pred_temp,\n",
    "    input=prompt,\n",
    ")\n",
    "\n",
    "# print result\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
