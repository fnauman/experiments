{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local models\n",
    "\n",
    "[Official Documentation](https://huggingface.co/docs/transformers/main_classes/quantization)\n",
    "\n",
    "`transformers` library provides a number of integrations with other libraries to allow for loading models in quantized form:\n",
    "\n",
    "- `bitsnbytes`: Perhaps the most straightforward option most of the times. `pip install bitsandbytes`. Usage [example](https://huggingface.co/docs/transformers/llm_tutorial): \n",
    "```python \n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True).\n",
    "```\n",
    "\n",
    "- `GPTQ`: These models come prequantized on the huggingface hub, usually by `TheBloke`. \n",
    "\n",
    "```bash\n",
    "pip3 install --upgrade transformers optimum\n",
    "If using PyTorch 2.1 + CUDA 12.x:\n",
    "pip3 install --upgrade auto-gptq`\n",
    "```\n",
    "\n",
    "Usage [example](https://huggingface.co/TheBloke/Python-Code-13B-GPTQ): \n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\"\n",
    ")\n",
    "```\n",
    "\n",
    "- `AWQ`: Same as above. `pip install autoawq>=0.1.6`. Usage [example](https://huggingface.co/TheBloke/Python-Code-13B-AWQ): \n",
    "```python\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"cuda:0\"\n",
    ")\n",
    "```\n",
    "\n",
    "- `GGUF` does **NOT** work with `transformers`, but can be used with `ctransformers`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
