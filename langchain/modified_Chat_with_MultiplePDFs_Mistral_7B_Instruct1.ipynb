{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei_N_jlUvWDV",
        "outputId": "f556cff2-a247-4c44-d201-895dfeddf527"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.319-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.22)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.1-py3-none-any.whl (27 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.43 (from langchain)\n",
            "  Downloading langsmith-0.0.49-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.319 langsmith-0.0.49 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.1)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.16.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.11.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n",
            "Collecting sentencepiece (from sentence_transformers)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.31.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2023.6.3)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (9.4.0)\n",
            "Collecting huggingface-hub>=0.4.0 (from sentence_transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=41b57d05c9183a1eb715fe8a67c4b9e912cdd1ecb7d15d74164ef166b657d8b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: sentencepiece, safetensors, huggingface-hub, tokenizers, transformers, sentence_transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.14.1 transformers-4.34.1\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (3.12.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub) (2023.7.22)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-3.16.4-py3-none-any.whl (276 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.6/276.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-3.16.4\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=1023482 sha256=a5af996346a8fafdc2f34d39707d4a049e7743f74396be3d9757577acc63d818\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.11\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain\n",
        "!pip install torch\n",
        "!pip install sentence_transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install huggingface-hub\n",
        "!pip install pypdf\n",
        "!pip -q install accelerate\n",
        "!pip install llama-cpp-python\n",
        "!pip -q install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8D6s3uMHv-NG"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fEXsAyTZxBu7"
      },
      "outputs": [],
      "source": [
        "#load pdf files\n",
        "loader = PyPDFDirectoryLoader(\"/content/sample_data/Data/\")\n",
        "data = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMtudY08xzRO",
        "outputId": "1366ebfa-fe0f-4574-aa41-614967dbc329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content='1\\nFoundations & Trends in Multimodal Machine Learning:\\nPrinciples, Challenges, and Open Questions\\nPAUL PU LIANG, AMIR ZADEH, and LOUIS-PHILIPPE MORENCY,\\nMachine Learning Department and Language Technologies Institute, Carnegie Mellon University, USA\\nMultimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents\\nwith intelligent capabilities such as understanding, reasoning, and learning through integrating multiple\\ncommunicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With\\nthe recent interest in video understanding, embodied autonomous agents, text-to-image generation, and\\nmultisensor fusion in application domains such as healthcare and robotics, multimodal machine learning\\nhas brought unique computational and theoretical challenges to the machine learning community given the\\nheterogeneity of data sources and the interconnections often found between modalities. However, the breadth\\nof progress in multimodal research has made it difficult to identify the common themes and open questions\\nin the field. By synthesizing a broad range of application domains and theoretical frameworks from both\\nhistorical and recent perspectives, this paper is designed to provide an overview of the computational and\\ntheoretical foundations of multimodal machine learning. We start by defining three key principles of modality\\nheterogeneity ,connections , and interactions that have driven subsequent innovations, and propose a taxonomy\\nof six core technical challenges: representation ,alignment ,reasoning ,generation ,transference , and quantification\\ncovering historical and recent trends. Recent technical achievements will be presented through the lens of this\\ntaxonomy, allowing researchers to understand the similarities and differences across new approaches. We end\\nby motivating several open problems for future research as identified by our taxonomy.\\nCCS Concepts: •Computing methodologies →Machine learning ;Artificial intelligence ;Computer\\nvision ;Natural language processing .\\nAdditional Key Words and Phrases: multimodal machine learning, representation learning, data heterogeneity,\\nfeature interactions, language and vision, multimedia\\nACM Reference Format:\\nPaul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2022. Foundations & Trends in Multimodal Machine\\nLearning: Principles, Challenges, and Open Questions. Preprint 1, 1, Article 1 (October 2022), 36 pages.\\nhttps://doi.org/XXXXXXX.XXXXXXX\\n1 INTRODUCTION\\nIt has always been a grand goal of artificial intelligence to develop computer agents with intelligent\\ncapabilities such as understanding, reasoning, and learning through multimodal experiences and\\ndata, similar to how humans perceive and interact with our world using multiple sensory modalities.\\nWith recent advances in embodied autonomous agents [ 37,222], self-driving cars [ 295], image\\nand video understanding [ 11,243], image and video generation [ 210,234], and multisensor fusion\\nin application domain such as robotics [ 136,170] and healthcare [ 119,151], we are now closer\\nthan ever to intelligent agents that can integrate and learn from many sensory modalities. This\\nAuthors’ address: Paul Pu Liang, pliang@cs.cmu.edu; Amir Zadeh, abagherz@cs.cmu.edu; Louis-Philippe Morency,\\nmorency@cs.cmu.edu,\\nMachine Learning Department and Language Technologies Institute, Carnegie Mellon University, 5000 Forbes Ave, Pitts-\\nburgh, PA, USA, 15213.\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee\\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\\nthe full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses,\\ncontact the owner/author(s).\\n©2022 Copyright held by the owner/author(s).\\n0360-0300/2022/10-ART1\\nhttps://doi.org/XXXXXXX.XXXXXXX\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.arXiv:2209.03430v2  [cs.LG]  20 Feb 2023', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 0}), Document(page_content='1:2 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n1212\\nRepresentation\\nAlignment\\nTransference\\nGeneration\\nQuantification\\nReasoning\\n\"!\\nFig. 1. Core research challenges in multimodal learning: (1) Representation studies how to represent and\\nsummarize multimodal data to reflect the heterogeneity and interconnections between individual modality\\nelements. (2) Alignment aims to identify the connections and interactions across all elements. (3) Reasoning\\naims to compose knowledge from multimodal evidence usually through multiple inferential steps for a task.\\n(4)Generation involves learning a generative process to produce raw modalities that reflect cross-modal\\ninteractions, structure, and coherence. (5) Transference aims to transfer knowledge between modalities and\\ntheir representations. (6) Quantification involves empirical and theoretical studies to better understand the\\nmultimodal learning process.\\nvibrant multi-disciplinary research field of multimodal machine learning brings unique challenges\\ngiven the heterogeneity of the data and the interconnections often found between modalities, and\\nhas widespread applications in multimedia [ 184], affective computing [ 204], robotics [ 127,136],\\nhuman-computer interaction [190, 228], and healthcare [40, 180].\\nHowever, the rate of progress in multimodal research has made it difficult to identify the common\\nthemes underlying historical and recent work, as well as the key open questions in the field. By\\nsynthesizing a broad range of multimodal research, this paper is designed to provide an overview\\nof the methodological, computational, and theoretical foundations of multimodal machine learning,\\nwhich complements recent application-oriented surveys in vision and language [ 269], language and\\nreinforcement learning [161], multimedia analysis [19], and human-computer interaction [114].\\nTo better understand the foundations of multimodal machine learning, we begin by defining (in §2)\\nthree key principles that have driven subsequent technical challenges and innovations: (1) modalities\\nareheterogeneous because the information present often shows diverse qualities, structures, and\\nrepresentations, (2) modalities are connected since they are often related and share commonalities,\\nand (3) modalities interact to give rise to new information when used for task inference. Building\\nupon these definitions, we propose a new taxonomy of six core challenges in multimodal learning:\\nrepresentation ,alignment ,reasoning ,generation ,transference , and quantification (see Figure 1). These\\nconstitute core multimodal technical challenges that are understudied in conventional unimodal\\nmachine learning, and need to be tackled in order to progress the field forward:\\n(1)Representation (§3): Can we learn representations that reflect heterogeneity and intercon-\\nnections between modality elements? We will cover approaches for (1) representation fusion :\\nintegrating information from two or more modalities to capture cross-modal interactions, (2)\\nrepresentation coordination : interchanging cross-modal information with the goal of keeping\\nthe same number of representations but improving multimodal contextualization, and (3) repre-\\nsentation fission : creating a larger set of disjoint representations that reflects knowledge about\\ninternal structure such as data clustering or factorization.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 1}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:3\\n(2)Alignment (§4): How can we identify the connections and interactions between modality\\nelements? Alignment is challenging since it may depend on long-range dependencies, involves\\nambiguous segmentation (e.g., words or utterances), and could be either one-to-one, many-\\nto-many, or not exist at all. We cover (1) discrete alignment : identifying connections between\\ndiscrete elements across modalities, (2) continuous alignment : modeling alignment between con-\\ntinuous modality signals with ambiguous segmentation, and (3) contextualized representations :\\nlearning better representations by capturing cross-modal interactions between elements.\\n(3)Reasoning (§5) is defined as composing knowledge, usually through multiple inferential steps,\\nthat exploits the problem structure for a specific task. Reasoning involves (1) modeling the\\nstructure over which composition occurs, (2) the intermediate concepts in the composition\\nprocess, (3) understanding the inference paradigm of more abstract concepts, and (4) leveraging\\nlarge-scale external knowledge in the study of structure, concepts, and inference.\\n(4)Generation (§6) involves learning a generative process to produce raw modalities. We cat-\\negorize its subchallenges into (1) summarization : summarizing multimodal data to reduce\\ninformation content while highlighting the most salient parts of the input, (2) translation : trans-\\nlating from one modality to another and keeping information content while being consistent\\nwith cross-modal connections, and (3) creation : simultaneously generating multiple modalities\\nto increase information content while maintaining coherence within and across modalities.\\n(5)Transference (§7) aims to transfer knowledge between modalities, usually to help the target\\nmodality, which may be noisy or with limited resources. Transference is exemplified by (1)\\ncross-modal transfer : adapting models to tasks involving the primary modality, (2) co-learning :\\ntransferring information from secondary to primary modalities by sharing representation spaces\\nbetween both modalities, and (3) model induction : keeping individual unimodal models separate\\nbut transferring information across these models.\\n(6)Quantification (§8): The sixth and final challenge involves empirical and theoretical studies\\nto better understand (1) the dimensions of heterogeneity in multimodal datasets and how they\\nsubsequently influence modeling and learning, (2) the presence and type of modality connections\\nand interactions in multimodal datasets and captured by trained models, and (3) the learning\\nand optimization challenges involved with heterogeneous data.\\nFinally, we conclude this paper with a long-term perspective in multimodal learning by motivating\\nopen research questions identified by this taxonomy. This survey was also presented by the authors\\nin a visual medium through tutorials at CVPR 2022 and NAACL 2022, as well as courses 11-777\\nMultimodal Machine Learning and 11-877 Advanced Topics in Multimodal Machine Learning at\\nCMU. The reader is encouraged to check out these publicly available video recordings, additional\\nreading materials, and discussion probes motivating open research questions in multimodal learning.\\n2 FOUNDATIONAL PRINCIPLES IN MULTIMODAL RESEARCH\\nAmodality refers to a way in which a natural phenomenon is perceived or expressed. For example,\\nmodalities include speech and audio recorded through microphones, images and videos captured\\nvia cameras, and force and vibrations captured via haptic sensors. Modalities can be placed along\\na spectrum from rawtoabstract : raw modalities are those more closely detected from a sensor,\\nsuch as speech recordings from a microphone or images captured by a camera. Abstract modalities\\nare those farther away from sensors, such as language extracted from speech recordings, objects\\ndetected from images, or even abstract concepts like sentiment intensity and object categories.\\nMultimodal refers to situations where multiple modalities are involved. From a research perspec-\\ntive, multimodal entails the computational study of heterogeneous andinterconnected (connections\\n+ interactions) modalities. Firstly, modalities are heterogeneous because the information present in\\ndifferent modalities will often show diverse qualities, structures, and representations. Secondly,\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 2}), Document(page_content='1:4 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n!()!()Dimensions of Heterogeneity$()$()\\nElementsDistributionStructureInformationNoiseRelevance\\n%!%\"\\nFig. 2. The information present in different modalities will often show diverse qualities, structures, and\\nrepresentations. Dimensions of heterogeneity can be measured via differences in individual elements and\\ntheir distribution, the structure of elements, as well as modality information, noise, and task relevance.\\nthese modalities are not independent entities but rather share connections due to complementary\\ninformation. Thirdly, modalities interact in different ways when they are integrated for a task. We\\nexpand on these three foundational principles of multimodal research in the following subsections.\\n2.1 Principle 1: Modalities are Heterogeneous\\nThe principle of heterogeneity reflects the observation that the information present in different\\nmodalities will often show diverse qualities, structures, and representations. Heterogeneity should\\nbe seen as a spectrum: two images from the same camera which capture the same view modulo\\ncamera wear and tear are closer to homogeneous, two different languages which capture the same\\nmeaning but are different depending on language families are slightly heterogeneous, language and\\nvision are even more heterogeneous, and so on. In this section, we present a non-exhaustive list of\\ndimensions of heterogeneity (see Figure 2 for an illustration). These dimensions are complementary\\nand may overlap; each multimodal problem likely involves heterogeneity in multiple dimensions.\\n(1)Element representation : Each modality is typically comprised of a set of elements - the most\\nbasic unit of data which cannot (or rather, the user chooses to not) be broken down into further\\nunits [ 26,147]. For example, typed text is recorded via a set of characters, videos are recorded\\nvia a set of frames, and graphs are recorded via a set of nodes and edges. What are the basic\\nelements present in each modality, and how can we represent them? Formally, this dimensions\\nmeasures heterogeneity in the sample space or representation space of modality elements.\\n(2)Distribution refers to the frequency and likelihood of elements in modalities. Elements typically\\nfollow a unique distribution, with words in a linguistic corpus following Zipf’s Law as a classic\\nexample. Distribution heterogeneity then refers to the differences in frequencies and likelihoods\\nof elements, such as different frequencies in recorded signals and the density of elements.\\n(3)Structure : Natural data exhibits structure in the way individual elements are composed to form\\nentire modalities [ 38]. For example, images exhibit spatial structure across individual object\\nelements, language is hierarchically composed of individual words, and signals exhibit temporal\\nstructure across time. Structure heterogeneity refers to differences in this underlying structure.\\n(4)Information measures the total information content present in each modality. Subsequently,\\ninformation heterogeneity measures the differences in information content across modalities,\\nwhich could be formally measured by information theoretic metrics [227].\\n(5)Noise : Noise can be introduced at several levels across naturally occurring data and also during\\nthe data recording process. Natural data noise includes occlusions, imperfections in human-\\ngenerated data (e.g., imperfect keyboard typing or unclear speech), or data ambiguity due to\\nsensor failures [ 151]. Noise heterogeneity measures differences in noise distributions across\\nmodalities, as well as differences in signal-to-noise ratio.\\n(6)Relevance : Finally, each modality shows different relevance toward specific tasks and contexts\\n- certain modalities may be more useful for certain tasks than others [ 78]. Task relevance\\ndescribes how modalities can be used for inference, while context relevance describes how\\nmodalities are contextualized with other modalities.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 3}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:5\\n9\\nStatistical\\nSemantic\\nAssociatione.g., correlation, co-occurrence\\nDependencye.g., causal, temporal\\nCorrespondencee.g., grounding\\nRelationship\\n=\\nlaptop\\nused fore.g., function\\nConnections: Shared information that relates modalities\\nunique\\nunique\\nstronger\\nweaker\\nunconnected\\nFig. 3. Modality connections describe how modalities are related and share commonalities, such as corre-\\nspondences between the same concept in language and images or dependencies across spatial and temporal\\ndimensions. Connections can be studied through both statistical and semantic perspectives.\\nIt is useful to take these dimensions of heterogeneity into account when studying both unimodal\\nand multimodal data. In the unimodal case, specialized encoders are typically designed to capture\\nthese unique characteristics in each modality [ 38]. In the multimodal case, modeling heterogeneity\\nis useful when learning representations and capturing alignment [ 314], and is a key subchallenge\\nin quantifying multimodal models [150].\\n2.2 Principle 2: Modalities are Connected\\nAlthough modalities are heterogeneous, they are often connected due to shared complementary\\ninformation. The presence of shared information is often in contrast to unique information that\\nexists solely in a single modality [ 290]. Modality connections describe the extent and dimensions\\nin which information can be shared across modalities. When reasoning about the connections in\\nmultimodal data, it is helpful to think about both bottom-up (statistical) and top-down (semantic)\\napproaches (see Figure 3). From a statistical data-driven perspective, connections are identified\\nfrom distributional patterns in multimodal data, while semantic approaches define connections\\nbased on our domain knowledge about how modalities share and contain unique information.\\n(1)Statistical association exists when the values of one variable relate to the values of another.\\nFor example, two elements may co-occur with each other, resulting in a higher frequency of\\nboth occurring at the same time. Statistically, this could lead to correlation - the degree in which\\nelements are linearly related, or other non-linear associations. From a data-driven perspective,\\ndiscovering which elements are associated with each other is important for modeling the joint\\ndistributions across modalities during multimodal representation and alignment [257].\\n(2)Statistical dependence goes deeper than association and requires an understanding of the\\nexact type of statistical dependency between two elements. For example, is there a causal\\ndependency from one element to another, or an underlying confounder causing both elements\\nto be present at the same time? Other forms of dependencies could be spatial or temporal: one\\nelement occurring above the other, or after the other. Typically, while statistical association\\ncan be estimated purely from data, understanding the nature of statistical dependence requires\\nsome knowledge of the elements and their underlying relationships [188, 267].\\n(3)Semantic correspondence can be seen as the problem of ascertaining which elements in one\\nmodality share the same semantic meaning as elements in another modality [ 192]. Identify-\\ning correspondences is fundamental in many problems related to language grounding [ 46],\\ntranslation and retrieval [203], and cross-modal alignment [248].\\n(4)Semantic relations : Finally, semantic relations generalize semantic correspondences: instead\\nof modality elements sharing the same exact meaning, semantic relations includes an attribute\\ndescribing the exact nature of the relationship between two modality elements, such as semantic,\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 4}), Document(page_content='1:6 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\nNon\\n-\\nadditive\\nAdditive\\nContextualized\\nAsymmetric\\nNoninteracting\\nNoninteracting\\nI\\nnformation\\nM\\nechanics\\nR\\nesponse\\nEquivalence\\nEnhancementand\\nIndependence\\nDominance\\nModulation(or)\\nEmergence+\\n+\\n+\\n+\\n+\\n+\\nRedundancy\"\\nNon\\n-\\nredundancy\"\\nFig. 4. Several dimensions of modality interactions : (1) Interaction information studies whether common\\nredundant information or unique non-redundant information is involved in interactions; (2) interaction\\nmechanics study the manner in which interaction occurs, and (3) interaction response studies how the\\ninferred task changes in the presence of multiple modalities.\\nlogical, causal, or functional relations. Identifying these semantically related connections is\\nimportant for higher-order reasoning [26, 172].\\n2.3 Principle 3: Modalities Interact\\nModality interactions study how modality elements interact to give rise to new information\\nwhen integrated together for task inference . We note an important difference between modality\\nconnections and interactions: connections exist within multimodal data itself, whereas interactions\\nonly arise when modalities are integrated and processed together to bring a new response. In\\nFigure 4, we provide a high-level illustration of some dimensions of interactions that can exist.\\n(1)Interaction information investigates the type of connected information that is involved in\\nan interaction. When an interaction involves shared information common to both modalities,\\nthe interaction is redundant , while a non-redundant interaction is one that does not solely rely\\non shared information, and instead relies on different ratios of shared, unique, or possibly even\\nsynergistic information [290].\\n(2)Interaction mechanics are the functional operators involved when integrating modality\\nelements for task inference. For example, interactions can be expressed as statistically additive,\\nnon-additive, and non-linear forms [ 117], as well as from a semantic perspective where two\\nelements interact through a logical, causal, or temporal operation [268].\\n(3)Interaction response studies how the inferred response changes in the presence of multiple\\nmodalities. For example, through sub-dividing redundant interactions, we can say that two\\nmodalities create an equivalence response if the multimodal response is the same as responses\\nfrom either modality, or enhancement if the multimodal response displays higher confidence.\\nOn the other hand, non-redundant interactions such as modulation or emergence happen when\\nthere exist different multimodal versus unimodal responses [197].\\n2.4 Core Technical Challenges\\nBuilding on these three core principles and on our detailed review of recent work, we propose a\\nnew taxonomy to characterize the core technical challenges in multimodal research: representation,\\nalignment, reasoning, generation, transference, and quantification. In Table 1 we summarize our full\\ntaxonomy of these six core challenges, their subchallenges, categories of corresponding approaches,\\nand recent examples in each category. In the following sections, we describe our new taxonomy\\nin detail and also revisit the principles of heterogeneity, connections, and interactions to see how\\nthey pose research questions and inspire research in each of these six challenges.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 5}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:7\\nTable 1. This table summarizes our taxonomy of 6core challenges in multimodal machine learning, their\\nsubchallenges, categories of corresponding approaches, and representative examples. We believe that this\\ntaxonomy can help to catalog rapid progress in this field and better identify the open research questions.\\nChallenge Subchallenge Approaches & key examples\\nRepresentation (§3)Fusion (§3.1) Abstract [117, 310] & raw [24, 209] fusion\\nCoordination (§3.2) Strong [75, 206] & partial [276, 319] coordination\\nFission (§3.3) Modality-level [94, 262] & fine-grained [1, 48] fission\\nAlignment (§4)Discrete connections (§4.1) Local [60, 100] & global [142] alignment\\nContinuous alignment (§4.2) Warping [90, 103] & segmentation [243]\\nContextualization (§4.3) Joint [140], cross-modal [93, 159] & graphical [301]\\nReasoning (§5)Structure modeling (§5.1) Hierarchical [15], temporal [297], interactive [161] & discovery [200]\\nIntermediate concepts (§5.2) Attention [299], discrete symbols [13, 274] & language [109, 317]\\nInference paradigm (§5.3) Logical [82, 246] & causal [4, 189, 304]\\nExternal knowledge (§5.4) Knowledge graphs [86, 324] & commonsense [196, 315]\\nGeneration (§6)Summarization (§6.1) Extractive [52, 270] & abstractive [139, 193]\\nTranslation (§6.2) Exemplar-based [122, 135] & generative [6, 115, 210]\\nCreation (§6.3) Conditional decoding [63, 191, 321]\\nTransference (§7)Cross-modal transfer (§7.1) Tuning [208, 266], multitask [150, 235] & transfer [160]\\nCo-learning (§7.2) Representation [118, 312] & generation [202, 249]\\nModel Induction (§7.3) Co-training [33, 68] & co-regularization [239, 302]\\nQuantification (§8)Heterogenity (§8.1) Importance [78, 195], bias [92, 199] & noise [163]\\nInterconnections (§8.2) Connections [3, 42, 255] & interactions [94, 149, 285]\\nLearning (§8.3) Generalization [150, 212], optimization [284, 293] & tradeoffs [151]\\n9Challenge 1: Representation\\nFusion\\nCoordination\\nFission\\n# modalities  > # representations# modalities  = # representations# modalities  < # representationsSub-challenges: Definition: Learning representations that reflect cross-modal interactions between individual elements, across different modalities\\nFig. 5. Challenge 1 aims to learn representations that reflect cross-modal interactions between individual\\nmodality elements, through (1) fusion : integrating information to reduce the number of separate representa-\\ntions, (2) coordination : interchanging cross-modal information with the goal of keeping the same number of\\nrepresentations but improving multimodal contextualization, and (3) fission : creating a larger set of decoupled\\nrepresentations that reflects knowledge about internal structure.\\n3 CHALLENGE 1: REPRESENTATION\\nThe first fundamental challenge is to learn representations that reflect cross-modal interactions\\nbetween individual elements across different modalities. This challenge can be seen as learning a\\n‘local’ representation between elements, or a representation using holistic features. This section\\ncovers (1) representation fusion : integrating information from 2 or more modalities, effectively\\nreducing the number of separate representations, (2) representation coordination : interchanging\\ncross-modal information with the goal of keeping the same number of representations but improv-\\ning multimodal contextualization, and (3) representation fission : creating a new decoupled set of\\nrepresentations, usually larger number than the input set, that reflects knowledge about internal\\nstructure such as data clustering or factorization (Figure 5).\\n3.1 Subchallenge 1a: Representation Fusion\\nRepresentation fusion aims to learn a joint representation that models cross-modal interactions\\nbetween individual elements of different modalities, effectively reducing the number of separate\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 6}), Document(page_content='1:8 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\nFusion with abstract modalities\\nHomogeneous\\nFusion with raw modalities\\nFusion\\nHeterogeneous\\nModality\\n-\\nlevel fission\\nFine\\n-\\ngrained fission\\nencoderencoderFusion\\nFig. 6. We categorize representation fusion approaches into (1) fusion with abstract modalities , where uni-\\nmodal encoders first capture a holistic representation of each element before fusion at relatively homogeneous\\nrepresentations, and (2) fusion with raw modalities which entails representation fusion at very early stages,\\nperhaps directly involving heterogeneous raw modalities.\\nrepresentations. We categorize these approaches into fusion with abstract modalities andfusion\\nwith raw modalities (Figure 6). In fusion with abstract modalities, suitable unimodal encoders are\\nfirst applied to capture a holistic representation of each element (or modality entirely), after which\\nseveral building blocks for representation fusion are used to learn a joint representation. As a\\nresult, fusion happens at the abstract representation level. On the other hand, fusion with raw\\nmodalities entails representation fusion at very early stages with minimal preprocessing, perhaps\\neven involving raw modalities themselves.\\nFusion with abstract modalities : We begin our treatment of representation fusion of abstract\\nrepresentations with additive and multiplicative interactions . These operators can be seen as differen-\\ntiable building blocks combining information from two streams of data that can be flexibly inserted\\ninto almost any unimodal machine learning pipeline. Given unimodal data or features x1andx2,\\nadditive fusion can be seen as learning a new joint representation zmm=𝑤0+𝑤1x1+𝑤2x2+𝜖,\\nwhere𝑤1and𝑤2are the weights learned for additive fusion of x1andx2,𝑤0the bias term, and\\n𝜖the error term. If the joint representation zmmis directly taken as a prediction ˆ𝑦, then additive\\nfusion resembles late or ensemble fusion ˆ𝑦=𝑓1(x1)+𝑓2(x2)with unimodal predictors 𝑓1and𝑓2[74].\\nOtherwise, the additive representation zmmcan also undergo subsequent unimodal or multimodal\\nprocessing [ 23]. Multiplicative interactions extend additive interactions to include a cross term\\n𝑤3(x1×x2). These models have been used extensively in statistics, where it can be interpreted as\\namoderation effect of x1affecting the linear relationship between x2and𝑦[25]. Overall, purely\\nadditive interactions zmm=𝑤0+𝑤1x1+𝑤2x2can be seen as a first-order polynomial between input\\nmodalities x1andx2, combining additive and multiplicative zmm=𝑤0+𝑤1x1+𝑤2x2+𝑤3(x1×x2)\\ncaptures a second-order polynomial.\\nTo further go beyond first and second-order interactions, tensors are specifically designed to\\nexplicitly capture higher-order interactions across modalities [ 310]. Given unimodal data x1,x2,\\ntensors are defined as zmm=x1⊗x2where⊗denotes an outer product [ 28,76]. Tensor products\\nof higher order represent polynomial interactions of higher order between elements [ 98]. However,\\ncomputing tensor products is expensive since their dimension scales exponentially with the number\\nof modalities, so several efficient approximations based on low-rank decomposition have been\\nproposed [ 98,158]. Finally, Multiplicative Interactions (MI) generalize additive and multiplicative\\noperators to include learnable parameters that capture second-order interactions [ 117]. In its most\\ngeneral form, MI defines a bilinear product zmm=x1Wx2+x⊤\\n1U+Vx2+bwhereW,U,Z, and bare\\ntrainable parameters.\\nMultimodal gated units/attention units learn representations that dynamically change for every\\ninput [ 47,284]. Its general form can be written as zmm=x1⊙ℎ(x2), whereℎrepresents a function\\nwith sigmoid activation and ⊙denotes element-wise product. ℎ(x2)is commonly referred to as\\n‘attention weights’ learned from x2to attend on x1. Recent work has explored more expressive forms\\nof learning attention weights such as using Query-Key-Value mechanisms [ 261], fully-connected\\nneural network layers [18, 47], or even hard gated units for sharper attention [55].\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 7}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:9\\nStrong coordination\\ncloser\\nPartial coordination\\nfurther\\nC\\norrelation\\nE\\nquivalence\\nO\\nrder\\nH\\nierarchy\\nRelationships\\nFig. 7. There is a spectrum of representation coordination functions: strong coordination aims to en-\\nforce strong equivalence in all dimensions, whereas in partial coordination only certain dimensions may be\\ncoordinated to capture more general connections such as correlation, order, hierarchies, or relationships.\\nFusion with raw modalities entails representation fusion at very early stages, perhaps even\\ninvolving raw modalities themselves. These approaches typically bear resemblance to early fu-\\nsion [ 23], which performs concatenation of input data before applying a prediction model (i.e.,\\nzmm=[x1,x2]). Fusing at the raw modality level is more challenging since raw modalities are\\nlikely to exhibit more dimensions of heterogeneity. Nevertheless, Barnum et al . [24] demonstrated\\nrobustness benefits of fusion at early stages, while Gadzicki et al . [77] also found that complex early\\nfusion can outperform abstract fusion. To account for the greater heterogeneity during complex\\nearly fusion, many approaches rely on generic encoders that are applicable to both modalities,\\nsuch as convolutional layers [ 24,77] and Transformers [ 150,153]. However, do these complex\\nnon-additive fusion models actually learn non-additive interactions between modality elements?\\nNot necessarily, according to Hessel and Lee [94]. We cover these fundamental analysis questions\\nand more in the quantification challenge (§8).\\n3.2 Subchallenge 1b: Representation Coordination\\nRepresentation coordination aims to learn multimodal contextualized representations that are\\ncoordinated through their interconnections (Figure 7). In contrast to representation fusion, coordi-\\nnation keeps the same number of representations but improves multimodal contextualization. We\\nstart our discussion with strong coordination that enforces strong equivalence between modality\\nelements, before moving on to partial coordination that captures more general connections such as\\ncorrelation, order, hierarchies, or relationships beyond similarity.\\nStrong coordination aims to bring semantically corresponding modalities close together in a\\ncoordinated space, thereby enforcing strong equivalence between modality elements. For example,\\nthese models would encourage the representation of the word ‘dog’ and an image of a dog to be close\\n(i.e., semantically positive pairs), while the distance between the word ‘dog’ and an image of a car\\nto be far apart (i.e., semantically negative pairs) [ 75]. The coordination distance is typically cosine\\ndistance [ 174,287] or max-margin losses [ 102]. Recent work has explored large-scale representation\\ncoordination by scaling up contrastive learning of image and text pairs [ 206], and also found that\\ncontrastive learning provably captures redundant information across the two views [ 256,258] (but\\nnot non-redundant information). In addition to contrastive learning, several approaches instead\\nlearn a coordinated space by mapping corresponding data from one modality to another [ 69]. For\\nexample, Socher et al . [236] maps image embeddings into word embedding spaces for zero-shot\\nimage classification. Similar ideas were used to learn coordinated representations between text,\\nvideo, and audio [202], as well as between pretrained language models and image features [249].\\nPartial coordination : Instead of strictly capturing equivalence via strong coordination, partial\\ncoordination instead captures more general modality connections such as correlation, order, hierar-\\nchies, or relationships. To achieve these goals, partially coordinated models enforce different types\\nof constraints on the representation space beyond semantic similarity, and perhaps only on certain\\ndimensions of the representation.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 8}), Document(page_content='1:10 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\nBasic \\nfusion\\nHomogeneous\\nComplex \\nfusion\\nFusion\\nHeterogeneous\\nModality\\n-\\nlevel fission\\nFine\\n-\\ngrained fission\\nencoderencoderFusion\\nStrong \\ncoordination\\ncloser\\nPartial \\ncoordination\\nfurther\\nFig. 8. Representation fission creates a larger set of decoupled representations that reflects knowledge\\nabout internal structure. (1) Modality-level fission factorizes into modality-specific information primarily\\nin each modality, and multimodal information redundant in both modalities, while (2) fine-grained fission\\nattempts to further break multimodal data down into individual subspaces.\\nCanonical correlation analysis (CCA) computes a linear projection that maximizes the corre-\\nlation between two random variables while enforcing each dimension in a new representation\\nto be orthogonal to each other [ 254]. CCA models have been used extensively for cross-modal\\nretrieval [ 211] audio-visual signal analysis [ 221], and emotion recognition [ 186]. To increase the ex-\\npressiveness of CCA, several nonlinear extensions have been proposed including Kernel CCA [ 134],\\nDeep CCA [16], and CCA Autoencoders [283].\\nOrdered and hierarchical spaces : Another example of representation coordination comes from\\norder-embeddings of images and language [ 276], which aims to capture a partial order on the\\nlanguage and image embeddings to enforce a hierarchy in the coordinated space. A similar model\\nusing denotation graphs was also proposed by Young et al . [306] where denotation graphs are used\\nto induce such a partial ordering hierarchy.\\nRelationship coordination : In order to learn a coordinated space that captures semantic relation-\\nships between elements beyond correspondences, Zhang et al . [319] use structured representations\\nof text and images to create multimodal concept taxonomies. Delaherche and Chetouani [61] learn\\ncoordinated representations capturing hierarchical relationships, while Alviar et al . [12] apply\\nmultiscale coordination of speech and music using partial correlation measures. Finally, Xu et al .\\n[298] learn coordinated representations using a Cauchy loss to strengthen robustness to outliers.\\n3.3 Subchallenge 1c: Representation Fission\\nFinally, representation fission aims to create a new decoupled set of representations (usually a\\nlarger number than the input representation set) that reflects knowledge about internal multimodal\\nstructure such as data clustering, independent factors of variation, or modality-specific information.\\nIn comparison with joint and coordinated representations, representation fission enables careful\\ninterpretation and fine-grained controllability. Depending on the granularity of decoupled factors,\\nmethods can be categorized into modality-level andfine-grained fission (Figure 8).\\nModality-level fission aims to factorize into modality-specific information primarily in each\\nmodality and multimodal information redundant in both modalities [ 101,262].Disentangled repre-\\nsentation learning aims to learn mutually independent latent variables that each explain a particular\\nvariation of the data [ 30,95], and has been useful for modality-level fission by enforcing indepen-\\ndence constraints on modality-specific and multimodal latent variables [ 101,262]. Tsai et al . [262]\\nand Hsu and Glass [101] study factorized multimodal representations and demonstrate the impor-\\ntance of modality-specific and multimodal factors towards generation and prediction. Shi et al . [231]\\nstudy modality-level fission in multimodal variational autoencoders using a mixture-of-experts\\nlayer, while Wu and Goodman [292] instead use a product-of-experts layer.\\nPost-hoc representation disentanglement is suitable when it is difficult to retrain a disentangled\\nmodel, especially for large pretrained multimodal models. Empirical multimodally-additive function\\nprojection (EMAP) [ 94] is an approach for post-hoc disentanglement of the effects of unimodal (ad-\\nditive) contributions from cross-modal interactions in multimodal tasks, which works for arbitrary\\nmultimodal models and tasks. EMAP is also closely related to the use of Shapley values for feature\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 9}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:11\\n1111Challenge 2: AlignmentDefinition:Identifying cross-modal correspondences and dependencies between elements of multiple modalities, following their structureSub-challenges: \\nDiscrete\\nAlignment\\nContinuous\\nAlignment\\nContextualized \\nRepresentations\\nDiscrete elementsand connectionsSegmentation andcontinuous warpingAlignment + representation\\nFig. 9. Alignment aims to identify cross-modal connections and interactions between modality elements.\\nRecent work has involved (1) discrete alignment to identify connections among discrete elements, (2) continuous\\nalignment of continuous signals with ambiguous segmentation, and (3) contextualized representation learning\\nto capture these cross-modal interactions between connected elements.\\ndisentanglement and interpretation [ 176], which can also be used for post-hoc representation\\ndisentanglement in general models.\\nFine-grained fission : Beyond factorizing only into individual modality representations, fine-\\ngrained fission attempts to further break multimodal data down into the individual subspaces\\ncovered by the modalities [ 277].Clustering approaches that group data based on semantic similar-\\nity [165] have been integrated with multimodal networks for end-to-end representation fission and\\nprediction. For example, Hu et al . [102] combine𝑘-means clustering in representations with unsu-\\npervised audiovisual learning. Chen et al . [48] combine𝑘-means clustering with self-supervised\\ncontrastive learning on videos. Subspace clustering [ 1], approximate graph Laplacians [ 125], conju-\\ngate mixture models [ 124], and dictionary learning [ 126] have also been integrated with multimodal\\nmodels. Motivated by similar goals of representation fission, matrix factorization techniques have\\nalso seen several applications in multimodal prediction [10] and image retrieval [41].\\n4 CHALLENGE 2: ALIGNMENT\\nA second challenge is to identify cross-modal connections and interactions between elements of\\nmultiple modalities. For example, when analyzing the speech and gestures of a human subject, how\\ncan we align specific gestures with spoken words or utterances? Alignment between modalities is\\nchallenging since it may depend on long-range dependencies, involves ambiguous segmentation\\n(e.g., words or utterances), and could be either one-to-one, many-to-many, or not exist at all. This\\nsection covers recent work in multimodal alignment involving (1) discrete alignment : identifying\\nconnections between discrete elements across modalities, (2) continuous alignment : modeling align-\\nment between continuous modality signals with ambiguous segmentation, and (3) contextualized\\nrepresentations : learning better multimodal representations by capturing cross-modal interactions\\nbetween elements (Figure 9).\\n4.1 Subchallenge 2a: Discrete Alignment\\nThe first subchallenge aims to identify connections between discrete elements of multiple modalities.\\nWe describe recent work in (1) local alignment to discover connections between a given matching\\npair of modality elements, and (2) global alignment where alignment must be performed globally to\\nlearn both the connections and matchings (Figure 10).\\nLocal alignment between connected elements is particularly suitable for multimodal tasks\\nwhere there is clear segmentation into discrete elements such as words in text or object bounding\\nboxes in images or videos (e.g., tasks such as visual coreference resolution [ 131], visual referring\\nexpression recognition [ 58,59], and cross-modal retrieval [ 75,203]). When we have supervised data\\nin the form of connected modality pairs, contrastive learning is a popular approach where the goal is\\nto match representations of the same concept expressed in different modalities [ 23]. Several objective\\nfunctions for learning aligned spaces from varying quantities of paired [ 43,107] and unpaired [ 85]\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 10}), Document(page_content='1:12 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\nContinuous \\nwarping\\nModality \\nsegmentation\\nLocal\\nGlobal\\nUndirected\\nDirected\\nFig. 10. Discrete alignment identifies connections between discrete elements, spanning (1) local alignment\\nto discover connections given matching pairs, and (2) global alignment where alignment must be performed\\nglobally to learn both the connections and matchings between modality elements.\\ndata have been proposed. Many of the ideas that enforce strong [ 75,152] or partial [ 16,276,319]\\nrepresentation coordination (§3.2) are also applicable for local alignment. Several examples include\\naligning books with their corresponding movies/scripts [ 323], matching referring expressions to\\nvisual objects [ 169], and finding similarities between image regions and their descriptions [ 105].\\nMethods for local alignment have also enabled the learning of shared semantic concepts not purely\\nbased on language but also on additional modalities such as vision [ 107], sound [ 60,236], and\\nmultimedia [323] that are useful for downstream tasks.\\nGlobal alignment : When the ground-truth modality pairings are not available, alignment must\\nbe performed globally between all elements across both modalities. Optimal transport (OT)-based\\napproaches [ 278] (which belong to a broader set of matching algorithms) are a potential solution\\nsince they jointly optimize the coordination function and optimal coupling between modality\\nelements by posing alignment as a divergence minimization problem. These approaches are useful\\nfor aligning multimodal representation spaces [ 142,205]. To alleviate computational issues, several\\nrecent advances have integrated them with neural networks [ 54], approximated optimal transport\\nwith entropy regularization [288], and formulated convex relaxations for efficient learning [85].\\n4.2 Subchallenge 2b: Continuous Alignment\\nSo far, one important assumption we have made is that modality elements are already segmented\\nand discretized. While certain modalities display clear segmentation (e.g., words/phrases in a\\nsentence or object regions in an image), there are many cases where the segmentation is not readily\\nprovided, such as in continuous signals (e.g, financial or medical time-series), spatio-temporal data\\n(e.g., satellite or weather images), or data without clear semantic boundaries (e.g., MRI images). In\\nthese settings, methods based on warping and segmentation have been recently proposed:\\nContinuous warping aims to align two sets of modality elements by representing them as\\ncontinuous representation spaces and forming a bridge between these representation spaces.\\nAdversarial training is a popular approach to warp one representation space into another. Initially\\nused in domain adaptation [ 27], adversarial training learns a domain-invariant representation across\\ndomains where a domain classifier is unable to identify which domain a feature came from [ 8]. These\\nideas have been extended to align multimodal spaces [ 100,103,181]. Hsu et al . [100] use adversarial\\ntraining to align images and medical reports, Hu et al . [103] design an adversarial network for cross-\\nmodal retrieval, and Munro and Damen [181] design both self-supervised alignment and adversarial\\nalignment objectives for multimodal action recognition. Dynamic time warping (DTW) [133] is a\\nrelated approach to segment and align multi-view time series data. DTW measures the similarity\\nbetween two sequences and finds an optimal match between them by time warping (inserting\\nframes) such that they are aligned across segmented time boundaries. For multimodal tasks, it is\\nnecessary to design similarity metrics between modalities [ 17,251]. DTW was extended using CCA\\nto map the modalities to a coordinated space, allowing for both alignment (through DTW) and\\ncoordination (through CCA) between different modality streams jointly [260].\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 11}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:13\\nContinuous \\nwarping\\nModality \\nsegmentation\\nLocal\\nGlobal\\nUndirected\\nDirected\\nFig. 11. Continuous alignment tackles the difficulty of aligning continuous signals where element segmen-\\ntation is not readily available. We cover related work in (1) continuous warping of representation spaces and\\n(2)modality segmentation of continuous signals into discrete elements at an appropriate granularity.\\nModality segmentation involves dividing high-dimensional data into elements with semantically-\\nmeaningful boundaries. A common problem involves temporal segmentation , where the goal is to\\ndiscover the temporal boundaries across sequential data. Several approaches for temporal segmen-\\ntation include forced alignment, a popular approach to align discrete speech units with individual\\nwords in a transcript [ 309]. Malmaud et al . [167] explore multimodal alignment using a factored\\nhidden Markov model to align ASR transcripts to the ground truth. Clustering approaches have also\\nbeen used to group continuous data based on semantic similarity [ 165]. Clustering-based discretiza-\\ntion has recently emerged as an important preprocessing step for generalizing language-based\\npretraining (with clear word/bytepair segmentation boundaries and discrete elements) to video\\nor audio-based pretraining (without clear segmentation boundaries and continuous elements). By\\nclustering raw video or audio features into a discrete set, approaches such as VideoBERT [ 243] per-\\nform masked pretraining on raw video and audio data. Similarly, approaches such as DALL.E [ 210],\\nVQ-VAE [ 271], and CMCM [ 156] also utilize discretized intermediate layers obtained via vector\\nquantization and showed benefits in modality alignment.\\n4.3 Subchallenge 2c: Contextualized Representations\\nFinally, contextualized representation learning aims to model all modality connections and in-\\nteractions to learn better representations. Contextualized representations have been used as an\\nintermediate (often latent) step enabling better performance on a number of downstream tasks in-\\ncluding speech recognition, machine translation, media description, and visual question-answering.\\nWe categorize work in contextualized representations into (1) joint undirected alignment , (2)cross-\\nmodal directed alignment , and (3) alignment with graph networks (Figure 12).\\nJoint undirected alignment aims to capture undirected connections across pairs of modalities,\\nwhere the connections are symmetric in either direction. This is commonly referred to in the\\nliterature as unimodal, bimodal, trimodal interactions, and so on [ 164]. Joint undirected alignment\\nis typically captured by parameterizing models with alignment layers and training end-to-end for a\\nmultimodal task. These alignment layers can include attention weights [ 47], tensor products [ 158,\\n310], and multiplicative interactions [ 117]. More recently, transformer models [ 273] have emerged\\nas powerful encoders for sequential data by automatically aligning and capturing complementary\\nfeatures at different time steps. Building upon the initial text-based transformer model, multimodal\\ntransformers have been proposed that perform joint alignment using a full self-attention over\\nmodality elements concatenated across the sequence dimension (i.e., early fusion) [ 140,243]. As a\\nresult, all modality elements become jointly connected to all other modality elements similarly (i.e.,\\nmodeling all connections using dot-product similarity kernels).\\nCross-modal directed alignment relates elements from a source modality in a directed man-\\nner to a target modality, which can model asymmetric connections. For example, temporal attention\\nmodels use alignment as a latent step to improve many sequence-based tasks [ 297,318]. These\\nattention mechanisms are typically directed from the output to the input so that the resulting\\nweights reflect a soft alignment distribution over the input. Multimodal transformers perform\\ndirected alignment using query-key-value attention mechanisms to attend from one modality’s\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 12}), Document(page_content='1:14 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\nJoint undirected \\nalignment\\nCross\\n-\\nmodal\\ndirected alignment\\nGraphical\\nalignment\\nFig. 12. Contextualized representation learning aims to model modality connections to learn better rep-\\nresentations. Recent directions include (1) joint undirected alignment that captures undirected symmetric\\nconnections, (2) cross-modal directed alignment that models asymmetric connections in a directed manner,\\nand (3) graphical alignment that generalizes the sequential pattern into arbitrary graph structures.\\n1111Challenge 3: ReasoningDefinition:Combining knowledge, usually through multiple inferential steps, exploiting multimodal alignment and problem structureSub-challenges: \\nStructure \\nModeling\\nIntermediate \\nconcepts\\nInference \\nParadigm\\nExternal \\nKnowledge\\n∧\\n\"#$%\\n)\\noror\\nwords\\nFig. 13. Reasoning aims to combine knowledge, usually through multiple inferential steps, exploiting the\\nproblem structure. Reasoning involves (1) structure modeling : defining or learning the relationships over which\\nreasoning occurs, (2) the intermediate concepts used in reasoning, (3) inference of increasingly abstract concepts\\nfrom evidence, and (4) leveraging external knowledge in the study of structure, concepts, and inference.\\nsequence to another, before repeating in a bidirectional manner. This results in two sets of asym-\\nmetric contextualized representations to account for the possibly asymmetric connections between\\nmodalities [ 159,248,261]. These methods are useful for sequential data by automatically aligning\\nand capturing complementary features at different time-steps [ 261]. Self-supervised multimodal\\npretraining has also emerged as an effective way to train these architectures, with the aim of\\nlearning general-purpose representations from larger-scale unlabeled multimodal data before trans-\\nferring to specific downstream tasks via supervised fine-tuning [ 140]. These pretraining objectives\\ntypically consist of unimodal masked prediction, crossmodal masked prediction, and multimodal\\nalignment prediction [93].\\nGraphical alignment generalizes the sequential pattern seen in undirected or directed align-\\nment into arbitrary graph structures between elements. This has several benefits since it does not\\nrequire all elements to be connected, and allows the user to choose different edge functions for dif-\\nferent connections. Solutions in this subcategory typically make use of graph neural networks [ 275]\\nto recursively learn element representations contextualized with the elements in locally connected\\nneighborhoods [ 223,275]. These approaches have been applied for multimodal sequential data\\nthrough MTAG [ 301] that captures connections in human videos, and F2F-CL [ 289] that additionally\\nadds factorizes nodes along speaker turns.\\n5 CHALLENGE 3: REASONING\\nReasoning is defined as combining knowledge, usually through multiple inferential steps, exploit-\\ning multimodal alignment and the problem structure. We categorize work towards multimodal\\nreasoning into 4subchallenges of structure modeling, intermediate concepts, inference paradigm,\\nand external knowledge (Figure 13). (1) Structure modeling involves defining or learning the re-\\nlationships over which reasoning occurs, (2) intermediate concepts studies the parameterization\\nof individual multimodal concepts in the reasoning process, (3) inference paradigm learns how\\nincreasingly abstract concepts are inferred from individual multimodal evidence, and (4) external\\nknowledge aims to leverage large-scale databases in the study of structure, concepts, and inference.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 13}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:15\\nHierarchical\\nTemporal\\nInteractive\\nDiscovery\\n#=%#=&\\n...\\n#=%#=&\\n&!&\"...\\nFig. 14. Structure modeling aims to define the relationship over which composition occurs, which can be\\n(1)hierarchical (i.e., more abstract concepts are defined as a function of less abstract ones), (2) temporal (i.e.,\\norganized across time), (3) interactive (i.e., where the state changes depending on each step’s decision), and\\n(4)discovered when the latent structure is unknown and instead directly inferred from data and optimization.\\n5.1 Subchallenge 3a: Structure Modeling\\nStructure modeling aims to capture the hierarchical relationship over which composition occurs,\\nusually via a data structure parameterizing atoms, relations, and the reasoning process. Commonly\\nused data structures include trees [ 97], graphs [ 308], or neural modules [ 15]. We cover recent work\\nin modeling latent hierarchical ,temporal , and interactive structure, as well as structure discovery\\nwhen the latent structure is unknown (Figure 14).\\nHierarchical structure defines a system of organization where abstract concepts are defined as\\na function of less abstract ones. Hierarchical structure is present in many tasks involving language\\nsyntax, visual syntax, or higher-order reasoning. These approaches typically construct a graph\\nbased on predefined node and edge categories before using (heterogeneous variants of) graph\\nneural networks to capture a representation of structure [ 230], such as using language syntactic\\nstructure to guide visual modules that discover specific information in images [ 15,58]. Graph-\\nbased reasoning approaches have been applied for visual commonsense reasoning [ 155], visual\\nquestion answering [ 220], machine translation [ 305], recommendation systems [ 250], web image\\nsearch [281], and social media analysis [224].\\nTemporal structure extends the notion of compositionality to elements across time, which is\\nnecessary when modalities contain temporal information, such as in video, audio, or time-series\\ndata. Explicit memory mechanisms have emerged as a popular choice to accumulate multimodal\\ninformation across time so that long-range cross-modal interactions can be captured through\\nstorage and retrieval from memory. Rajagopalan et al . [209] explore various memory representations\\nincluding multimodal fusion, coordination, and factorization. Insights from key-value memory [ 297]\\nand attention-based memory [ 311] have also been successfully applied to applications including\\nquestion answering, video captioning, emotion recognition, and sentiment analysis.\\nInteractive structure extends the challenge of reasoning to interactive settings, where the\\nstate of the reasoning agent changes depending on the local decisions made at every step. Typi-\\ncally formalized by the sequential decision-making framework, the challenge lies in maximizing\\nlong-term cumulative reward despite only interacting with the environment through short-term\\nactions [ 244]. To tackle the challenges of interactive reasoning, the growing research field of multi-\\nmodal reinforcement learning (RL) has emerged from the intersection of language understanding,\\nembodiment in the visual world, deep reinforcement learning, and robotics. We refer the reader to\\nthe extensive survey paper by Luketina et al. [161] and the position paper by Bisk et al. [32] for a\\nfull review of this field. Luketina et al . [161] separate the literature into multimodal-conditional\\nRL (in which multimodal interaction is necessitated by the problem formulation itself, such as\\ninstruction following [ 47,286]) and language-assisted RL (in which multimodal data is optionally\\nused to facilitate learning, such as reading instruction manuals [185]).\\nStructure discovery : It may be challenging to define the structure of multimodal composition\\nwithout some domain knowledge of the given task. As an alternative approach, recent work has\\nalso explored using differentiable strategies to automatically search for the structure in a fully\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 14}), Document(page_content='1:16 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\ndata-driven manner. To do so, one first needs to define a candidate set of reasoning atoms and\\nrelationships, before using a ‘meta’ approach such as architecture search to automatically search\\nfor the ideal sequence of compositions for a given task [ 200,300]. These approaches can benefit\\nfrom optimization tricks often used in the neural architecture search literature. Memory, Attention,\\nand Composition (MAC) similarly search for a series of attention-based reasoning steps from data\\nin an end-to-end approach [ 110]. Finally, Hu et al . [104] extend the predefined reasoning structure\\nobtained through language parsing in Andreas et al . [15] by instead using policy gradients to\\nautomatically optimize a compositional structure over a discrete set of neural modules.\\n5.2 Subchallenge 3b: Intermediate Concepts\\nThe second subchallenge studies how we can parameterize individual multimodal concepts within\\nthe reasoning process. While intermediate concepts are usually dense vector representations in\\nstandard neural architectures, there has also been substantial work towards interpretable attention\\nmaps, discrete symbols, and language as an intermediate medium for reasoning.\\nAttention maps are a popular choice for intermediate concepts since they are, to a certain\\nextent, human-interpretable, while retaining differentiability. For example, Andreas et al . [15] design\\nindividual modules such as ‘attend’, ‘combine’, ‘count’, and ‘measure’ that are each parametrized by\\nattention operations on the input image for visual question answering. Xu et al . [299] explore both\\nsoft and hard attention mechanisms for reasoning in image captioning generation. Related work\\nhas also used attention maps through dual attention architectures [ 182] or stacked latent attention\\narchitectures [ 71] for multimodal reasoning. These are typically applied for problems involving\\ncomplex reasoning steps such as CLEVR [120] or VQA [320].\\nDiscrete symbols : A further level of discretization beyond attention maps involves using\\ndiscrete symbols to represent intermediate concepts. Recent work in neuro-symbolic learning aims\\nto integrate these discrete symbols as intermediate steps in multimodal reasoning in tasks such as\\nvisual question answering [ 15,168,274] or referring expression recognition [ 58]. A core challenge\\nin this approach lies in maintaining differentiability of discrete symbols, which has been tackled\\nvia logic-based differentiable reasoning [13, 226].\\nLanguage as a medium : Finally, perhaps the most human-understandable form of intermediate\\nconcepts is natural language (through discrete words or phrases) as a medium. Recently, Zeng\\net al. [317] explore using language as an intermediate medium to coordinate multiple separate\\npretrained models in a zero-shot manner. Several approaches also used language phrases obtained\\nfrom external knowledge graphs to facilitate interpretable reasoning [ 86,324]. Hudson and Manning\\n[109] designed a neural state machine to simulate the execution of a question being asked about an\\nimage, while using discrete words as intermediate concepts.\\n5.3 Subchallenge 3c: Inference Paradigms\\nThe third subchallenge in multimodal reasoning defines the way in which increasingly abstract\\nconcepts are inferred from individual multimodal evidence. While advances in local representation\\nfusion (such as additive, multiplicative, tensor-based, attention-based, and sequential fusion, see §3.1\\nfor a full review) are also generally applicable here, the goal is reasoning is to be more interpretable\\nin the inference process through domain knowledge about the multimodal problem. To that end, we\\ncover recent directions in explicitly modeling the inference process via logical and causal operators\\nas examples of recent trends in this direction.\\nLogical inference : Logic-based differentiable reasoning has been widely used to represent\\nknowledge in neural networks [ 13,226]. Many of these approaches use differentiable fuzzy\\nlogic [ 272] which provides a probabilistic interpretation of logical predicates, functions, and con-\\nstants to ensure differentiability. These logical operators have been applied for visual question\\nanswering [ 82] and visual reasoning [ 13]. Among the greatest benefits of logical reasoning lies in its\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 15}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:17\\nability to perform interpretable and compositional multi-step reasoning [ 111]. Logical frameworks\\nhave also been useful for visual-textual entailment [ 246] and geometric numerical reasoning [ 50],\\nfields where logical inductive biases are crucial toward strong performance.\\nCausal inference extends the associational level of reasoning to interventional and counter-\\nfactual levels [ 198], which requires extensive knowledge of the world to imagine counterfactual\\neffects. For example, Yi et al . [304] propose the CLEVRER benchmark focusing on four specific\\nelements of reasoning on videos: descriptive (e.g., ‘what color’), explanatory (‘what’s responsible\\nfor’), predictive (‘what will happen next’), and counterfactual (‘what if’). Beyond CLEVRER, recent\\nwork has also proposed Causal VQA [ 4] and Counterfactual VQA [ 189] to measure the robustness of\\nVQA models under controlled interventions to the question as a step towards mitigating language\\nbias in VQA models. Methods inspired by integrating causal reasoning capabilities into neural\\nnetwork models have also been shown to improve robustness and reduce biases [282].\\n5.4 Subchallenge 3d: External Knowledge\\nThe final subchallenge studies the derivation of knowledge in the study of defining composition\\nand structure. Knowledge is typically derived from domain knowledge on task-specific datasets. As\\nan alternative to using domain knowledge to pre-define the compositional structure, recent work\\nhas also explored reasoning automatically using data-driven methods, such as widely accessible\\nbut more weakly supervised data outside the immediate task domain.\\nMultimodal knowledge graphs extend classic work in language and symbolic knowledge\\ngraphs (e.g., Freebase [ 35], DBpedia [ 20], YAGO [ 241], WordNet [ 178]) to semantic networks\\ncontaining multimodal concepts as nodes and multimodal relationships as edges [ 322]. Multimodal\\nknowledge graphs are important because they enable the grounding of structured information in\\nthe visual and physical world. For example, Liu et al . [157] constructs multimodal knowledge graphs\\ncontaining both numerical features and images for entities. Visual Genome is another example\\ncontaining dense annotations of objects, attributes, and relationships in images and text [ 132]. These\\nmultimodal knowledge bases have been shown to benefit visual question answering [ 294,324],\\nknowledge base completion [ 201], and image captioning [ 175,303]. Gui et al . [86] integrates\\nknowledge into vision-and-language transformers for automatic reasoning over both knowledge\\nsources. We refer the reader to a comprehensive survey by Zhu et al . [322] for additional references.\\nMultimodal commonsense reasoning requires deeper real-world knowledge potentially\\nspanning logical, causal, and temporal relationships between concepts. For example, elements\\nof causal reasoning are required to answer the questions regarding images in VCR [ 315] and Vi-\\nsualCOMET [ 196], while other works have also introduced datasets with video and text inputs\\nto test for temporal reasoning (e.g., MovieQA [ 252], MovieFIB [ 166], TVQA [ 137]). Benchmarks\\nfor multimodal commonsense typically require leveraging external knowledge from knowledge\\nbases [237] or pretraining paradigms on large-scale datasets [159, 316].\\n6 CHALLENGE 4: GENERATION\\nThe fourth challenge involves learning a generative process to produce raw modalities that reflect\\ncross-modal interactions, structure, and coherence, through summarization ,translation , and creation\\n(Figure 15). These three categories are distinguished based on the information change from input to\\noutput modalities, following categorizations in text generation [ 62]. We will cover recent advances\\nas well as the evaluation of generated content.\\n6.1 Subchallenge 4a: Summarization\\nSummarization aims to compress data to create an abstract that represents the most important or\\nrelevant information within the original content. Recent work has explored various input modalities\\nto guide text summarization, such as images [ 51], video [ 141], and audio [ 70,116,139]. Recent\\ntrends in multimodal summarization include extractive and abstractive approaches. Extractive\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 16}), Document(page_content='1:18 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n7Challenge 4: GenerationDefinition:Learning a generative process to produce raw modalities that reflects cross-modal interactions, structure and coherenceSub-challenges: \\nSummarization\\nTranslation\\nCreation\\nReduction\\nExpansion\\nMaintenance\\n>\\nInformation:\\n(content)\\n<\\n=\\nFig. 15. How can we learn a generative process to produce raw modalities that reflect cross-modal interactions,\\nstructure, and coherence? Generation involves (1) summarizing multimodal data to highlight the most salient\\nparts, (2) translating from one modality to another while being consistent with modality connections, and (3)\\ncreating multiple modalities simultaneously while maintaining coherence.\\napproaches aim to filter words, phrases, and other unimodal elements from the input to create a\\nsummary [ 52,116,139]. Beyond text as output, video summarization is the task of producing a\\ncompact version of the video (visual summary) by encapsulating the most informative parts [ 218]. Li\\net al. [139] collected a dataset of news videos and articles paired with manually annotated summaries\\nas a benchmark towards multimodal summarization. Finally, UzZaman et al . [270] aim to simplify\\ncomplex sentences by extracting multimodal summaries for accessibility. On the other hand,\\nabstractive approaches define a generative model to generate the summary at multiple levels of\\ngranularity [ 51,143]. Although most approaches only focus on generating a textual summary from\\nmultimodal data [ 193], several directions have also explored generating summarized images to\\nsupplement the generated textual summary [51, 141].\\n6.2 Subchallenge 4b: Translation\\nTranslation aims to map one modality to another while respecting semantic connections and\\ninformation content [ 279]. For example, generating a descriptive caption of an image can help\\nimprove the accessibility of visual content for blind people [ 88]. Multimodal translation brings\\nabout new difficulties involving the generation of high-dimensional structured data as well as\\ntheir evaluation. Recent approaches can be classified as exemplar-based , which are limited to\\nretrieving from training instances to translate between modalities but guarantee fidelity [ 72], and\\ngenerative models which can translate into arbitrary instances interpolating beyond the data but\\nface challenges in quality, diversity, and evaluation [ 128,210,266]. Despite these challenges, recent\\nprogress in large-scale translation models has yielded impressive quality of generated content in\\ntext-to-image [ 210,215], text-to-video [ 234], audio-to-image [ 115], text-to-speech [ 213], speech-to-\\ngesture [ 6], speaker-to-listener [ 187], language to pose [ 7], and speech and music generation [ 191].\\n6.3 Subchallenge 4c: Creation\\nCreation aims to generate novel high-dimensional data (which could span text, images, audio, video,\\nand other modalities) from small initial examples or latent conditional variables. This conditional\\ndecoding process is extremely challenging since it needs to be (1) conditional: preserve semanti-\\ncally meaningful mappings from the initial seed to a series of long-range parallel modalities, (2)\\nsynchronized: semantically coherent across modalities, (3) stochastic: capture many possible future\\ngenerations given a particular state, and (4) auto-regressive across possibly long ranges. Many\\nmodalities have been considered as targets for creation. Language generation has been explored\\nfor a long time [ 207], and recent work has explored high-resolution speech and sound generation\\nusing neural networks [ 191]. Photorealistic image generation has also recently become possible due\\nto advances in large-scale generative modeling [ 123]. Furthermore, there have been a number of at-\\ntempts at generating abstract scenes [ 247], computer graphics [ 177], and talking heads [ 321]. While\\nthere has been some progress toward video generation [ 234], complete synchronized generation of\\nrealistic video, text, and audio remains a challenge.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 17}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:19\\n1313Challenge 5: TransferenceDefinition:Transfer knowledge between modalities, usually to help the target modality which may be noisy or with limited resourcesSub-challenges: \\nTransfer\\nCo\\n-\\nlearning\\n!\\n!\\nModel Induction\\n!!\\n!\"\\nFig. 16. Transference studies the transfer of knowledge between modalities, usually to help a noisy or limited\\nprimary modality, via (1) cross-modal transfer from models trained with abundant data in the secondary\\nmodality, (2) multimodal co-learning to share information across modalities by sharing representations, and\\n(3)model induction that keeps individual unimodal models separate but induces behavior in separate models.\\nFinally, one of the biggest challenges facing multimodal generation is difficulty in evaluating\\ngenerated content, especially when there exist serious ethical issues when fake news [ 29], hate\\nspeech [ 2,79], deepfakes [ 89], and lip-syncing videos [ 245] can be easily generated. While the ideal\\nway to evaluate generated content is through user studies, it is time-consuming, costly, and can\\npotentially introduce subjectivity bias into the evaluation process [ 81]. Several automatic proxy\\nmetrics have been proposed [ 14,53] by none are universally robust across many generation tasks.\\n7 CHALLENGE 5: TRANSFERENCE\\nTransference aims to transfer knowledge between modalities and their representations. How can\\nknowledge learned from a secondary modality (e.g., predicted labels or representation) help a model\\ntrained on a primary modality? This challenge is particularly relevant when the primary modality\\nhas limited resources — a lack of annotated data, noisy inputs, or unreliable labels. We call this\\nchallenge transference since the transfer of information from the secondary modality gives rise to\\nnew behaviors previously unseen in the primary modality. We identify three types of transference\\napproaches: (1) cross-modal transfer , (2)multimodal co-learning , and (3) model induction (Figure 16).\\n7.1 Subchallenge 5a: Cross-modal Transfer\\nIn most settings, it may be easier to collect either labeled or unlabeled data in the secondary\\nmodality and train strong supervised or pretrained models. These models can then be conditioned\\nor fine-tuned for a downstream task involving the primary modality. In other words, this line of\\nresearch extends unimodal transfer and fine-tuning to cross-modal settings.\\nTuning : Inspired by prior work in NLP involving prefix tuning [ 146] and prompt tuning [ 138],\\nrecent work has also studied the tuning of pretrained language models to condition on visual and\\nother modalities. For example, Tsimpoukelli et al . [266] quickly conditions a pretrained, frozen\\nlanguage model on images for image captioning. Related work has also adapted prefix tuning\\nfor image captioning [ 49], multimodal fusion [ 91], and summarization [ 307]. While prefix tuning\\nis simple and efficient, it provides the user with only limited control over how information is\\ntransferred. Representation tuning goes a level deeper by modifying the inner representations of the\\nlanguage model via contextualization with other modalities. For example, Ziegler et al . [325] includes\\nadditional self-attention layers between language model layers and external modalities. Rahman\\net al. [208] design a shifting gate to adapt language model layers with audio and visual information.\\nMultitask learning aims to use multiple large-scale tasks to improve performance as compared\\nto learning on individual tasks. Several models such as Perceiver [ 113], MultiModel [ 121], ViT-\\nBERT [ 144], and PolyViT [ 153] have explored the possibility of using the same unimodal encoder\\narchitecture for different inputs across unimodal tasks (i.e., language, image, video, or audio-only).\\nThe Transformer architecture has emerged as a popular choice due to its suitability for serialized\\ninputs such as text (sequence of tokens) [ 65], images (sequence of patches) [ 67], video (sequence\\nof images) [ 243], and other time-series data (sequence of timesteps) [ 154]. There have also been\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 18}), Document(page_content='1:20 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\nseveral attempts to build a single model that works well on a suite of multimodal tasks, including\\nboth not limited to HighMMT [150], VATT [9], FLAVA [235], and Gato [212].\\nTransfer learning : While more research has focused on transfer within the same modality\\nwith external information [ 236,296,312], Liang et al . [152] studies transfer to new modalities using\\nsmall amounts of paired but unlabeled data. Lu et al . [160] found that Transformers pretrained on\\nlanguage transfer to other sequential modalities as well. Liang et al . [150] builds a single multimodal\\nmodel capable of transferring to completely new modalities and tasks. Recently, there has also\\nbeen a line of work investigating the transfer of pretrained language models for planning [ 106]\\nand interactive decision-making [145].\\n7.2 Subchallenge 5b: Multimodal Co-learning\\nMultimodal co-learning aims to transfer information learned through secondary modalities to\\ntarget tasks involving the primary modality by sharing intermediate representation spaces between\\nboth modalities. These approaches essentially result in a single joint model across all modalities.\\nCo-learning via representation aims to learn either a joint or coordinated representation space\\nusing both modalities as input. Typically, this involves adding secondary modalities during the\\ntraining process, designing a suitable representation space, and investigating how the multimodal\\nmodel transfers to the primary modality during testing. For example, DeViSE learns a coordinated\\nsimilarity space between image and text to improve image classification [ 75]. Marino et al . [171]\\nuse knowledge graphs for image classification via a graph-based joint representation space. Jia\\net al. [118] improve image classifiers with contrastive representation learning between images and\\nnoisy captions. Finally, Zadeh et al . [312] showed that implicit co-learning is also possible without\\nexplicit co-learning objectives.\\nCo-learning via generation instead learns a translation model from the primary to secondary\\nmodality, resulting in enriched representations of the primary modality that can predict both\\nthe label and ‘hallucinate’ secondary modalities containing shared information. Classic examples\\nin this category includes language modeling by mapping contextualized text embeddings into\\nimages [ 249], image classification by projecting image embeddings into word embeddings [ 236],\\nand language sentiment analysis by translating language into video and audio [202].\\n7.3 Subchallenge 5c: Model Induction\\nIn contrast to co-learning, model induction approaches keep individual unimodal models across\\nprimary and secondary modalities separate but aim to induce behavior in both models. Model\\ninduction is exemplified by co-training, in which two learning algorithms are trained separately\\non each view of the data before using each algorithm’s predictions to pseudo-label new unlabeled\\nexamples to enlarge the training set of the other view [ 33]. Therefore, information is transferred\\nacross multiple views through model predictions instead of shared representation spaces.\\nMultimodal co-training extends co-training by jointly learning classifiers for multiple modali-\\nties [ 96]. Guillaumin et al . [87] study semi-supervised learning by using a classifier on both image\\nand text to pseudo-label unlabeled images before training a final classifier on both labeled and\\nunlabeled images. Cheng et al . [56] performs semi-supervised multimodal learning using a diversity-\\npreserving co-training algorithm. Finally, Dunnmon et al . [68] applies ideas from data programming\\nto the problem of cross-modal weak supervision, where weak labels derived from a secondary\\nmodality (e.g., text) are used to train models over the primary modality (e.g., images).\\nCo-regularization : Another set of models employs a regularizer that penalizes functions from ei-\\nther modality that disagree with each other. This class of models, called co-regularization, is a useful\\ntechnique to control model complexity by preferring hypothesis classes containing models that pre-\\ndict similarly across the two views [ 233]. Sridharan and Kakade [239] provide guarantees for these\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 19}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:21\\n1616Challenge 6: QuantificationDefinition:Empirical and theoretical study to better understand heterogeneity, cross-modal interactions and the multimodal learning processSub-challenges: \\nHeterogeneity\\nInterconnections\\nLearning\\nEpoch\\nLoss\\nFig. 17. Quantification : what are the empirical and theoretical studies we can design to better understand\\n(1) the dimensions of heterogeneity , (2) the presence and type of interconnections , and (3) the learning and\\noptimization challenges?\\napproaches using an information-theoretic framework. More recently, similar co-regularization ap-\\nproaches have also been applied for multimodal feature selection [ 99], semi-supervised multimodal\\nlearning [302], and video summarization [179].\\n8 CHALLENGE 6: QUANTIFICATION\\nQuantification aims to provide a deeper empirical and theoretical study of multimodal models to\\ngain insights and improve their robustness, interpretability, and reliability in real-world applications.\\nWe break down quantification into 3 sub-challenges: (1) quantifying the dimensions of heterogeneity\\nand how they subsequently influence modeling and learning, (2) quantifying the presence and type\\nofconnections and interactions in multimodal datasets and trained models, and (3) characterizing the\\nlearning and optimization challenges involved when learning from heterogeneous data (Figure 17).\\n8.1 Subchallenge 6a: Dimensions of Heterogeneity\\nThis subchallenge aims to understand the dimensions of heterogeneity commonly encountered in\\nmultimodal research, and how they subsequently influence modeling and learning (Figure 18).\\nModality information : Understanding the information of entire modalities and their con-\\nstituents is important for determining which segment of each modality contributed to subsequent\\nmodeling. Recent work can be categorized into: (1) interpretable methods that explicitly model how\\neach modality is used [ 195,263,313] or (2) post-hoc explanations of black-box models [ 45,84]. In\\nthe former, methods such as Concept Bottleneck Models [ 129] and fitting sparse linear layers [ 291]\\nor decision trees [ 280] on top of deep feature representations have emerged as promising choices. In\\nthe latter, approaches such as gradient-based visualizations [ 84,225,232]) and feature attributions\\n(e.g., modality contribution [ 78], LIME [ 214], and Shapley values [ 176]) have been used to highlight\\nregions of each modality used by the model.\\nModality biases are unintended correlations between input and outputs that could be introduced\\nduring data collection [ 31,36], modeling [ 80], or during human annotation [ 64]. Modality biases\\ncan lead to unexpectedly poor performance in the real world [ 219], or even more dangerously,\\npotential for harm towards underrepresented groups [ 92,199]. For example, Goyal et al . [83] found\\nunimodal biases in the language modality of VQA tasks, resulting in mistakes due to ignoring\\nvisual information [ 5]. Subsequent work has developed carefully-curated diagnostic benchmarks to\\nmitigate data collection biases, like VQA 2.0 [ 83], GQA [ 111], and NLVR2 [ 242]. Recent work has\\nalso found compounding social biases in multimodal systems [ 57,216,240] stemming from gender\\nbias in both language and visual modalities [ 39,229], which may cause danger when deployed [ 199].\\nModality noise topologies and robustness : The study of modality noise topologies aims\\nto benchmark and improve how multimodal models perform in the presence of real-world data\\nimperfections. Each modality has a unique noise topology, which determines the distribution of\\nnoise and imperfections that it commonly encounters. For example, images are susceptible to blurs\\nand shifts, typed text is susceptible to typos following keyboard positions, and multimodal time-\\nseries data is susceptible to correlated imperfections across synchronized time steps. Liang et al .\\n[151] collect a comprehensive set of targeted noisy distributions unique to each modality. In addition\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 20}), Document(page_content='1:22 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\nInformation\\nBiases\\nNoise\\n%%%&\\nGeneralization\\nO\\nptimization\\nTradeoffs\\nEpoch\\nLoss\\nFig. 18. The subchallenge of heterogeneity quantification aims to understand the dimensions of heterogene-\\nity commonly encountered in multimodal research, such as (1) different quantities and usages of modality\\ninformation , (2) the presence of modality biases , and (3) quantifying and mitigating modality noise .\\nInformation\\nBiases\\nNoise\\n%!%\"\\nGeneralization\\nOptimization\\nTradeoffs\\nEpoch\\nLoss\\nConnections\\nInteractions!signalsresponse\\ninference\\nFig. 19. Quantifying modality interconnections studies (1) connections : can we discover what modality\\nelements are related to each other and why, and (2) interactions : can we understand how modality elements\\ninteract during inference?\\nto natural noise topologies, related work has also explored adversarial attacks [ 66] and distribution\\nshifts [ 73] in multimodal systems. There has also been some progress in accounting for noisy or\\nmissing modalities by modality imputation using probabilistic models [ 163], autoencoders [ 259],\\ntranslation models [ 202], or low-rank approximations [ 148]. However, they run the risk of possible\\nerror compounding and require knowing which modalities are imperfect beforehand.\\n8.2 Subchallenge 6b: Modality Interconnections\\nModality connections and interactions are an essential component of multimodal models, which\\nhas inspired an important line of work in visualizing and understanding the nature of modality\\ninterconnections in datasets and trained models. We divide recent work into quantification of (1)\\nconnections : how modalities are related and share commonality, and (2) interactions : how modality\\nelements interact during inference (Figure 19).\\nConnections : Recent work has explored the quantification of modality connections through\\nvisualization tools on joint representation spaces [ 112] or attention maps [ 3]. Perturbation-based\\nanalysis perturbs the input and observes changes in the output to understand internal connec-\\ntions [ 149,189]. Finally, specifically curated diagnostic datasets are also useful in understanding\\nsemantic connections: Winoground [ 255] probes vision and language models for visio-linguistic\\ncompositionality, and PaintSkills [57] measures the connections necessary for visual reasoning.\\nInteractions : One common categorization of interactions involves redundancy, uniqueness, and\\nsynergy [ 290]. Redundancy describes task-relevant information shared among features, uniqueness\\nstudies the task-relevant information present in only one of the features, and synergy investigates\\nthe emergence of new information when both features are present. From a statistical perspective,\\nmeasures of redundancy include mutual information [ 22,33] and contrastive learning estima-\\ntors [ 258,264]. Other approaches have studied these measures in isolation, such as redundancy\\nvia distance between prediction logits using either feature [ 173], statistical distribution tests on\\ninput features [ 21], or via human annotations [ 217]. From the semantic view, recent work in Causal\\nVQA [ 4] and Counterfactual VQA [ 189] seek to understand the interactions captured by trained\\nmodels by measuring their robustness under controlled semantic edits to the question or image.\\nFinally, recent work has formalized definitions of non-additive interactions to quantify their pres-\\nence in trained models [ 238,265]. Parallel research such as EMAP [ 94], DIME [ 162], M2Lens [ 285],\\nand MultiViz [ 149] aims to quantify the interactions in real-world multimodal datasets and models.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 21}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:23\\nInformation\\nBiases\\nNoise\\n%%%&\\nGeneralization\\nO\\nptimization\\nTradeoffs\\nEpoch\\nLoss\\nFig. 20. Studying the multimodal learning process involves understanding (1) generalization across modali-\\nties and tasks, (2) optimization for balanced and efficient training, and (3) tradeoffs between performance,\\nrobustness, and complexity in the real-world deployment of multimodal models.\\n8.3 Subchallenge 6c: Multimodal Learning Process\\nFinally, there is a need to characterize the learning and optimization challenges involved when\\nlearning from heterogeneous data. This section covers recent work in (1) generalization across\\nmodalities and tasks, (2) better optimization for balanced and efficient training, and (3) balancing the\\ntradeoffs between performance, robustness, and complexity in real-world deployment (Figure 20).\\nGeneralization : With advances in sensing technologies, many real-world platforms such as\\ncellphones, smart devices, self-driving cars, healthcare technologies, and robots now integrate a\\nmuch larger number of sensors beyond the prototypical text, video, and audio modalities [ 108].\\nRecent work has studied generalization across paired modality inputs [ 152,206] and in unpaired\\nscenarios where each task is defined over only a small subset of all modalities [150, 160, 212].\\nOptimization challenges : Related work has also explored the optimization challenges of multi-\\nmodal learning, where multimodal networks are often prone to overfitting due to increased capacity,\\nand different modalities overfit and generalize at different rates so training them jointly with a\\nsingle optimization strategy is sub-optimal [ 284]. Subsequent work has suggested both empirical\\nand theoretical studies of why joint training of multimodal networks may be difficult and has\\nproposed methods to improve the optimization process via weighting approaches [293].\\nModality Tradeoffs : In real-world deployment, a balance between performance, robustness,\\nand complexity is often required. Therefore, one often needs to balance the utility of additional\\nmodalities with the additional complexity in data collection and modeling [ 151] as well as increased\\nsusceptibility to noise and imperfection in the additional modality [ 202]. How can we formally\\nquantify the utility and risks of each input modality, while balancing these tradeoffs for reliable real-\\nworld usage? There have been several attempts toward formalizing the semantics of a multimodal\\nrepresentation and how these benefits can transfer to downstream tasks [ 147,253,264], while\\ninformation-theoretic arguments have also provided useful insights [33, 239].\\n9 CONCLUSION\\nThis paper defined three core principles of modality heterogeneity, connections, and interactions\\ncentral to multimodal machine learning research, before proposing a taxonomy of six core technical\\nchallenges: representation, alignment, reasoning, generation, transference, and quantification\\ncovering historical and recent directions. Despite the immense opportunities afforded by recent\\nprogress in multimodal machine learning, there remain many unsolved challenges from theoretical,\\ncomputational, and application perspectives:\\n9.1 Future Directions\\nRepresentation :Theoretical and empirical frameworks . How can we formally define the three\\ncore principles of heterogeneity, connections, and interactions? What mathematical or empirical\\nframeworks will enable us to taxonomize the dimensions of heterogeneity and interconnections,\\nand subsequently quantify their presence in multimodal datasets and models? Answering these\\nfundamental questions will lead to a better understanding of the capabilities and limitations of\\ncurrent multimodal representations. Beyond additive and multiplicative cross-modal interactions .\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 22}), Document(page_content='1:24 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\nWhile recent work has been successful at modeling multiplicative interactions of increasing order,\\nhow can we capture causal, logical, and temporal connections and interactions? What is the right\\ntype of data and domain knowledge necessary to model these interactions? Brain and multimodal\\nperception . There are many core insights regarding multimodal processing to be gained from the\\nbrain and human cognition, including the brain’s neural architecture [ 34], intrinsic multimodal\\nproperties [ 130], mental imagery [ 183], and the nature of neural signals [ 194]. How does the human\\nbrain represent different modalities, how is multisensory integration performed, and how can\\nthese insights inform multimodal learning? In the other direction, what are several challenges and\\nopportunities in processing high-resolution brain signals such as fMRI and MEG/EEG, and how\\ncan multimodal learning help in the future analysis of data collected in neuroscience?\\nAlignment :Memory and long-term interactions . Many current multimodal benchmarks only\\nhave a short temporal dimension, which has limited the demand for models that can accurately\\nprocess long-range sequences and learn long-range interactions. Capturing long-term interactions\\npresents challenges since it is difficult to semantically relate information when they occur very\\nfar apart in time or space and raises complexity issues. How can we design models (perhaps with\\nmemory mechanisms) to ensure that these long-term cross-modal interactions are captured?\\nReasoning :Multimodal compositionality . How can we understand the reasoning process of\\ntrained models, especially in regard to how they combine information from modality elements?\\nThis challenge of compositional generalization is difficult since many compositions of elements\\nare typically not present during training, and the possible number of compositions increases\\nexponentially with the number of elements [ 255]. How can we best test for compositionality, and\\nwhat reasoning approaches can enable compositional generalization?\\nTransference :High-modality learning aims to learn representations from an especially large\\nnumber of heterogeneous data sources, which is a common feature of many real-world multimodal\\nsystems such as self-driving cars and IoT [ 108]. More modalities introduce more dimensions of\\nheterogeneity, incur complexity challenges in unimodal and multimodal processing, and require\\ndealing with non-parallel data (i.e., not all modalities are present at the same time).\\nGeneration :Creation and real-world ethical concerns . The complete synchronized creation of\\nrealistic video, text, and audio remains a challenge. Furthermore, the recent success in modality\\ngeneration has brought ethical concerns regarding their use. For example, large-scale pretrained\\nlanguage models can potentially generate text denigrating to particular social groups [ 229], toxic\\nspeech [ 79], and sensitive pretraining data [ 44]. Future work should study how these risks are\\npotentially amplified or reduced when the dataset is multimodal, and whether there are ethical\\nissues specific to multimodal generation.\\nQuantification :Modality utility, tradeoffs, and selection . How can we formalize why modalities\\ncan be useful for a task, and the potential reasons a modality can be harmful? Can we come up with\\nformal guidelines to compare these tradeoffs and subsequently select modalities? Explainability\\nand interpretability . Before models can be safely used by real-world stakeholders such as doctors,\\neducators, or policymakers, we need to understand the taxonomy of multimodal phenomena\\nin datasets and trained models we should aim to interpret. How can we evaluate whether these\\nphenomena are accurately interpreted? These challenges are exacerbated for relatively understudied\\nmodalities beyond language and vision, where the modalities themselves are not easy to visualize.\\nFinally, how can we tailor these explanations, possibly in a human-in-the-loop manner, to inform\\nreal-world decision-making? There are also core challenges in understanding and quantifying\\nmodality and social biases as well as robustness to imperfect, noisy, and out-of-distribution modalities.\\nIn conclusion, we believe that our taxonomy will help to catalog future research papers and\\nbetter understand the remaining unresolved problems in multimodal machine learning.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 23}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:25\\nACKNOWLEDGEMENTS\\nThis material is based upon work partially supported by the National Science Foundation (Awards\\n#1722822 and #1750439), National Institutes of Health (Awards #R01MH125740, #R01MH096951,\\nand #U01MH116925), BMW of North America, and Meta. PPL is partially supported by a Facebook\\nPhD Fellowship and a Carnegie Mellon University’s Center for Machine Learning and Health\\nFellowship. Any opinions, findings, conclusions, or recommendations expressed in this material are\\nthose of the author(s) and do not necessarily reflect the views of the National Science Foundation,\\nNational Institutes of Health, BMW of North America, Facebook, or Carnegie Mellon University’s\\nCenter for Machine Learning and Health, and no official endorsement should be inferred. We are\\nextremely grateful to Alex Wilf, Arav Agarwal, Catherine Cheng, Chaitanya Ahuja, Daniel Fried,\\nDong Won Lee, Jack Hessel, Leena Mathur, Lenore Blum, Manuel Blum, Martin Ma, Peter Wu,\\nRichard Chen, Ruslan Salakhutdinov, Santiago Benoit, Su Min Park, Torsten Wortwein, Victoria\\nLin, Volkan Cirik, Yao-Hung Hubert Tsai, Yejin Choi, Yiwei Lyu, Yonatan Bisk, and Youssouf Kebe\\nfor helpful discussions and feedback on initial versions of this paper.\\nREFERENCES\\n[1]Mahdi Abavisani and Vishal M Patel. 2018. Deep multimodal subspace clustering networks. IEEE Journal of Selected\\nTopics in Signal Processing 12, 6 (2018), 1601–1614.\\n[2]Abubakar Abid, Maheen Farooqi, and James Zou. 2021. Persistent anti-muslim bias in large language models. In\\nProceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society . 298–306.\\n[3]Estelle Aflalo, Meng Du, Shao-Yen Tseng, Yongfei Liu, Chenfei Wu, Nan Duan, and Vasudev Lal. 2022. VL-InterpreT:\\nAn Interactive Visualization Tool for Interpreting Vision-Language Transformers. In CVPR . 21406–21415.\\n[4]Vedika Agarwal, Rakshith Shetty, and Mario Fritz. 2020. Towards causal vqa: Revealing and reducing spurious\\ncorrelations by invariant and covariant semantic editing. In CVPR . 9690–9698.\\n[5]Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016. Analyzing the Behavior of Visual Question Answering\\nModels. In EMNLP . 1955–1960.\\n[6]Chaitanya Ahuja, Dong Won Lee, Yukiko I Nakano, and Louis-Philippe Morency. 2020. Style transfer for co-speech\\ngesture animation: A multi-speaker conditional-mixture approach. In ECCV . Springer, 248–265.\\n[7]Chaitanya Ahuja and Louis-Philippe Morency. 2019. Language2pose: Natural language grounded pose forecasting. In\\n3DV. IEEE, 719–728.\\n[8]Hana Ajakan, Pascal Germain, Hugo Larochelle, François Laviolette, and Mario Marchand. 2014. Domain-adversarial\\nneural networks. arXiv preprint arXiv:1412.4446 (2014).\\n[9]Hassan Akbari, Liangzhe Yuan, Rui Qian, et al .2021. Vatt: Transformers for multimodal self-supervised learning\\nfrom raw video, audio and text. arXiv preprint arXiv:2104.11178 (2021).\\n[10] Mehmet Aktukmak, Yasin Yilmaz, and Ismail Uysal. 2019. A probabilistic framework to incorporate mixed-data type\\nfeatures: Matrix factorization with multimodal side information. Neurocomputing 367 (2019), 164–175.\\n[11] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, et al .2022. Flamingo: a\\nvisual language model for few-shot learning. arXiv preprint arXiv:2204.14198 (2022).\\n[12] Camila Alviar, Rick Dale, Akeiylah Dewitt, and Christopher Kello. 2020. Multimodal coordination of sound and\\nmovement in music and speech. Discourse Processes 57, 8 (2020), 682–702.\\n[13] Saeed Amizadeh, Hamid Palangi, Alex Polozov, Yichen Huang, and Kazuhito Koishida. 2020. Neuro-Symbolic Visual\\nReasoning: Disentangling Visual from Reasoning. In ICML . PMLR, 279–290.\\n[14] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. Spice: Semantic propositional image\\ncaption evaluation. In European conference on computer vision . Springer, 382–398.\\n[15] Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2016. Neural module networks. In CVPR . 39–48.\\n[16] Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. 2013. Deep canonical correlation analysis. In ICML .\\n[17] Xavier Anguera, Jordi Luque, and Ciro Gracia. 2014. Audio-to-text alignment for speech recognition with very limited\\nresources. In Fifteenth Annual Conference of the International Speech Communication Association .\\n[18] John Arevalo, Thamar Solorio, Manuel Montes-y Gómez, and Fabio A González. 2017. Gated multimodal units for\\ninformation fusion. arXiv preprint arXiv:1702.01992 (2017).\\n[19] Pradeep K Atrey, M Anwar Hossain, Abdulmotaleb El Saddik, and Mohan S Kankanhalli. 2010. Multimodal fusion for\\nmultimedia analysis: a survey. Multimedia systems 16, 6 (2010), 345–379.\\n[20] Sören Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. Dbpedia: A\\nnucleus for a web of open data. In The semantic web . Springer, 722–735.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 24}), Document(page_content='1:26 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n[21] Benjamin Auffarth, Maite López, and Jesús Cerquides. 2010. Comparison of redundancy and relevance measures for\\nfeature selection in tissue classification of CT images. In Industrial conference on data mining . Springer, 248–262.\\n[22] Maria-Florina Balcan, Avrim Blum, and Ke Yang. 2004. Co-training and expansion: Towards bridging theory and\\npractice. NeurIPS 17 (2004).\\n[23] Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2018. Multimodal machine learning: A survey and\\ntaxonomy. IEEE TPAMI 41, 2 (2018), 423–443.\\n[24] George Barnum, Sabera J Talukder, and Yisong Yue. 2020. On the Benefits of Early Fusion in Multimodal Representation\\nLearning. In NeurIPS 2020 Workshop SVRHM .\\n[25] Reuben M Baron and David A Kenny. 1986. The moderator–mediator variable distinction in social psychological\\nresearch: Conceptual, strategic, and statistical considerations. Journal of personality and social psychology (1986).\\n[26] Roland Barthes. 1977. Image-music-text . Macmillan.\\n[27] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. 2006. Analysis of representations for domain\\nadaptation. NeurIPS 19 (2006).\\n[28] Hedi Ben-Younes, Rémi Cadene, Matthieu Cord, and Nicolas Thome. 2017. Mutan: Multimodal tucker fusion for\\nvisual question answering. In ICCV . 2612–2620.\\n[29] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of\\nStochastic Parrots: Can Language Models Be Too Big?. In FaaCT . 610–623.\\n[30] Yoshua Bengio, Aaron Courville, and Pascal Vincent. 2013. Representation Learning: A Review and New Perspectives.\\nTPAMI 35, 8 (Aug. 2013).\\n[31] Abeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. 2021. Multimodal datasets: misogyny, pornography,\\nand malignant stereotypes. arXiv preprint arXiv:2110.01963 (2021).\\n[32] Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki\\nLazaridou, Jonathan May, Aleksandr Nisnevich, et al. 2020. Experience Grounds Language. In EMNLP . 8718–8735.\\n[33] Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In COLT . 92–100.\\n[34] Lenore Blum and Manuel Blum. 2022. A theory of consciousness from a theoretical computer science perspective:\\nInsights from the Conscious Turing Machine. Proceedings of the National Academy of Sciences (2022).\\n[35] Kurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created\\ngraph database for structuring human knowledge. In ACM SIGMOD . 1247–1250.\\n[36] Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer\\nprogrammer as woman is to homemaker? debiasing word embeddings. In NeurIPS . 4349–4357.\\n[37] Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, et al .2017. HoME: a Household Multimodal\\nEnvironment. In NIPS 2017’s Visually-Grounded Interaction and Language Workshop .\\n[38] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veličković. 2021. Geometric deep learning: Grids, groups,\\ngraphs, geodesics, and gauges. arXiv preprint arXiv:2104.13478 (2021).\\n[39] Joy Buolamwini and Timnit Gebru. 2018. Gender shades: Intersectional accuracy disparities in commercial gender\\nclassification. In Conference on fairness, accountability and transparency . PMLR, 77–91.\\n[40] Qiong Cai, Hao Wang, Zhenmin Li, and Xiao Liu. 2019. A survey on multimodal data-driven smart healthcare systems:\\napproaches and applications. IEEE Access 7 (2019), 133583–133599.\\n[41] Juan C Caicedo and Fabio A González. 2012. Online matrix factorization for multimodal image retrieval. In Iberoamer-\\nican Congress on Pattern Recognition . Springer, 340–347.\\n[42] Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, and Jingjing Liu. 2020. Behind the scene: Revealing the\\nsecrets of pre-trained vision-and-language models. In European Conference on Computer Vision . Springer, 565–580.\\n[43] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Qiang Yang. 2017. Transitive hashing network for heterogeneous\\nmultimedia retrieval. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence . 81–87.\\n[44] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, et al .2021. Extracting training\\ndata from large language models. In USENIX Security . 2633–2650.\\n[45] Arjun Chandrasekaran, Viraj Prabhu, Deshraj Yadav, Prithvijit Chattopadhyay, and Devi Parikh. 2018. Do explanations\\nmake VQA models more predictable to a human?. In EMNLP .\\n[46] Khyathi Raghavi Chandu, Yonatan Bisk, and Alan W Black. 2021. Grounding ‘Grounding’in NLP. In Findings of the\\nAssociation for Computational Linguistics: ACL-IJCNLP 2021 . 4283–4305.\\n[47] Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar Pasumarthi, Dheeraj Rajagopal, and Ruslan\\nSalakhutdinov. 2018. Gated-attention architectures for task-oriented language grounding. In AAAI .\\n[48] Brian Chen, Andrew Rouditchenko, Kevin Duarte, Hilde Kuehne, Samuel Thomas, Angie Boggust, et al .2021.\\nMultimodal clustering networks for self-supervised learning from unlabeled videos. In ICCV . 8012–8021.\\n[49] Jun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. 2021. VisualGPT: Data-efficient adaptation of\\npretrained language models for image captioning. arXiv preprint arXiv:2102.10407 (2021).\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 25}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:27\\n[50] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. 2021. GeoQA: A\\nGeometric Question Answering Benchmark Towards Multimodal Numerical Reasoning. In ACL-IJCNLP Findings .\\n[51] Jingqiang Chen and Hai Zhuge. 2018. Abstractive Text-Image Summarization Using Multi-Modal Attentional\\nHierarchical RNN. In EMNLP .\\n[52] Jingqiang Chen and Hai Zhuge. 2018. Extractive Text-Image Summarization Using Multi-Modal RNN. In 2018 14th\\nInternational Conference on Semantics, Knowledge and Grids (SKG) . IEEE, 245–248.\\n[53] Lele Chen, Guofeng Cui, Ziyi Kou, Haitian Zheng, and Chenliang Xu. 2020. What comprises a good talking-head\\nvideo generation?: A survey and benchmark. arXiv preprint arXiv:2005.03201 (2020).\\n[54] Liqun Chen, Zhe Gan, Yu Cheng, Linjie Li, Lawrence Carin, and Jingjing Liu. 2020. Graph optimal transport for\\ncross-domain alignment. In ICML . PMLR, 1542–1553.\\n[55] Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Baltrušaitis, Amir Zadeh, and Louis-Philippe Morency. 2017. Multi-\\nmodal sentiment analysis with word-level fusion and reinforcement learning. In ICMI . 163–171.\\n[56] Yanhua Cheng, Xin Zhao, Rui Cai, Zhiwei Li, Kaiqi Huang, Yong Rui, et al .2016. Semi-Supervised Multimodal Deep\\nLearning for RGB-D Object Recognition.. In IJCAI . 3345–3351.\\n[57] Jaemin Cho, Abhay Zala, and Mohit Bansal. 2022. Dall-eval: Probing the reasoning skills and social biases of\\ntext-to-image generative transformers. arXiv preprint arXiv:2202.04053 (2022).\\n[58] Volkan Cirik, Taylor Berg-Kirkpatrick, and Louis-Philippe Morency. 2018. Using syntax to ground referring expressions\\nin natural images. In AAAI , Vol. 32.\\n[59] Volkan Cirik, Taylor Berg-Kirkpatrick, and L-P Morency. 2020. Refer360: A Referring Expression Recognition Dataset\\nin 360 Images. In ACL.\\n[60] Volkan Cirik, Louis-Philippe Morency, and Taylor Berg-Kirkpatrick. 2018. Visual Referring Expression Recognition:\\nWhat Do Systems Actually Learn?. In NAACL . 781–787.\\n[61] Emilie Delaherche and Mohamed Chetouani. 2010. Multimodal coordination: exploring relevant features and measures.\\nInProceedings of the 2nd international workshop on Social signal processing . 47–52.\\n[62] Mingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing, and Zhiting Hu. 2021. Compression, Transduction, and\\nCreation: A Unified Framework for Evaluating Natural Language Generation. In EMNLP . 7580–7605.\\n[63] Emily Denton and Rob Fergus. 2018. Stochastic video generation with a learned prior. In ICML . PMLR, 1174–1183.\\n[64] Laurence Devillers, Laurence Vidrascu, and Lori Lamel. 2005. Challenges in real-life emotion annotation and machine\\nlearning based detection. Neural Networks 18, 4 (2005), 407–422.\\n[65] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding. In NAACL-HLT (1) .\\n[66] Wenhao Ding, Baiming Chen, Bo Li, Kim Ji Eun, and Ding Zhao. 2021. Multimodal safety-critical scenarios generation\\nfor decision-making algorithms evaluation. IEEE Robotics and Automation Letters 6, 2 (2021), 1551–1558.\\n[67] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, et al .2021. An image is\\nworth 16x16 words: Transformers for image recognition at scale. ICLR (2021).\\n[68] Jared A Dunnmon, Alexander J Ratner, Khaled Saab, Nishith Khandwala, Matthew Markert, Hersh Sagreiya, Roger\\nGoldman, et al. 2020. Cross-modal data programming enables rapid medical machine learning. Patterns (2020).\\n[69] Chris Dyer. 2014. Notes on noise contrastive estimation and negative sampling. arXiv preprint arXiv:1410.8251 (2014).\\n[70] Georgios Evangelopoulos, Athanasia Zlatintsi, Alexandros Potamianos, et al .2013. Multimodal saliency and fusion\\nfor movie summarization based on aural, visual, and textual attention. IEEE Transactions on Multimedia (2013).\\n[71] Haoqi Fan and Jiatong Zhou. 2018. Stacked latent attention for multimodal reasoning. In CVPR . 1072–1080.\\n[72] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and\\nDavid Forsyth. 2010. Every picture tells a story: Generating sentences from images. In ECCV . Springer, 15–29.\\n[73] Andreas Foltyn and Jessica Deuschel. 2021. Towards Reliable Multimodal Stress Detection under Distribution Shift.\\nInCompanion Publication of the 2021 International Conference on Multimodal Interaction . 329–333.\\n[74] Jerome H Friedman and Bogdan E Popescu. 2008. Predictive learning via rule ensembles. The annals of applied\\nstatistics 2, 3 (2008), 916–954.\\n[75] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013.\\nDevise: A deep visual-semantic embedding model. In Advances in neural information processing systems . 2121–2129.\\n[76] Akira Fukui, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. Multimodal\\nCompact Bilinear Pooling for Visual Question Answering and Visual Grounding. In EMNLP . ACL, 457–468.\\n[77] Konrad Gadzicki, Razieh Khamsehashari, and Christoph Zetzsche. 2020. Early vs late fusion in multimodal convolu-\\ntional neural networks. In 2020 IEEE 23rd International Conference on Information Fusion (FUSION) . IEEE, 1–6.\\n[78] Itai Gat, Idan Schwartz, and Alex Schwing. 2021. Perceptual Score: What Data Modalities Does Your Model Perceive?\\nNeurIPS 34 (2021).\\n[79] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. RealToxicityPrompts:\\nEvaluating Neural Toxic Degeneration in Language Models. In EMNLP Findings . 3356–3369.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 26}), Document(page_content='1:28 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n[80] Robert Geirhos, Jörn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and\\nFelix A Wichmann. 2020. Shortcut learning in deep neural networks. Nature Machine Intelligence (2020).\\n[81] Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are We Modeling the Task or the Annotator? An Investigation\\nof Annotator Bias in Natural Language Understanding Datasets. In EMNLP-IJCNLP . 1161–1166.\\n[82] Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. 2020. Vqa-lol: Visual question answering under the\\nlens of logic. In European conference on computer vision . Springer, 379–396.\\n[83] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. Making the v in vqa matter:\\nElevating the role of image understanding in visual question answering. In CVPR . 6904–6913.\\n[84] Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Batra. 2016. Towards transparent ai systems: Interpreting\\nvisual question answering models. arXiv preprint arXiv:1608.08974 (2016).\\n[85] Edouard Grave, Armand Joulin, and Quentin Berthet. 2019. Unsupervised alignment of embeddings with wasserstein\\nprocrustes. In The 22nd International Conference on Artificial Intelligence and Statistics . PMLR, 1880–1890.\\n[86] Liangke Gui, Borui Wang, Qiuyuan Huang, Alex Hauptmann, Yonatan Bisk, and Jianfeng Gao. 2021. KAT: A Knowledge\\nAugmented Transformer for Vision-and-Language. arXiv preprint arXiv:2112.08614 (2021).\\n[87] Matthieu Guillaumin, Jakob Verbeek, and Cordelia Schmid. 2010. Multimodal semi-supervised learning for image\\nclassification. In 2010 IEEE Computer society conference on computer vision and pattern recognition . IEEE, 902–909.\\n[88] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.\\n2018. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR . 3608–3617.\\n[89] Jeffrey T Hancock and Jeremy N Bailenson. 2021. The social impact of deepfakes. , 149–152 pages.\\n[90] Sanjay Haresh, Sateesh Kumar, Huseyin Coskun, Shahram N Syed, Andrey Konin, Zeeshan Zia, and Quoc-Huy Tran.\\n2021. Learning by aligning videos in time. In CVPR . 5548–5558.\\n[91] Md Kamrul Hasan, Sangwu Lee, Wasifur Rahman, Amir Zadeh, Rada Mihalcea, Louis-Philippe Morency, and Ehsan\\nHoque. 2021. Humor knowledge enriched transformer for understanding multimodal humor. In AAAI .\\n[92] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. 2018. Women also snowboard:\\nOvercoming bias in captioning models. In Proceedings of the European Conference on Computer Vision (ECCV) . 771–787.\\n[93] Lisa Anne Hendricks, John Mellor, Rosalia Schneider, Jean-Baptiste Alayrac, and Aida Nematzadeh. 2021. Decoupling\\nthe role of data, attention, and losses in multimodal transformers. arXiv preprint arXiv:2102.00529 (2021).\\n[94] Jack Hessel and Lillian Lee. 2020. Does my multimodal model learn cross-modal interactions? It’s harder to tell than\\nyou might think!. In EMNLP .\\n[95] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and\\nAlexander Lerchner. 2016. beta-vae: Learning basic visual concepts with a constrained variational framework. (2016).\\n[96] Ryota Hinami, Junwei Liang, Shin’ichi Satoh, and Alexander Hauptmann. 2018. Multimodal Co-Training for Selecting\\nGood Examples from Webly Labeled Video. arXiv preprint arXiv:1804.06057 (2018).\\n[97] Richang Hong, Daqing Liu, Xiaoyu Mo, Xiangnan He, and Hanwang Zhang. 2019. Learning to compose and reason\\nwith language tree structures for visual grounding. IEEE TPAMI (2019).\\n[98] Ming Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong, and Qibin Zhao. 2019. Deep multimodal multilinear fusion\\nwith high-order polynomial pooling. NeurIPS 32 (2019), 12136–12145.\\n[99] Tsung-Yu Hsieh, Yiwei Sun, Suhang Wang, and Vasant Honavar. 2019. Adaptive structural co-regularization for\\nunsupervised multi-view feature selection. In 2019 IEEE International Conference on Big Knowledge (ICBK) . IEEE.\\n[100] Tzu-Ming Harry Hsu, Wei-Hung Weng, Willie Boag, Matthew McDermott, and Peter Szolovits. 2018. Unsupervised\\nmultimodal representation learning across medical images and reports. arXiv preprint arXiv:1811.08615 (2018).\\n[101] Wei-Ning Hsu and James Glass. 2018. Disentangling by Partitioning: A Representation Learning Framework for\\nMultimodal Sensory Data. arXiv preprint arXiv:1805.11264 (2018).\\n[102] Di Hu, Feiping Nie, and Xuelong Li. 2019. Deep multimodal clustering for unsupervised audiovisual learning. In\\nCVPR . 9248–9257.\\n[103] Peng Hu, Dezhong Peng, Xu Wang, and Yong Xiang. 2019. Multimodal adversarial network for cross-modal retrieval.\\nKnowledge-Based Systems 180 (2019), 38–50.\\n[104] Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. 2017. Learning to reason: End-to-end\\nmodule networks for visual question answering. In ICCV . 804–813.\\n[105] Ronghang Hu, Huazhe Xu, Marcus Rohrbach, Jiashi Feng, Kate Saenko, and Trevor Darrell. 2016. Natural language\\nobject retrieval. In CVPR . 4555–4564.\\n[106] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language Models as Zero-Shot Planners:\\nExtracting Actionable Knowledge for Embodied Agents. arXiv preprint arXiv:2201.07207 (2022).\\n[107] Xin Huang, Yuxin Peng, and Mingkuan Yuan. 2017. Cross-modal common representation learning by hybrid transfer\\nnetwork. In Proceedings of the 26th International Joint Conference on Artificial Intelligence . 1893–1900.\\n[108] Zhenhua Huang, Xin Xu, Juan Ni, Honghao Zhu, and Cheng Wang. 2019. Multimodal representation learning for\\nrecommendation in Internet of Things. IEEE Internet of Things Journal 6, 6 (2019), 10675–10685.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 27}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:29\\n[109] Drew Hudson and Christopher D Manning. 2019. Learning by abstraction: The neural state machine. NeurIPS (2019).\\n[110] Drew A Hudson and Christopher D Manning. 2018. Compositional attention networks for machine reasoning. arXiv\\npreprint arXiv:1803.03067 (2018).\\n[111] Drew A Hudson and Christopher D Manning. 2019. Gqa: A new dataset for real-world visual reasoning and\\ncompositional question answering. In CVPR . 6700–6709.\\n[112] Masha Itkina, B. Ivanovic, Ransalu Senanayake, Mykel J. Kochenderfer, and Marco Pavone. 2020. Evidential Sparsifi-\\ncation of Multimodal Latent Spaces in Conditional Variational Autoencoders. ArXiv abs/2010.09164 (2020).\\n[113] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. 2021. Perceiver:\\nGeneral perception with iterative attention. arXiv preprint arXiv:2103.03206 (2021).\\n[114] Alejandro Jaimes and Nicu Sebe. 2007. Multimodal human–computer interaction: A survey. Computer vision and\\nimage understanding 108, 1-2 (2007), 116–134.\\n[115] Amir Jamaludin, Joon Son Chung, and Andrew Zisserman. 2019. You said that?: Synthesising talking faces from\\naudio. International Journal of Computer Vision 127, 11 (2019), 1767–1779.\\n[116] Anubhav Jangra, Adam Jatowt, Mohammad Hasanuzzaman, and Sriparna Saha. 2020. Text-Image-Video Summary\\nGeneration Using Joint Integer Linear Programming. In European Conference on Information Retrieval . Springer.\\n[117] Siddhant M. Jayakumar, Wojciech M. Czarnecki, Jacob Menick, Jonathan Schwarz, Jack Rae, Simon Osindero, Yee Whye\\nTeh, Tim Harley, and Razvan Pascanu. 2020. Multiplicative Interactions and Where to Find Them. In ICLR .\\n[118] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, et al .2021. Scaling up visual and vision-\\nlanguage representation learning with noisy text supervision. In ICML . PMLR, 4904–4916.\\n[119] Alistair EW Johnson, Tom J Pollard, Lu Shen, H Lehman Li-Wei, Mengling Feng, Mohammad Ghassemi, et al .2016.\\nMIMIC-III, a freely accessible critical care database. Scientific data 3, 1 (2016), 1–9.\\n[120] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017.\\nClevr: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR . 2901–2910.\\n[121] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit. 2017.\\nOne model to learn them all. arXiv preprint arXiv:1706.05137 (2017).\\n[122] Andrej Karpathy, Armand Joulin, and Li F Fei-Fei. 2014. Deep fragment embeddings for bidirectional image sentence\\nmapping. NeurIPS 27 (2014).\\n[123] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and\\nimproving the image quality of stylegan. In CVPR . 8110–8119.\\n[124] Vasil Khalidov, Florence Forbes, and Radu Horaud. 2011. Conjugate mixture models for clustering multimodal data.\\nNeural Computation (2011).\\n[125] Aparajita Khan and Pradipta Maji. 2019. Approximate graph Laplacians for multimodal data clustering. IEEE TPAMI\\n43, 3 (2019), 798–813.\\n[126] Minjae Kim, David K Han, and Hanseok Ko. 2016. Joint patch clustering-based dictionary learning for multimodal\\nimage fusion. Information Fusion 27 (2016), 198–214.\\n[127] Elsa A Kirchner, Stephen H Fairclough, and Frank Kirchner. 2019. Embedded multimodal interfaces in robotics:\\napplications, future trends, and societal implications. In The Handbook of Multimodal-Multisensor Interfaces: Language\\nProcessing, Software, Commercialization, and Emerging Directions-Volume 3 . 523–576.\\n[128] Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. 2021. Text-to-image generation grounded by fine-grained\\nuser attention. In WACV .\\n[129] Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and Percy Liang. 2020.\\nConcept bottleneck models. In ICML . PMLR, 5338–5348.\\n[130] Stephen M Kosslyn, Giorgio Ganis, and William L Thompson. 2010. Multimodal images in the brain. The neurophysi-\\nological foundations of mental and motor imagery (2010), 3–16.\\n[131] Satwik Kottur, José MF Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. 2018. Visual coreference resolution\\nin visual dialog using neural module networks. In ECCV . 153–169.\\n[132] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, et al .2017. Visual genome:\\nConnecting language and vision using crowdsourced dense image annotations. IJCV 123, 1 (2017), 32–73.\\n[133] Joseph B Kruskal. 1983. An overview of sequence comparison: Time warps, string edits, and macromolecules. SIAM\\nreview 25, 2 (1983), 201–237.\\n[134] Pei Ling Lai and Colin Fyfe. 2000. Kernel and nonlinear canonical correlation analysis. International Journal of Neural\\nSystems (2000).\\n[135] Rémi Lebret, Pedro Pinheiro, and Ronan Collobert. 2015. Phrase-based image captioning. In ICML . PMLR, 2085–2094.\\n[136] Michelle A Lee, Yuke Zhu, Krishnan Srinivasan, Parth Shah, Silvio Savarese, et al .2019. Making sense of vision and\\ntouch: Self-supervised learning of multimodal representations for contact-rich tasks. In ICRA . IEEE, 8943–8950.\\n[137] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. 2018. TVQA: Localized, Compositional Video Question Answering.\\nInEMNLP . 1369–1379.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 28}), Document(page_content='1:30 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n[138] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The Power of Scale for Parameter-Efficient Prompt Tuning. In\\nEMNLP . 3045–3059.\\n[139] Haoran Li, Junnan Zhu, Cong Ma, Jiajun Zhang, and Chengqing Zong. 2017. Multi-modal Summarization for\\nAsynchronous Collection of Text, Image, Audio and Video. In EMNLP . 1092–1102.\\n[140] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant\\nbaseline for vision and language. arXiv preprint arXiv:1908.03557 (2019).\\n[141] Mingzhe Li, Xiuying Chen, Shen Gao, Zhangming Chan, Dongyan Zhao, and Rui Yan. 2020. VMSMO: Learning to\\nGenerate Multimodal Summary for Video-based News Articles. arXiv preprint arXiv:2010.05406 (2020).\\n[142] Manling Li, Ruochen Xu, Shuohang Wang, Luowei Zhou, Xudong Lin, Chenguang Zhu, Michael Zeng, Heng Ji, and\\nShih-Fu Chang. 2022. Clip-event: Connecting text and images with event structures. In CVPR . 16420–16429.\\n[143] Manling Li, Lingyu Zhang, Heng Ji, and Richard J Radke. 2019. Keep meeting summaries on topic: Abstractive\\nmulti-modal meeting summarization. In ACL. 2190–2196.\\n[144] Qing Li, Boqing Gong, Yin Cui, Dan Kondratyuk, Xianzhi Du, et al .2021. Towards a Unified Foundation Model:\\nJointly Pre-Training Transformers on Unpaired Images and Text. arXiv preprint arXiv:2112.07074 (2021).\\n[145] Shuang Li, Xavier Puig, Yilun Du, Clinton Wang, Ekin Akyurek, Antonio Torralba, Jacob Andreas, and Igor Mordatch.\\n2022. Pre-Trained Language Models for Interactive Decision-Making. arXiv preprint arXiv:2202.01771 (2022).\\n[146] Xiang Lisa Li and Percy Liang. 2021. Prefix-Tuning: Optimizing Continuous Prompts for Generation. In ACL-IJCNLP .\\n[147] Paul Pu Liang. 2022. Brainish: Formalizing A Multimodal Language for Intelligence and Consciousness. arXiv preprint\\narXiv:2205.00001 (2022).\\n[148] Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, and Louis-Philippe Morency.\\n2019. Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization. In ACL.\\n[149] Paul Pu Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, et al .2023. MultiViz: Towards Visualizing and\\nUnderstanding Multimodal Models. In ICLR .\\n[150] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Shengtong Mo, Dani Yogatama, et al .2022. HighMMT: Towards Modality and\\nTask Generalization for High-Modality Representation Learning. arXiv preprint arXiv:2203.01311 (2022).\\n[151] Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Yufan Chen, et al .2021. MultiBench:\\nMultiscale Benchmarks for Multimodal Representation Learning. In NeurIPS Datasets and Benchmarks Track .\\n[152] Paul Pu Liang, Peter Wu, Liu Ziyin, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021. Cross-Modal General-\\nization: Learning in Low Resource Modalities via Meta-Alignment. In ACM Multimedia . 2680–2689.\\n[153] Valerii Likhosherstov, Mostafa Dehghani, Anurag Arnab, Krzysztof Marcin Choromanski, Mario Lucic, Yi Tay, and\\nAdrian Weller. 2022. PolyViT: Co-training Vision Transformers on Images, Videos and Audio.\\n[154] Bryan Lim, Sercan Ö Arık, Nicolas Loeff, and Tomas Pfister. 2021. Temporal fusion transformers for interpretable\\nmulti-horizon time series forecasting. International Journal of Forecasting (2021).\\n[155] Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-Aware Graph Networks for\\nCommonsense Reasoning. In EMNLP-IJCNLP . 2829–2839.\\n[156] Alex Liu, SouYoung Jin, Cheng-I Lai, Andrew Rouditchenko, Aude Oliva, and James Glass. 2022. Cross-Modal Discrete\\nRepresentation Learning. In ACL. 3013–3035.\\n[157] Ye Liu, Hui Li, Alberto Garcia-Duran, Mathias Niepert, Daniel Onoro-Rubio, and David S Rosenblum. 2019. MMKG:\\nmulti-modal knowledge graphs. In European Semantic Web Conference . Springer, 459–474.\\n[158] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, AmirAli Bagher Zadeh, and Louis-\\nPhilippe Morency. 2018. Efficient Low-rank Multimodal Fusion With Modality-Specific Factors. In ACL. 2247–2256.\\n[159] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic represen-\\ntations for vision-and-language tasks. In Advances in Neural Information Processing Systems . 13–23.\\n[160] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. 2021. Pretrained transformers as universal computation\\nengines. arXiv preprint arXiv:2103.05247 (2021).\\n[161] Jelena Luketina, Nantas Nardelli, Gregory Farquhar, Jakob N Foerster, Jacob Andreas, Edward Grefenstette, Shimon\\nWhiteson, and Tim Rocktäschel. 2019. A Survey of Reinforcement Learning Informed by Natural Language. In IJCAI .\\n[162] Yiwei Lyu, Paul Pu Liang, Zihao Deng, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2022. DIME: Fine-grained\\nInterpretations of Multimodal Models via Disentangled Local Explanations. arXiv preprint arXiv:2203.02013 (2022).\\n[163] Mengmeng Ma, Jian Ren, Long Zhao, Sergey Tulyakov, Cathy Wu, and Xi Peng. 2021. Smil: Multimodal learning with\\nseverely missing modality. arXiv preprint arXiv:2103.05677 (2021).\\n[164] Emiliano Macaluso and Jon Driver. 2005. Multisensory spatial interactions: a window onto functional integration in\\nthe human brain. Trends in neurosciences 28, 5 (2005), 264–271.\\n[165] T Soni Madhulatha. 2012. An overview on clustering methods. arXiv preprint arXiv:1205.1117 (2012).\\n[166] Tegan Maharaj, Nicolas Ballas, Anna Rohrbach, Aaron Courville, and Christopher Pal. 2017. A dataset and exploration\\nof models for understanding video data through fill-in-the-blank question-answering. In CVPR . 6884–6893.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 29}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:31\\n[167] Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nicholas Johnston, Andrew Rabinovich, and Kevin Murphy. 2015.\\nWhat’s Cookin’? Interpreting Cooking Videos using Text, Speech and Vision. In NAACL-HLT . 143–152.\\n[168] Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. 2018. The Neuro-Symbolic Concept\\nLearner: Interpreting Scenes, Words, and Sentences From Natural Supervision. In ICLR .\\n[169] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. 2016. Generation\\nand comprehension of unambiguous object descriptions. In CVPR . 11–20.\\n[170] Matthew Marge, Carol Espy-Wilson, Nigel G Ward, Abeer Alwan, Yoav Artzi, Mohit Bansal, et al .2022. Spoken\\nlanguage interaction with robots: Recommendations for future research. Computer Speech & Language (2022).\\n[171] Kenneth Marino, Ruslan Salakhutdinov, and Abhinav Gupta. 2017. The More You Know: Using Knowledge Graphs\\nfor Image Classification. In CVPR . IEEE, 20–28.\\n[172] Emily E Marsh and Marilyn Domas White. 2003. A taxonomy of relationships between images and text. Journal of\\ndocumentation (2003).\\n[173] Alessio Mazzetto, Dylan Sam, Andrew Park, Eli Upfal, and Stephen Bach. 2021. Semi-Supervised Aggregation of\\nDependent Weak Supervision Sources With Performance Guarantees. In AISTATS .\\n[174] Dalila Mekhaldi. 2007. Multimodal document alignment: towards a fully-indexed multimedia archive. In Proceedings\\nof the Multimedia Information Retrieval Workshop, SIGIR, Amsterdam, the Netherlands .\\n[175] Luke Melas-Kyriazi, Alexander M Rush, and George Han. 2018. Training for diversity in image paragraph captioning.\\nInProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 757–761.\\n[176] Luke Merrick and Ankur Taly. 2020. The explanation game: Explaining machine learning models using shapley\\nvalues. In International Cross-Domain Conference for Machine Learning and Knowledge Extraction . Springer, 17–38.\\n[177] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. 2020. Nerf:\\nRepresenting scenes as neural radiance fields for view synthesis. In European conference on computer vision . Springer.\\n[178] George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM 38, 11 (1995), 39–41.\\n[179] Olivier Morere, Hanlin Goh, Antoine Veillard, Vijay Chandrasekhar, and Jie Lin. 2015. Co-regularized deep represen-\\ntations for video summarization. In 2015 IEEE International Conference on Image Processing (ICIP) . IEEE, 3165–3169.\\n[180] Ghulam Muhammad, Fatima Alshehri, Fakhri Karray, Abdulmotaleb El Saddik, et al .2021. A comprehensive survey\\non multimodal medical signals fusion for smart healthcare systems. Information Fusion 76 (2021), 355–375.\\n[181] Jonathan Munro and Dima Damen. 2020. Multi-modal domain adaptation for fine-grained action recognition. In\\nCVPR . 122–132.\\n[182] Hyeonseob Nam, Jung-Woo Ha, and Jeonghee Kim. 2017. Dual attention networks for multimodal reasoning and\\nmatching. In CVPR . 299–307.\\n[183] Bence Nanay. 2018. Multimodal mental imagery. Cortex 105 (2018), 125–134.\\n[184] Milind Naphade, John R Smith, Jelena Tesic, Shih-Fu Chang, Winston Hsu, Lyndon Kennedy, Alexander Hauptmann,\\nand Jon Curtis. 2006. Large-scale concept ontology for multimedia. IEEE multimedia 13, 3 (2006), 86–91.\\n[185] Karthik Narasimhan, Regina Barzilay, and Tommi Jaakkola. 2018. Grounding language for transfer in deep reinforce-\\nment learning. Journal of Artificial Intelligence Research 63 (2018), 849–874.\\n[186] Shahla Nemati, Reza Rohani, Mohammad Ehsan Basiri, Moloud Abdar, Neil Y Yen, and Vladimir Makarenkov. 2019. A\\nhybrid latent space data fusion method for multimodal emotion recognition. IEEE Access 7 (2019), 172948–172964.\\n[187] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell, Angjoo Kanazawa, and Shiry Ginosar. 2022. Learning to\\nListen: Modeling Non-Deterministic Dyadic Facial Motion. In CVPR . 20395–20405.\\n[188] Maximilian Nickel, Kevin Murphy, Volker Tresp, and Evgeniy Gabrilovich. 2015. A review of relational machine\\nlearning for knowledge graphs. Proc. IEEE 104, 1 (2015), 11–33.\\n[189] Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. 2021. Counterfactual vqa: A\\ncause-effect look at language bias. In CVPR . 12700–12710.\\n[190] Zeljko Obrenovic and Dusan Starcevic. 2004. Modeling multimodal human-computer interaction. Computer (2004).\\n[191] Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol Vinyals, et al .2018. Parallel wavenet: Fast high-fidelity\\nspeech synthesis. In ICML . PMLR, 3918–3926.\\n[192] Christian Otto, Matthias Springstein, Avishek Anand, and Ralph Ewerth. 2020. Characterization and classification of\\nsemantic image-text relations. International Journal of Multimedia Information Retrieval 9 (2020), 31–45.\\n[193] Shruti Palaskar, Jindrich Libovick `y, Spandana Gella, and Florian Metze. 2019. Multimodal abstractive summarization\\nfor how2 videos. arXiv preprint arXiv:1906.07901 (2019).\\n[194] Simone Palazzo, Concetto Spampinato, Isaak Kavasidis, Daniela Giordano, Joseph Schmidt, and Mubarak Shah. 2020.\\nDecoding brain representations by multimodal learning of neural activity and visual features. IEEE TPAMI (2020).\\n[195] Dong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus\\nRohrbach. 2018. Multimodal explanations: Justifying decisions and pointing to the evidence. In CVPR . 8779–8788.\\n[196] Jae Sung Park, Chandra Bhagavatula, Roozbeh Mottaghi, Ali Farhadi, and Yejin Choi. 2020. Visualcomet: Reasoning\\nabout the dynamic context of a still image. In European Conference on Computer Vision . Springer, 508–524.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 30}), Document(page_content='1:32 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n[197] Sarah Partan and Peter Marler. 1999. Communication goes multimodal. Science 283, 5406 (1999), 1272–1273.\\n[198] Judea Pearl. 2009. Causality . Cambridge university press.\\n[199] Alejandro Peña, Ignacio Serna, Aythami Morales, and Julian Fierrez. 2020. FairCVtest Demo: Understanding Bias in\\nMultimodal Learning with a Testbed in Fair Automatic Recruitment. In ICMI . 760–761.\\n[200] Juan-Manuel Pérez-Rúa, Valentin Vielzeuf, Stéphane Pateux, Moez Baccouche, and Frédéric Jurie. 2019. Mfas:\\nMultimodal fusion architecture search. In CVPR . 6966–6975.\\n[201] Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. 2018. Embedding Multimodal Relational Data for Knowledge Base\\nCompletion. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing . 3208–3218.\\n[202] Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency, and Barnabás Póczos. 2019. Found in translation:\\nLearning robust joint representations by cyclic translations between modalities. In AAAI , Vol. 33. 6892–6899.\\n[203] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. 2015.\\nFlickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV .\\n[204] Soujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. 2017. A review of affective computing: From unimodal\\nanalysis to multimodal fusion. Information Fusion (2017).\\n[205] Shraman Pramanick, Aniket Roy, and Vishal M Patel. 2022. Multimodal Learning using Optimal Transport for Sarcasm\\nand Humor Detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision .\\n[206] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, et al .2021. Learning\\ntransferable visual models from natural language supervision. In ICML . PMLR, 8748–8763.\\n[207] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are\\nUnsupervised Multitask Learners. (2019).\\n[208] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, AmirAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe Morency,\\nand Ehsan Hoque. 2020. Integrating Multimodal Information in Large Pretrained Transformers. In ACL. 2359–2369.\\n[209] Shyam Sundar Rajagopalan, Louis-Philippe Morency, Tadas Baltrušaitis, and Roland Goecke. 2016. Extending long\\nshort-term memory for multi-view structured learning. In European Conference on Computer Vision .\\n[210] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\\n2021. Zero-shot text-to-image generation. In ICML . PMLR, 8821–8831.\\n[211] Nikhil Rasiwasia, Jose Costa Pereira, Emanuele Coviello, Gabriel Doyle, Gert R.G. Lanckriet, Roger Levy, and Nuno\\nVasconcelos. 2010. A new approach to cross-modal multimedia retrieval. In ACMMM . 251–260.\\n[212] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gómez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron,\\nMai Giménez, Yury Sulsky, et al. 2022. One model to learn them all. Deepmind Technical Report (2022).\\n[213] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. 2019. Fastspeech: Fast, robust and\\ncontrollable text to speech. NeurIPS 32 (2019).\\n[214] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \" Why should i trust you?\" Explaining the predictions\\nof any classifier. In KDD . 1135–1144.\\n[215] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image\\nsynthesis with latent diffusion models. In CVPR . 10684–10695.\\n[216] Candace Ross, Boris Katz, and Andrei Barbu. 2020. Measuring Social Biases in Grounded Vision and Language\\nEmbeddings. arXiv preprint arXiv:2002.08911 (2020).\\n[217] Natalie Ruiz, Ronnie Taib, and Fang Chen. 2006. Examining the redundancy of multimodal input. In Proceedings of\\nthe 18th Australia conference on Computer-Human Interaction: Design: Activities, Artefacts and Environments . 389–392.\\n[218] Shagan Sah, Sourabh Kulhare, Allison Gray, Subhashini Venugopalan, Emily Prud’Hommeaux, and Raymond Ptucha.\\n2017. Semantic text summarization of long videos. In WACV . IEEE, 989–997.\\n[219] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2020. Winogrande: An adversarial winograd\\nschema challenge at scale. In AAAI , Vol. 34. 8732–8740.\\n[220] Raeid Saqur and Karthik Narasimhan. 2020. Multimodal graph networks for compositional generalization in visual\\nquestion answering. NeurIPS 33 (2020), 3070–3081.\\n[221] Mehmet Emre Sargin, Yücel Yemez, Engin Erzin, and A. Murat Tekalp. 2007. Audiovisual synchronization and fusion\\nusing canonical correlation analysis. IEEE Transactions on Multimedia 9, 7 (2007), 1396–1403.\\n[222] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\\nLiu, Vladlen Koltun, Jitendra Malik, et al. 2019. Habitat: A platform for embodied ai research. In ICCV . 9339–9347.\\n[223] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. 2008. The graph\\nneural network model. IEEE transactions on neural networks 20, 1 (2008), 61–80.\\n[224] Manos Schinas, Symeon Papadopoulos, Georgios Petkos, Yiannis Kompatsiaris, and Pericles A Mitkas. 2015. Multi-\\nmodal graph-based event detection and summarization in social media streams. In ACM Multimedia .\\n[225] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra.\\n2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV . 618–626.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 31}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:33\\n[226] Luciano Serafini and Artur d’Avila Garcez. 2016. Logic tensor networks: Deep learning and logical reasoning from\\ndata and knowledge. arXiv preprint arXiv:1606.04422 (2016).\\n[227] Claude Elwood Shannon. 1948. A mathematical theory of communication. The Bell system technical journal (1948).\\n[228] Rajeev Sharma, Vladimir I Pavlović, and Thomas S Huang. 2002. Toward multimodal human–computer interface. In\\nAdvances in image processing and understanding: A Festschrift for Thomas S Huang . World Scientific, 349–365.\\n[229] Emily Sheng, Kai-Wei Chang, Prem Natarajan, and Nanyun Peng. 2019. The Woman Worked as a Babysitter: On\\nBiases in Language Generation. In EMNLP-IJCNLP . 3398–3403.\\n[230] Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and S Yu Philip. 2016. A survey of heterogeneous information\\nnetwork analysis. IEEE Transactions on Knowledge and Data Engineering 29, 1 (2016), 17–37.\\n[231] Yuge Shi, Brooks Paige, and Philip Torr. 2019. Variational mixture-of-experts autoencoders for multimodal deep\\ngenerative models. NeurIPS (2019).\\n[232] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. 2013. Deep inside convolutional networks: Visualising\\nimage classification models and saliency maps. arXiv preprint arXiv:1312.6034 (2013).\\n[233] Vikas Sindhwani, Partha Niyogi, and Mikhail Belkin. 2005. A co-regularization approach to semi-supervised learning\\nwith multiple views. In Proceedings of ICML workshop on learning with multiple views , Vol. 2005. Citeseer, 74–79.\\n[234] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, et al .2022. Make-a-video:\\nText-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 (2022).\\n[235] Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, et al .2021. FLAVA: A Foundational\\nLanguage And Vision Alignment Model. arXiv preprint arXiv:2112.04482 (2021).\\n[236] Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng. 2013. Zero-shot learning through cross-\\nmodal transfer. NeurIPS (2013).\\n[237] Dandan Song, Siyi Ma, Zhanchen Sun, Sicheng Yang, and Lejian Liao. 2021. Kvl-bert: Knowledge enhanced visual-\\nand-linguistic bert for visual commonsense reasoning. Knowledge-Based Systems 230 (2021), 107408.\\n[238] Daria Sorokina, Rich Caruana, Mirek Riedewald, and Daniel Fink. 2008. Detecting statistical interactions with additive\\ngroves of trees. In Proceedings of the 25th international conference on Machine learning . 1000–1007.\\n[239] Karthik Sridharan and Sham M Kakade. 2008. An information theoretic framework for multi-view learning. (2008).\\n[240] Tejas Srinivasan and Yonatan Bisk. 2021. Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language\\nModels. arXiv preprint arXiv:2104.08666 (2021).\\n[241] Fabian M Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2007. Yago: a core of semantic knowledge. In WWW .\\n[242] Alane Suhr and Yoav Artzi. 2019. Nlvr2 visual bias analysis. arXiv preprint arXiv:1909.10411 (2019).\\n[243] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. 2019. Videobert: A joint model for\\nvideo and language representation learning. In ICCV . 7464–7473.\\n[244] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An introduction . MIT press.\\n[245] Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman. 2017. Synthesizing obama: learning lip\\nsync from audio. ACM Transactions on Graphics (ToG) 36, 4 (2017), 1–13.\\n[246] Riko Suzuki, Hitomi Yanaka, Masashi Yoshikawa, Koji Mineshima, and Daisuke Bekki. 2019. Multimodal logical\\ninference system for visual-textual entailment. arXiv preprint arXiv:1906.03952 (2019).\\n[247] Fuwen Tan, Song Feng, and Vicente Ordonez. 2019. Text2scene: Generating compositional scenes from textual\\ndescriptions. In CVPR . 6710–6719.\\n[248] Hao Tan and Mohit Bansal. 2019. LXMERT: Learning Cross-Modality Encoder Representations from Transformers.\\nInEMNLP-IJCNLP . 5100–5111.\\n[249] Hao Tan and Mohit Bansal. 2020. Vokenization: Improving Language Understanding with Contextualized, Visual-\\nGrounded Supervision. In EMNLP . 2066–2080.\\n[250] Zhulin Tao, Yinwei Wei, Xiang Wang, Xiangnan He, Xianglin Huang, and Tat-Seng Chua. 2020. Mgat: Multimodal\\ngraph attention network for recommendation. Information Processing & Management 57, 5 (2020), 102277.\\n[251] Makarand Tapaswi, Martin Bauml, and Rainer Stiefelhagen. 2015. Book2movie: Aligning video scenes with book\\nchapters. In CVPR . 1827–1835.\\n[252] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2016.\\nMovieqa: Understanding stories in movies through question-answering. In CVPR . 4631–4640.\\n[253] Jesse Thomason, Jivko Sinapov, Maxwell Svetlik, Peter Stone, and Raymond J Mooney. 2016. Learning multi-modal\\ngrounded linguistic semantics by playing\" I Spy\". In IJCAI . 3477–3483.\\n[254] Bruce Thompson. 2000. Canonical correlation analysis. (2000).\\n[255] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. 2022.\\nWinoground: Probing Vision and Language Models for Visio-Linguistic Compositionality. In CVPR . 5238–5248.\\n[256] Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020. Contrastive multiview coding. In ECCV . Springer, 776–794.\\n[257] Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. 2020. What makes for good\\nviews for contrastive learning? NeurIPS 33 (2020), 6827–6839.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 32}), Document(page_content='1:34 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n[258] Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. 2021. Contrastive learning, multi-view redundancy, and\\nlinear models. In ALT.\\n[259] Luan Tran, Xiaoming Liu, Jiayu Zhou, and Rong Jin. 2017. Missing Modalities Imputation via Cascaded Residual\\nAutoencoder. In CVPR .\\n[260] George Trigeorgis, Mihalis A Nicolaou, Björn W Schuller, and Stefanos Zafeiriou. 2017. Deep canonical time warping\\nfor simultaneous alignment and representation learning of sequences. IEEE TPAMI 40, 5 (2017), 1128–1138.\\n[261] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov.\\n2019. Multimodal Transformer for Unaligned Multimodal Language Sequences. In ACL. 6558–6569.\\n[262] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Learning\\nfactorized multimodal representations. ICLR (2019).\\n[263] Yao-Hung Hubert Tsai, Martin Ma, Muqiao Yang, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2020. Multimodal\\nRouting: Improving Local and Global Interpretability of Multimodal Language Analysis. In EMNLP . 1823–1833.\\n[264] Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2020. Self-supervised Learning\\nfrom a Multi-view Perspective. In ICLR .\\n[265] Michael Tsang, Dehua Cheng, and Yan Liu. 2018. Detecting Statistical Interactions from Neural Network Weights. In\\nICLR .\\n[266] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. 2021. Multimodal few-shot\\nlearning with frozen language models. NeurIPS 34 (2021).\\n[267] Peter D Turney and Michael L Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine\\nLearning 60 (2005), 251–278.\\n[268] Len Unsworth and Chris Cléirigh. 2014. Multimodality and reading: The construction of meaning through image-text\\ninteraction. Routledge.\\n[269] Shagun Uppal, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumder, et al .2022. Multimodal research in vision\\nand language: A review of current and emerging trends. Information Fusion 77 (2022), 149–171.\\n[270] Naushad UzZaman, Jeffrey P Bigham, and James F Allen. 2011. Multimodal summarization of complex sentences. In\\nProceedings of the 16th international conference on Intelligent user interfaces . ACM, 43–52.\\n[271] Aaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. NeurIPS 30 (2017).\\n[272] Emile van Krieken, Erman Acar, and Frank van Harmelen. 2022. Analyzing differentiable fuzzy logic operators.\\nArtificial Intelligence (2022).\\n[273] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\\nPolosukhin. 2017. Attention is all you need. In Advances in neural information processing systems . 5998–6008.\\n[274] Ramakrishna Vedantam, Karan Desai, Stefan Lee, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. 2019. Probabilistic\\nneural symbolic models for interpretable visual question answering. In ICML . PMLR, 6428–6437.\\n[275] Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018. Graph\\nAttention Networks. In ICLR .\\n[276] Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. 2015. Order-embeddings of images and language. arXiv\\npreprint arXiv:1511.06361 (2015).\\n[277] René Vidal. 2011. Subspace clustering. IEEE Signal Processing Magazine 28, 2 (2011), 52–68.\\n[278] Cédric Villani. 2009. Optimal transport: old and new . Vol. 338. Springer.\\n[279] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2016. Show and tell: Lessons learned from the\\n2015 mscoco image captioning challenge. IEEE TPAMI (2016).\\n[280] Alvin Wan, Lisa Dunlap, Daniel Ho, Jihan Yin, Scott Lee, Suzanne Petryk, Sarah Adel Bargal, and Joseph E Gonzalez.\\n2020. NBDT: Neural-Backed Decision Tree. In ICLR .\\n[281] Meng Wang, Hao Li, Dacheng Tao, Ke Lu, and Xindong Wu. 2012. Multimodal graph-based reranking for web image\\nsearch. IEEE transactions on image processing 21, 11 (2012), 4649–4661.\\n[282] Tan Wang, Jianqiang Huang, Hanwang Zhang, and Qianru Sun. 2020. Visual commonsense representation learning\\nvia causal inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops .\\n[283] Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. 2015. On deep multi-view representation learning. In\\nICML . PMLR, 1083–1092.\\n[284] Weiyao Wang, Du Tran, and Matt Feiszli. 2020. What Makes Training Multi-Modal Classification Networks Hard?. In\\nCVPR . 12695–12705.\\n[285] Xingbo Wang, Jianben He, Zhihua Jin, et al .2021. M2Lens: Visualizing and explaining multimodal models for\\nsentiment analysis. IEEE Transactions on Visualization and Computer Graphics (2021).\\n[286] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, et al .2019. Reinforced cross-modal matching and\\nself-supervised imitation learning for vision-language navigation. In CVPR . 6629–6638.\\n[287] Jônatas Wehrmann, Anderson Mattjie, and Rodrigo C Barros. 2018. Order embeddings and character-level convolutions\\nfor multimodal alignment. Pattern Recognition Letters 102 (2018), 15–22.\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 33}), Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:35\\n[288] Xiaofan Wei, Huibin Li, Jian Sun, and Liming Chen. 2018. Unsupervised domain adaptation with regularized optimal\\ntransport for multimodal 2d+ 3d facial expression recognition. In FG 2018 . IEEE, 31–37.\\n[289] Alex Wilf, Qianli M Ma, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2022. Face-to-Face Contrastive\\nLearning for Social Intelligence Question-Answering. arXiv preprint arXiv:2208.01036 (2022).\\n[290] Paul L Williams and Randall D Beer. 2010. Nonnegative decomposition of multivariate information. arXiv preprint\\narXiv:1004.2515 (2010).\\n[291] Eric Wong, Shibani Santurkar, and Aleksander Madry. 2021. Leveraging sparse linear layers for debuggable deep\\nnetworks. In ICML .\\n[292] Mike Wu and Noah Goodman. 2018. Multimodal generative models for scalable weakly-supervised learning. NeurIPS\\n31 (2018).\\n[293] Nan Wu, Stanisław Jastrzębski, Kyunghyun Cho, and Krzysztof J Geras. 2022. Characterizing and overcoming the\\ngreedy nature of learning in multi-modal deep neural networks. arXiv preprint arXiv:2202.05306 (2022).\\n[294] Qi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2016. Ask me anything: Free-form\\nvisual question answering based on knowledge from external sources. In CVPR . 4622–4630.\\n[295] Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu, and Antonio M López. 2020. Multimodal end-to-end\\nautonomous driving. IEEE Transactions on Intelligent Transportation Systems (2020).\\n[296] Chen Xing, Negar Rostamzadeh, Boris Oreshkin, and Pedro O O. Pinheiro. 2019. Adaptive Cross-Modal Few-shot\\nLearning. In NeurIPS .\\n[297] Caiming Xiong, Stephen Merity, and Richard Socher. 2016. Dynamic memory networks for visual and textual question\\nanswering. In ICML .\\n[298] Chang Xu, Dacheng Tao, and Chao Xu. 2015. Multi-view intact space learning. IEEE TPAMI (2015).\\n[299] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\\nBengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In ICML . 2048–2057.\\n[300] Zhen Xu, David R So, and Andrew M Dai. 2021. MUFASA: Multimodal Fusion Architecture Search for Electronic\\nHealth Records. arXiv preprint arXiv:2102.02340 (2021).\\n[301] Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, et al .2021. MTAG: Modal-Temporal\\nAttention Graph for Unaligned Human Multimodal Language Sequences. In NAACL-HLT .\\n[302] Yang Yang, Ke-Tao Wang, De-Chuan Zhan, Hui Xiong, and Yuan Jiang. 2019. Comprehensive Semi-Supervised\\nMulti-Modal Learning.. In IJCAI .\\n[303] Ting Yao, Yingwei Pan, Yehao Li, and Tao Mei. 2018. Exploring visual relationship for image captioning. In ECCV .\\n[304] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B Tenenbaum. 2019.\\nClevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442 (2019).\\n[305] Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, Zhengyuan Yang, Jie Zhou, and Jiebo Luo. 2020. A Novel\\nGraph-based Multi-modal Fusion Encoder for Neural Machine Translation. In ACL. 3025–3035.\\n[306] M. H. Peter Young, Alice Lai, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New\\nsimilarity metrics for semantic inference over event descriptions. TACL 2 (2014), 67–68.\\n[307] Tiezheng Yu, Wenliang Dai, Zihan Liu, and Pascale Fung. 2021. Vision Guided Generative Pre-trained Language\\nModels for Multimodal Abstractive Summarization. In EMNLP . 3995–4007.\\n[308] Weijiang Yu, Jingwen Zhou, Weihao Yu, Xiaodan Liang, and Nong Xiao. 2019. Heterogeneous Graph Learning for\\nVisual Commonsense Reasoning. In NeurIPS .\\n[309] Jiahong Yuan, Mark Liberman, et al .2008. Speaker identification on the SCOTUS corpus. Journal of the Acoustical\\nSociety of America (2008).\\n[310] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2017. Tensor fusion network\\nfor multimodal sentiment analysis. arXiv preprint arXiv:1707.07250 (2017).\\n[311] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018.\\nMemory fusion network for multi-view sequential learning. In AAAI , Vol. 32.\\n[312] Amir Zadeh, Paul Pu Liang, and Louis-Philippe Morency. 2020. Foundations of multimodal co-learning. Information\\nFusion 64 (2020), 188–193.\\n[313] AmirAli Bagher Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2018. Multimodal\\nlanguage analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph. In ACL.\\n[314] Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese. 2018. Taskonomy:\\nDisentangling task transfer learning. In CVPR . 3712–3722.\\n[315] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From Recognition to Cognition: Visual Commonsense\\nReasoning. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) .\\n[316] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. 2021.\\nMerlot: Multimodal neural script knowledge models. NeurIPS 34 (2021).\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 34}), Document(page_content='1:36 Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency\\n[317] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, et al .2022. Socratic models:\\nComposing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598 (2022).\\n[318] Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan Carlos Niebles, and Min Sun. 2017.\\nLeveraging video descriptions to learn video question answering. In AAAI .\\n[319] Hao Zhang, Zhiting Hu, Yuntian Deng, Mrinmaya Sachan, Zhicheng Yan, and Eric Xing. 2016. Learning Concept\\nTaxonomies from Multi-modal Data. In ACL. 1791–1801.\\n[320] Weifeng Zhang, Jing Yu, Hua Hu, Haiyang Hu, and Zengchang Qin. 2020. Multimodal feature fusion by relational\\nreasoning and attention for visual question answering. Information Fusion 55 (2020), 116–126.\\n[321] Hao Zhu, Huaibo Huang, Yi Li, Aihua Zheng, and Ran He. 2021. Arbitrary talking face generation via attentional\\naudio-visual coherence learning. In IJCAI . 2362–2368.\\n[322] Xiangru Zhu, Zhixu Li, Xiaodan Wang, Xueyao Jiang, Penglei Sun, Xuwu Wang, et al .2022. Multi-Modal Knowledge\\nGraph Construction and Application: A Survey. arXiv preprint arXiv:2202.05786 (2022).\\n[323] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015.\\nAligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV .\\n[324] Yuke Zhu, Ce Zhang, Christopher Ré, and Li Fei-Fei. 2015. Building a large-scale multimodal knowledge base system\\nfor answering visual queries. arXiv preprint arXiv:1507.05670 (2015).\\n[325] Zachary M Ziegler, Luke Melas-Kyriazi, Sebastian Gehrmann, and Alexander M Rush. 2019. Encoder-agnostic\\nadaptation for conditional language generation. arXiv preprint arXiv:1908.06938 (2019).\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 35})]\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "T1L27WlvyJIu"
      },
      "outputs": [],
      "source": [
        "#Step 05: Split the Extracted Data into Text Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
        "\n",
        "text_chunks = text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Jq5TiVcyiu2",
        "outputId": "07ee381b-efa0-48c7-ab0c-c5826f81b9c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1o8aXL-Ryp9e",
        "outputId": "a6efe82d-f237-426d-85b0-8d9a76e8369d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Foundations & Trends in Multimodal Machine Learning 1:3\\n(2)Alignment (§4): How can we identify the connections and interactions between modality\\nelements? Alignment is challenging since it may depend on long-range dependencies, involves\\nambiguous segmentation (e.g., words or utterances), and could be either one-to-one, many-\\nto-many, or not exist at all. We cover (1) discrete alignment : identifying connections between\\ndiscrete elements across modalities, (2) continuous alignment : modeling alignment between con-\\ntinuous modality signals with ambiguous segmentation, and (3) contextualized representations :\\nlearning better representations by capturing cross-modal interactions between elements.\\n(3)Reasoning (§5) is defined as composing knowledge, usually through multiple inferential steps,\\nthat exploits the problem structure for a specific task. Reasoning involves (1) modeling the\\nstructure over which composition occurs, (2) the intermediate concepts in the composition\\nprocess, (3) understanding the inference paradigm of more abstract concepts, and (4) leveraging\\nlarge-scale external knowledge in the study of structure, concepts, and inference.\\n(4)Generation (§6) involves learning a generative process to produce raw modalities. We cat-\\negorize its subchallenges into (1) summarization : summarizing multimodal data to reduce\\ninformation content while highlighting the most salient parts of the input, (2) translation : trans-\\nlating from one modality to another and keeping information content while being consistent\\nwith cross-modal connections, and (3) creation : simultaneously generating multiple modalities\\nto increase information content while maintaining coherence within and across modalities.\\n(5)Transference (§7) aims to transfer knowledge between modalities, usually to help the target\\nmodality, which may be noisy or with limited resources. Transference is exemplified by (1)\\ncross-modal transfer : adapting models to tasks involving the primary modality, (2) co-learning :\\ntransferring information from secondary to primary modalities by sharing representation spaces\\nbetween both modalities, and (3) model induction : keeping individual unimodal models separate\\nbut transferring information across these models.\\n(6)Quantification (§8): The sixth and final challenge involves empirical and theoretical studies\\nto better understand (1) the dimensions of heterogeneity in multimodal datasets and how they\\nsubsequently influence modeling and learning, (2) the presence and type of modality connections\\nand interactions in multimodal datasets and captured by trained models, and (3) the learning\\nand optimization challenges involved with heterogeneous data.\\nFinally, we conclude this paper with a long-term perspective in multimodal learning by motivating\\nopen research questions identified by this taxonomy. This survey was also presented by the authors\\nin a visual medium through tutorials at CVPR 2022 and NAACL 2022, as well as courses 11-777\\nMultimodal Machine Learning and 11-877 Advanced Topics in Multimodal Machine Learning at\\nCMU. The reader is encouraged to check out these publicly available video recordings, additional\\nreading materials, and discussion probes motivating open research questions in multimodal learning.\\n2 FOUNDATIONAL PRINCIPLES IN MULTIMODAL RESEARCH\\nAmodality refers to a way in which a natural phenomenon is perceived or expressed. For example,\\nmodalities include speech and audio recorded through microphones, images and videos captured\\nvia cameras, and force and vibrations captured via haptic sensors. Modalities can be placed along\\na spectrum from rawtoabstract : raw modalities are those more closely detected from a sensor,\\nsuch as speech recordings from a microphone or images captured by a camera. Abstract modalities\\nare those farther away from sensors, such as language extracted from speech recordings, objects\\ndetected from images, or even abstract concepts like sentiment intensity and object categories.\\nMultimodal refers to situations where multiple modalities are involved. From a research perspec-\\ntive, multimodal entails the computational study of heterogeneous andinterconnected (connections\\n+ interactions) modalities. Firstly, modalities are heterogeneous because the information present in\\ndifferent modalities will often show diverse qualities, structures, and representations. Secondly,\\nPreprint, Vol. 1, No. 1, Article 1. Publication date: October 2022.', metadata={'source': '/content/sample_data/Data/review_multimodal_220903430.pdf', 'page': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "#get the third chunk\n",
        "text_chunks[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "deaf26428e444193b7835c16651c4e70",
            "d0c1d8aa3ed641e3af581f21aa81efcc",
            "232e91ef62de4a37947e972a12d94f50",
            "56bed2e8db694adbb90ca8802d9eb216",
            "5e8cca4021434ec28b2c8b39a2584636",
            "26901b2238224a2cafec245012530a45",
            "b9f1b7a727914d3f8ecb286b3914953b",
            "0f26dabc268a44b4a2cc3860d7d71726",
            "0016c1d8a08f48cf952c412fc452ef28",
            "bdcbe2cc4f6b4c1a8fc8a42f0dc5bcb7",
            "90eb90a4390347f99db4a0d728b9b2e6",
            "a4c42b8c80e7475d95bf4b5a03c094ce",
            "92c3d2fec37f4d1daa7089213e7a188e",
            "981a955f2ca847e2b47f2282d7473f36",
            "4e2b93cc97fd4687bb108efa61cee907",
            "1fe34ee4686c41f8a20298caa23188d5",
            "b3c4893ff3b44cb585816eb8ebe50670",
            "d672e0050aa54697bfb3f8f353d5f2f3",
            "c3730b45a9024ba3b8cf5db7f4342d21",
            "440cbeeccb5340969b5068e93c8e97c1",
            "d58c40acaf094700a9e77728d56eb053",
            "e54a44ce8c554e78b43f1fa61863cd30",
            "140759cf68404190ba306000b50560c7",
            "d9b3fa569f5649839b137d4cea336ad7",
            "3bc6848de3cc422b9ca357b7b7cf3d56",
            "b564f02482264ab1bf3d5d71ae655265",
            "11093772bdb4423b89778db8dd7f7f09",
            "84d84e5d7a044d228e4d0118efd01312",
            "62788c5d59134b43a5e384c978c7bec2",
            "f3ec0be1a85f421b9d8a27b85d0220e6",
            "1c2b8038fe2541aabe7d04f785214e48",
            "404278207ac64a8583b732fb8a29e0b6",
            "436bbd1aa07c44558a4c32c1c67d09e6",
            "df778d749080427380b2516f5f598361",
            "0546c532d47943cfaee773100b7cdd93",
            "f51fcc0242db4a4d998c2cc45cb6c5f9",
            "76f3494df8944926a1ddb80e77b12b20",
            "9ab20e3df972476a87c9ac4513e5c1f8",
            "e387278fbf094a69a380797cc321be26",
            "85bfc0cdf5d4458eb27682f109212ffa",
            "378ae9eccc7440b887a5ee3eb138e3f0",
            "66cf4d80fe314379802147aaef9e1d9c",
            "a9e78e9d486b43f7a120558f15d73966",
            "063e4c0e131142379320e61a9e200eef",
            "e43eddde034c4c20a81624dfeb6c77b1",
            "9af27fce929f48ffb1b4476e68402882",
            "5a4061e7f08b4a548024e5cc038a128d",
            "bd6be24bdfd9418bbc6ede5adc3d9d22",
            "6db740c953fc456b9ee0400e4ceef280",
            "96443006b82b4478afbdef57be6418ac",
            "031941c566b44da88542c23c467c341b",
            "153f800c208d45ff93e8aadc90e3fd5d",
            "d9cc8e1f6222423099ce8a54658797d7",
            "95e1427f500d40c49283d1cf07c46461",
            "4db91d4d39df4cee9975cea628ba0035",
            "75355e9d461340aaada41e1bce9c3a2e",
            "a822091b0db4482088fa924a1bd0e64d",
            "cb1b35ae370a46448883c4a8da30ccfa",
            "38751ae8352e41278baa8bf9b7aaf2ed",
            "93fe8a06dde24fa396c3602ac3783563",
            "8c5dafc6844a41f886d6d0e933e544b7",
            "46f0a12b7e3d401c8492a2e364c2781a",
            "36457ae2e08547ba8afab1a3da3303c6",
            "706591eac6c648e5857776d2b1d9a923",
            "17b5d926131d46c5b0d8744955eba6a3",
            "f7f417a4001f42ac9b1a045c9969e819",
            "7d48f406e4d8472f871c4fd94b9bed1b",
            "ecc116b96f034a30a18b09fa4a184e01",
            "8742076bf00e4e3ebb33622dd3e532cd",
            "5d3c52d7949c4b799efa21363ac03d13",
            "7b05e6d378cd4daa8748bcbc8972f312",
            "32a4e24a201b4b508f6040042821a579",
            "f55e4d405fa14a81ab4dd1373ec34683",
            "e01570dcb7ce40acad61027d1e81a0f5",
            "10bb984163b141a68048c5b6e3e8d82f",
            "2cebc591e217483f96d0bd6ab213ee64",
            "962f5f56f22c41c499a91f451ac12d92",
            "80f0fbbe55bd49379ef51cbbf318ad6c",
            "78a83553a5cc461e9a90179311e3f638",
            "b57327b44a4d45fc8da0cf0f3eb7e121",
            "055c92df8bc5457cb4dcab5a379ce7cc",
            "20436a33d03a4d7a87ba33536424352d",
            "4a211f068c5740548a441a83c478642b",
            "9e1b7303cf484208a93d76f8dc1c1216",
            "66897db385754a1085ebc7edd82ff847",
            "e5c680dbd6224939b10dcf66049aae22",
            "d08db9e363844827b9bb598a2cdf98ed",
            "f7d1628de0f842dbbda4666318d5a3d2",
            "31972df429bf440b97709cc0a93ba132",
            "1fbc182409a842ba924387fea2710346",
            "1052254e62d94e24af6ce6217e2ecd7d",
            "7cc0d4f0dfd1462b9e75abde4ca63927",
            "ad8ff6ba940e4986805b9852eff45bf0",
            "e83e31f163e74df3917469738cf98b51",
            "4ff14cefa8b64e559faf5b3919748143",
            "4601ca50a61c42dbab91034a3b19a3aa",
            "f8c2a7208205404db5f2fbe009f32abc",
            "f06b0afd18c64605b3eb1d2893c57567",
            "d0258853a7ad42549b74d0f29979130a",
            "fa4eb01f472a46a9b1d836fa6435aee2",
            "31128ea22fc5463a9453d847d97aaf07",
            "863dd478edef4c9890efe916fa071069",
            "e7a361da10e04a58983f12a25d77c66c",
            "26fac1793b5a4f4dbf8e92f1949fe980",
            "6238a769894a47f6a12aac03911248f1",
            "4332ef4dd21946ed925375338062ccb7",
            "4bba06c36299451e98d590595312c4ca",
            "9d44da566718475f881c875af76468f6",
            "ed16ff08b0b1431a96de1cca0448b78e",
            "e791dfe037cf4b26b61a6d958c3af68a",
            "29caa2db95f5484592a52199f49ed3ba",
            "76479a52595248108c9acfba0b1d9a6c",
            "b5ae63d084e246509a50f0dc9180b1d9",
            "c07837c55ab7438b97776580f34af772",
            "c5661fe9cff64bd8bf1de8227de6c680",
            "90b06c999f284d35bfed66784d18953a",
            "25ce552599fc47109b8908dabdc14730",
            "007737ed9f784e559029eb2911a585fb",
            "dfeba16a3db04486826177a426e7961a",
            "332907903b504415921a11d4077961c5",
            "a42fe1975dc44c429fcaf5753f57cc0f",
            "9549d1092e9c49f5900c27efef733772",
            "6a9df9855f794068a1a10f437c6e28eb",
            "9854758908c74005bc1050197559a418",
            "41fb99a287df4fc58b3ab6eefa831956",
            "e01b8a99bb224f0eaaa854ac99d6b0ce",
            "301373a1f3454a0daff1b835f89b2c5d",
            "3d60fb3d8a294db2ab34a4bae451b331",
            "086bbb147d734111ab800815768ee83c",
            "dd1e714a763045ee99d4e7b5780d800c",
            "40d52a5386c542c4b50dc66a680597bd",
            "4bc9715c955a4681a137c93e041e3ae1",
            "c7398be318ad447a9e1193a6d363644b",
            "632a914aee5c4b758d1980d1ef534388",
            "f014a87ab88c424daf23bf58babbc019",
            "9a25900958af4924bd804bd2f9865dd2",
            "d00a803deca84098b3b66e79c974255c",
            "4ad89437cdaa48259f7a6214e3633fbe",
            "513b811f83b64b65b3338bd65c1cfb46",
            "d7d170c6de2f411299c23868a288f96e",
            "dfd813be7408402f9148e0503a3f8bd6",
            "a8e5fd18e2b74eabb02e1145a60ecc40",
            "d625a8aef3a14ff1a89cf72252a8fa1b",
            "a16204ac38cc43cbbe4337e5c5faa0f2",
            "0d0a550e7d13491887352f45fc79fc10",
            "3587cf9b660f4f10a488a1ef9d2b94d9",
            "20cc15c7aae549c3ad98e434a144bc48",
            "1de426c9229745979d0b3e2acfc2d87d",
            "2f1fcb9ba5de4e2ba002223e3d1dc2a8",
            "02d5896b580e4d738a59c1ab918f067c",
            "e24edcd4b9ca4c6a9afbdd507fa0d0ba",
            "4ecf7e9dcfc04bc595cedd78ff88d7df",
            "dd07c7acbca44ae483331638ebf041a8",
            "84f9308f674046c3afbadc5142e9a297"
          ]
        },
        "id": "M7oKKdTPyxax",
        "outputId": "218dfd1b-f1fe-4605-adfc-578f1a1010ca"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "deaf26428e444193b7835c16651c4e70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a4c42b8c80e7475d95bf4b5a03c094ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "140759cf68404190ba306000b50560c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df778d749080427380b2516f5f598361"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e43eddde034c4c20a81624dfeb6c77b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "75355e9d461340aaada41e1bce9c3a2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d48f406e4d8472f871c4fd94b9bed1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80f0fbbe55bd49379ef51cbbf318ad6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31972df429bf440b97709cc0a93ba132"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa4eb01f472a46a9b1d836fa6435aee2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29caa2db95f5484592a52199f49ed3ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9549d1092e9c49f5900c27efef733772"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7398be318ad447a9e1193a6d363644b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a16204ac38cc43cbbe4337e5c5faa0f2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Step 06:Downlaod the Embeddings\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MJbn_hjzznhm"
      },
      "outputs": [],
      "source": [
        "#Step 08: Create Embeddings for each of the Text Chunk\n",
        "vector_store = FAISS.from_documents(text_chunks, embedding=embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "hf_hub_download(repo_id=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", filename=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "4415baacc246488f98db0ebc25837ff5",
            "8fcfa0bf67474a3fbc396385a2e89651",
            "5ba7923642614fec8ac8c240a80db671",
            "d15ac69f523b4e37af387513571e28e0",
            "27890ecfed7343d597f0a09da4ac6771",
            "6ecd1f9128d044c88bd99c9b15e88b23",
            "334292415615430c8c7638ed5085fc84",
            "9e94d79f85d44cc984e9943789c20546",
            "4aec83729dab4259a5c7bcef90592172",
            "46d24745434a4c78a56b9be2923d4984",
            "1ee622b54ce340029b0447b6afb08742"
          ]
        },
        "id": "AQQT8jcbSLmq",
        "outputId": "97ef66bd-7c3b-45ef-b732-935875e3e5c2"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)uct-v0.1.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4415baacc246488f98db0ebc25837ff5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/45167a542b6fa64a14aea61a4c468bbbf9f258a8/mistral-7b-instruct-v0.1.Q4_K_M.gguf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHFvAdTfSWC2",
        "outputId": "eb5c3f78-bbf3-4576-84f4-3f31934b2e60"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9Y5Va538RDL",
        "outputId": "61f98630-03a1-41a5-fc39-104ee3904113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "#Import Model\n",
        "llm = LlamaCpp(\n",
        "    streaming = True,\n",
        "    model_path=\"/root/.cache/huggingface/hub/models--TheBloke--Mistral-7B-Instruct-v0.1-GGUF/snapshots/45167a542b6fa64a14aea61a4c468bbbf9f258a8/mistral-7b-instruct-v0.1.Q4_K_M.gguf\",\n",
        "    temperature=0.75,\n",
        "    top_p=1,\n",
        "    verbose=True,\n",
        "    n_ctx=4096\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4Vpb_w-i94Th"
      },
      "outputs": [],
      "source": [
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vector_store.as_retriever(search_kwargs={\"k\": 2}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "8DWiBe3cENZB"
      },
      "outputs": [],
      "source": [
        "# query = \"What is linear regression model\"\n",
        "query = \"What is the difference between CLIP and FLamingo?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "mdFYMQfR_gzI",
        "outputId": "fa2bf82f-bbb4-446c-9178-8011d9d8929d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' CLIP (Concept Labelled Image Priors) and FLamingo are both large language models, but they differ in the tasks they were trained on and the way they interact with their corresponding image embeddings. CLIP was primarily trained on image captioning and task-specific labels, while FLamingo was primarily trained on generative modeling of images and can be used for tasks such as style transfer and denoising. In terms of interaction with image embeddings, CLIP generates text descriptions of images, while FLamingo allows users to generate new images by interacting with the pre-trained language model.\\nReference(s):\\nhttps://arxiv.org/abs/2103.04566\\nhttps://arxiv.org/abs/2012.07897'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "qa.run(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl_4JdUatnTQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "while True:\n",
        "  user_input = input(f\"Input Prompt: \")\n",
        "  if user_input == 'exit':\n",
        "    print('Exiting')\n",
        "    sys.exit()\n",
        "  if user_input == '':\n",
        "    continue\n",
        "  result = qa({'query': user_input})\n",
        "  print(f\"Answer: {result['result']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "deaf26428e444193b7835c16651c4e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0c1d8aa3ed641e3af581f21aa81efcc",
              "IPY_MODEL_232e91ef62de4a37947e972a12d94f50",
              "IPY_MODEL_56bed2e8db694adbb90ca8802d9eb216"
            ],
            "layout": "IPY_MODEL_5e8cca4021434ec28b2c8b39a2584636"
          }
        },
        "d0c1d8aa3ed641e3af581f21aa81efcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26901b2238224a2cafec245012530a45",
            "placeholder": "​",
            "style": "IPY_MODEL_b9f1b7a727914d3f8ecb286b3914953b",
            "value": "Downloading (…)e9125/.gitattributes: 100%"
          }
        },
        "232e91ef62de4a37947e972a12d94f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f26dabc268a44b4a2cc3860d7d71726",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0016c1d8a08f48cf952c412fc452ef28",
            "value": 1175
          }
        },
        "56bed2e8db694adbb90ca8802d9eb216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdcbe2cc4f6b4c1a8fc8a42f0dc5bcb7",
            "placeholder": "​",
            "style": "IPY_MODEL_90eb90a4390347f99db4a0d728b9b2e6",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 76.4kB/s]"
          }
        },
        "5e8cca4021434ec28b2c8b39a2584636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26901b2238224a2cafec245012530a45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9f1b7a727914d3f8ecb286b3914953b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f26dabc268a44b4a2cc3860d7d71726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0016c1d8a08f48cf952c412fc452ef28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bdcbe2cc4f6b4c1a8fc8a42f0dc5bcb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90eb90a4390347f99db4a0d728b9b2e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4c42b8c80e7475d95bf4b5a03c094ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92c3d2fec37f4d1daa7089213e7a188e",
              "IPY_MODEL_981a955f2ca847e2b47f2282d7473f36",
              "IPY_MODEL_4e2b93cc97fd4687bb108efa61cee907"
            ],
            "layout": "IPY_MODEL_1fe34ee4686c41f8a20298caa23188d5"
          }
        },
        "92c3d2fec37f4d1daa7089213e7a188e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3c4893ff3b44cb585816eb8ebe50670",
            "placeholder": "​",
            "style": "IPY_MODEL_d672e0050aa54697bfb3f8f353d5f2f3",
            "value": "Downloading (…)_Pooling/config.json: 100%"
          }
        },
        "981a955f2ca847e2b47f2282d7473f36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3730b45a9024ba3b8cf5db7f4342d21",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_440cbeeccb5340969b5068e93c8e97c1",
            "value": 190
          }
        },
        "4e2b93cc97fd4687bb108efa61cee907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d58c40acaf094700a9e77728d56eb053",
            "placeholder": "​",
            "style": "IPY_MODEL_e54a44ce8c554e78b43f1fa61863cd30",
            "value": " 190/190 [00:00&lt;00:00, 13.2kB/s]"
          }
        },
        "1fe34ee4686c41f8a20298caa23188d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3c4893ff3b44cb585816eb8ebe50670": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d672e0050aa54697bfb3f8f353d5f2f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3730b45a9024ba3b8cf5db7f4342d21": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "440cbeeccb5340969b5068e93c8e97c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d58c40acaf094700a9e77728d56eb053": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e54a44ce8c554e78b43f1fa61863cd30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "140759cf68404190ba306000b50560c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9b3fa569f5649839b137d4cea336ad7",
              "IPY_MODEL_3bc6848de3cc422b9ca357b7b7cf3d56",
              "IPY_MODEL_b564f02482264ab1bf3d5d71ae655265"
            ],
            "layout": "IPY_MODEL_11093772bdb4423b89778db8dd7f7f09"
          }
        },
        "d9b3fa569f5649839b137d4cea336ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84d84e5d7a044d228e4d0118efd01312",
            "placeholder": "​",
            "style": "IPY_MODEL_62788c5d59134b43a5e384c978c7bec2",
            "value": "Downloading (…)7e55de9125/README.md: 100%"
          }
        },
        "3bc6848de3cc422b9ca357b7b7cf3d56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3ec0be1a85f421b9d8a27b85d0220e6",
            "max": 10610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c2b8038fe2541aabe7d04f785214e48",
            "value": 10610
          }
        },
        "b564f02482264ab1bf3d5d71ae655265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_404278207ac64a8583b732fb8a29e0b6",
            "placeholder": "​",
            "style": "IPY_MODEL_436bbd1aa07c44558a4c32c1c67d09e6",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 435kB/s]"
          }
        },
        "11093772bdb4423b89778db8dd7f7f09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84d84e5d7a044d228e4d0118efd01312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62788c5d59134b43a5e384c978c7bec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3ec0be1a85f421b9d8a27b85d0220e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c2b8038fe2541aabe7d04f785214e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "404278207ac64a8583b732fb8a29e0b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "436bbd1aa07c44558a4c32c1c67d09e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df778d749080427380b2516f5f598361": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0546c532d47943cfaee773100b7cdd93",
              "IPY_MODEL_f51fcc0242db4a4d998c2cc45cb6c5f9",
              "IPY_MODEL_76f3494df8944926a1ddb80e77b12b20"
            ],
            "layout": "IPY_MODEL_9ab20e3df972476a87c9ac4513e5c1f8"
          }
        },
        "0546c532d47943cfaee773100b7cdd93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e387278fbf094a69a380797cc321be26",
            "placeholder": "​",
            "style": "IPY_MODEL_85bfc0cdf5d4458eb27682f109212ffa",
            "value": "Downloading (…)55de9125/config.json: 100%"
          }
        },
        "f51fcc0242db4a4d998c2cc45cb6c5f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_378ae9eccc7440b887a5ee3eb138e3f0",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66cf4d80fe314379802147aaef9e1d9c",
            "value": 612
          }
        },
        "76f3494df8944926a1ddb80e77b12b20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9e78e9d486b43f7a120558f15d73966",
            "placeholder": "​",
            "style": "IPY_MODEL_063e4c0e131142379320e61a9e200eef",
            "value": " 612/612 [00:00&lt;00:00, 36.2kB/s]"
          }
        },
        "9ab20e3df972476a87c9ac4513e5c1f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e387278fbf094a69a380797cc321be26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85bfc0cdf5d4458eb27682f109212ffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "378ae9eccc7440b887a5ee3eb138e3f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66cf4d80fe314379802147aaef9e1d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a9e78e9d486b43f7a120558f15d73966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "063e4c0e131142379320e61a9e200eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e43eddde034c4c20a81624dfeb6c77b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9af27fce929f48ffb1b4476e68402882",
              "IPY_MODEL_5a4061e7f08b4a548024e5cc038a128d",
              "IPY_MODEL_bd6be24bdfd9418bbc6ede5adc3d9d22"
            ],
            "layout": "IPY_MODEL_6db740c953fc456b9ee0400e4ceef280"
          }
        },
        "9af27fce929f48ffb1b4476e68402882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96443006b82b4478afbdef57be6418ac",
            "placeholder": "​",
            "style": "IPY_MODEL_031941c566b44da88542c23c467c341b",
            "value": "Downloading (…)ce_transformers.json: 100%"
          }
        },
        "5a4061e7f08b4a548024e5cc038a128d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_153f800c208d45ff93e8aadc90e3fd5d",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9cc8e1f6222423099ce8a54658797d7",
            "value": 116
          }
        },
        "bd6be24bdfd9418bbc6ede5adc3d9d22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95e1427f500d40c49283d1cf07c46461",
            "placeholder": "​",
            "style": "IPY_MODEL_4db91d4d39df4cee9975cea628ba0035",
            "value": " 116/116 [00:00&lt;00:00, 7.81kB/s]"
          }
        },
        "6db740c953fc456b9ee0400e4ceef280": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96443006b82b4478afbdef57be6418ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "031941c566b44da88542c23c467c341b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "153f800c208d45ff93e8aadc90e3fd5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9cc8e1f6222423099ce8a54658797d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "95e1427f500d40c49283d1cf07c46461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4db91d4d39df4cee9975cea628ba0035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "75355e9d461340aaada41e1bce9c3a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a822091b0db4482088fa924a1bd0e64d",
              "IPY_MODEL_cb1b35ae370a46448883c4a8da30ccfa",
              "IPY_MODEL_38751ae8352e41278baa8bf9b7aaf2ed"
            ],
            "layout": "IPY_MODEL_93fe8a06dde24fa396c3602ac3783563"
          }
        },
        "a822091b0db4482088fa924a1bd0e64d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c5dafc6844a41f886d6d0e933e544b7",
            "placeholder": "​",
            "style": "IPY_MODEL_46f0a12b7e3d401c8492a2e364c2781a",
            "value": "Downloading (…)125/data_config.json: 100%"
          }
        },
        "cb1b35ae370a46448883c4a8da30ccfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36457ae2e08547ba8afab1a3da3303c6",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_706591eac6c648e5857776d2b1d9a923",
            "value": 39265
          }
        },
        "38751ae8352e41278baa8bf9b7aaf2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17b5d926131d46c5b0d8744955eba6a3",
            "placeholder": "​",
            "style": "IPY_MODEL_f7f417a4001f42ac9b1a045c9969e819",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 2.32MB/s]"
          }
        },
        "93fe8a06dde24fa396c3602ac3783563": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c5dafc6844a41f886d6d0e933e544b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46f0a12b7e3d401c8492a2e364c2781a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36457ae2e08547ba8afab1a3da3303c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "706591eac6c648e5857776d2b1d9a923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17b5d926131d46c5b0d8744955eba6a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f417a4001f42ac9b1a045c9969e819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7d48f406e4d8472f871c4fd94b9bed1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ecc116b96f034a30a18b09fa4a184e01",
              "IPY_MODEL_8742076bf00e4e3ebb33622dd3e532cd",
              "IPY_MODEL_5d3c52d7949c4b799efa21363ac03d13"
            ],
            "layout": "IPY_MODEL_7b05e6d378cd4daa8748bcbc8972f312"
          }
        },
        "ecc116b96f034a30a18b09fa4a184e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32a4e24a201b4b508f6040042821a579",
            "placeholder": "​",
            "style": "IPY_MODEL_f55e4d405fa14a81ab4dd1373ec34683",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "8742076bf00e4e3ebb33622dd3e532cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e01570dcb7ce40acad61027d1e81a0f5",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_10bb984163b141a68048c5b6e3e8d82f",
            "value": 90888945
          }
        },
        "5d3c52d7949c4b799efa21363ac03d13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cebc591e217483f96d0bd6ab213ee64",
            "placeholder": "​",
            "style": "IPY_MODEL_962f5f56f22c41c499a91f451ac12d92",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 235MB/s]"
          }
        },
        "7b05e6d378cd4daa8748bcbc8972f312": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32a4e24a201b4b508f6040042821a579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f55e4d405fa14a81ab4dd1373ec34683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e01570dcb7ce40acad61027d1e81a0f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10bb984163b141a68048c5b6e3e8d82f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cebc591e217483f96d0bd6ab213ee64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "962f5f56f22c41c499a91f451ac12d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80f0fbbe55bd49379ef51cbbf318ad6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78a83553a5cc461e9a90179311e3f638",
              "IPY_MODEL_b57327b44a4d45fc8da0cf0f3eb7e121",
              "IPY_MODEL_055c92df8bc5457cb4dcab5a379ce7cc"
            ],
            "layout": "IPY_MODEL_20436a33d03a4d7a87ba33536424352d"
          }
        },
        "78a83553a5cc461e9a90179311e3f638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a211f068c5740548a441a83c478642b",
            "placeholder": "​",
            "style": "IPY_MODEL_9e1b7303cf484208a93d76f8dc1c1216",
            "value": "Downloading (…)nce_bert_config.json: 100%"
          }
        },
        "b57327b44a4d45fc8da0cf0f3eb7e121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66897db385754a1085ebc7edd82ff847",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e5c680dbd6224939b10dcf66049aae22",
            "value": 53
          }
        },
        "055c92df8bc5457cb4dcab5a379ce7cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d08db9e363844827b9bb598a2cdf98ed",
            "placeholder": "​",
            "style": "IPY_MODEL_f7d1628de0f842dbbda4666318d5a3d2",
            "value": " 53.0/53.0 [00:00&lt;00:00, 3.51kB/s]"
          }
        },
        "20436a33d03a4d7a87ba33536424352d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a211f068c5740548a441a83c478642b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e1b7303cf484208a93d76f8dc1c1216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66897db385754a1085ebc7edd82ff847": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c680dbd6224939b10dcf66049aae22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d08db9e363844827b9bb598a2cdf98ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7d1628de0f842dbbda4666318d5a3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31972df429bf440b97709cc0a93ba132": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fbc182409a842ba924387fea2710346",
              "IPY_MODEL_1052254e62d94e24af6ce6217e2ecd7d",
              "IPY_MODEL_7cc0d4f0dfd1462b9e75abde4ca63927"
            ],
            "layout": "IPY_MODEL_ad8ff6ba940e4986805b9852eff45bf0"
          }
        },
        "1fbc182409a842ba924387fea2710346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e83e31f163e74df3917469738cf98b51",
            "placeholder": "​",
            "style": "IPY_MODEL_4ff14cefa8b64e559faf5b3919748143",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "1052254e62d94e24af6ce6217e2ecd7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4601ca50a61c42dbab91034a3b19a3aa",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8c2a7208205404db5f2fbe009f32abc",
            "value": 112
          }
        },
        "7cc0d4f0dfd1462b9e75abde4ca63927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f06b0afd18c64605b3eb1d2893c57567",
            "placeholder": "​",
            "style": "IPY_MODEL_d0258853a7ad42549b74d0f29979130a",
            "value": " 112/112 [00:00&lt;00:00, 7.88kB/s]"
          }
        },
        "ad8ff6ba940e4986805b9852eff45bf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e83e31f163e74df3917469738cf98b51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ff14cefa8b64e559faf5b3919748143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4601ca50a61c42dbab91034a3b19a3aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8c2a7208205404db5f2fbe009f32abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f06b0afd18c64605b3eb1d2893c57567": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0258853a7ad42549b74d0f29979130a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa4eb01f472a46a9b1d836fa6435aee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31128ea22fc5463a9453d847d97aaf07",
              "IPY_MODEL_863dd478edef4c9890efe916fa071069",
              "IPY_MODEL_e7a361da10e04a58983f12a25d77c66c"
            ],
            "layout": "IPY_MODEL_26fac1793b5a4f4dbf8e92f1949fe980"
          }
        },
        "31128ea22fc5463a9453d847d97aaf07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6238a769894a47f6a12aac03911248f1",
            "placeholder": "​",
            "style": "IPY_MODEL_4332ef4dd21946ed925375338062ccb7",
            "value": "Downloading (…)e9125/tokenizer.json: 100%"
          }
        },
        "863dd478edef4c9890efe916fa071069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bba06c36299451e98d590595312c4ca",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d44da566718475f881c875af76468f6",
            "value": 466247
          }
        },
        "e7a361da10e04a58983f12a25d77c66c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed16ff08b0b1431a96de1cca0448b78e",
            "placeholder": "​",
            "style": "IPY_MODEL_e791dfe037cf4b26b61a6d958c3af68a",
            "value": " 466k/466k [00:00&lt;00:00, 1.89MB/s]"
          }
        },
        "26fac1793b5a4f4dbf8e92f1949fe980": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6238a769894a47f6a12aac03911248f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4332ef4dd21946ed925375338062ccb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bba06c36299451e98d590595312c4ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d44da566718475f881c875af76468f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ed16ff08b0b1431a96de1cca0448b78e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e791dfe037cf4b26b61a6d958c3af68a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29caa2db95f5484592a52199f49ed3ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76479a52595248108c9acfba0b1d9a6c",
              "IPY_MODEL_b5ae63d084e246509a50f0dc9180b1d9",
              "IPY_MODEL_c07837c55ab7438b97776580f34af772"
            ],
            "layout": "IPY_MODEL_c5661fe9cff64bd8bf1de8227de6c680"
          }
        },
        "76479a52595248108c9acfba0b1d9a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90b06c999f284d35bfed66784d18953a",
            "placeholder": "​",
            "style": "IPY_MODEL_25ce552599fc47109b8908dabdc14730",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "b5ae63d084e246509a50f0dc9180b1d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_007737ed9f784e559029eb2911a585fb",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfeba16a3db04486826177a426e7961a",
            "value": 350
          }
        },
        "c07837c55ab7438b97776580f34af772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_332907903b504415921a11d4077961c5",
            "placeholder": "​",
            "style": "IPY_MODEL_a42fe1975dc44c429fcaf5753f57cc0f",
            "value": " 350/350 [00:00&lt;00:00, 21.9kB/s]"
          }
        },
        "c5661fe9cff64bd8bf1de8227de6c680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b06c999f284d35bfed66784d18953a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25ce552599fc47109b8908dabdc14730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "007737ed9f784e559029eb2911a585fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfeba16a3db04486826177a426e7961a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "332907903b504415921a11d4077961c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a42fe1975dc44c429fcaf5753f57cc0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9549d1092e9c49f5900c27efef733772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6a9df9855f794068a1a10f437c6e28eb",
              "IPY_MODEL_9854758908c74005bc1050197559a418",
              "IPY_MODEL_41fb99a287df4fc58b3ab6eefa831956"
            ],
            "layout": "IPY_MODEL_e01b8a99bb224f0eaaa854ac99d6b0ce"
          }
        },
        "6a9df9855f794068a1a10f437c6e28eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_301373a1f3454a0daff1b835f89b2c5d",
            "placeholder": "​",
            "style": "IPY_MODEL_3d60fb3d8a294db2ab34a4bae451b331",
            "value": "Downloading (…)9125/train_script.py: 100%"
          }
        },
        "9854758908c74005bc1050197559a418": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_086bbb147d734111ab800815768ee83c",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd1e714a763045ee99d4e7b5780d800c",
            "value": 13156
          }
        },
        "41fb99a287df4fc58b3ab6eefa831956": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d52a5386c542c4b50dc66a680597bd",
            "placeholder": "​",
            "style": "IPY_MODEL_4bc9715c955a4681a137c93e041e3ae1",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 492kB/s]"
          }
        },
        "e01b8a99bb224f0eaaa854ac99d6b0ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301373a1f3454a0daff1b835f89b2c5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d60fb3d8a294db2ab34a4bae451b331": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "086bbb147d734111ab800815768ee83c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd1e714a763045ee99d4e7b5780d800c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40d52a5386c542c4b50dc66a680597bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bc9715c955a4681a137c93e041e3ae1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7398be318ad447a9e1193a6d363644b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_632a914aee5c4b758d1980d1ef534388",
              "IPY_MODEL_f014a87ab88c424daf23bf58babbc019",
              "IPY_MODEL_9a25900958af4924bd804bd2f9865dd2"
            ],
            "layout": "IPY_MODEL_d00a803deca84098b3b66e79c974255c"
          }
        },
        "632a914aee5c4b758d1980d1ef534388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ad89437cdaa48259f7a6214e3633fbe",
            "placeholder": "​",
            "style": "IPY_MODEL_513b811f83b64b65b3338bd65c1cfb46",
            "value": "Downloading (…)7e55de9125/vocab.txt: 100%"
          }
        },
        "f014a87ab88c424daf23bf58babbc019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7d170c6de2f411299c23868a288f96e",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfd813be7408402f9148e0503a3f8bd6",
            "value": 231508
          }
        },
        "9a25900958af4924bd804bd2f9865dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8e5fd18e2b74eabb02e1145a60ecc40",
            "placeholder": "​",
            "style": "IPY_MODEL_d625a8aef3a14ff1a89cf72252a8fa1b",
            "value": " 232k/232k [00:00&lt;00:00, 9.58MB/s]"
          }
        },
        "d00a803deca84098b3b66e79c974255c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad89437cdaa48259f7a6214e3633fbe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "513b811f83b64b65b3338bd65c1cfb46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7d170c6de2f411299c23868a288f96e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfd813be7408402f9148e0503a3f8bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a8e5fd18e2b74eabb02e1145a60ecc40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d625a8aef3a14ff1a89cf72252a8fa1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a16204ac38cc43cbbe4337e5c5faa0f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d0a550e7d13491887352f45fc79fc10",
              "IPY_MODEL_3587cf9b660f4f10a488a1ef9d2b94d9",
              "IPY_MODEL_20cc15c7aae549c3ad98e434a144bc48"
            ],
            "layout": "IPY_MODEL_1de426c9229745979d0b3e2acfc2d87d"
          }
        },
        "0d0a550e7d13491887352f45fc79fc10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f1fcb9ba5de4e2ba002223e3d1dc2a8",
            "placeholder": "​",
            "style": "IPY_MODEL_02d5896b580e4d738a59c1ab918f067c",
            "value": "Downloading (…)5de9125/modules.json: 100%"
          }
        },
        "3587cf9b660f4f10a488a1ef9d2b94d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e24edcd4b9ca4c6a9afbdd507fa0d0ba",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ecf7e9dcfc04bc595cedd78ff88d7df",
            "value": 349
          }
        },
        "20cc15c7aae549c3ad98e434a144bc48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd07c7acbca44ae483331638ebf041a8",
            "placeholder": "​",
            "style": "IPY_MODEL_84f9308f674046c3afbadc5142e9a297",
            "value": " 349/349 [00:00&lt;00:00, 13.2kB/s]"
          }
        },
        "1de426c9229745979d0b3e2acfc2d87d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f1fcb9ba5de4e2ba002223e3d1dc2a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02d5896b580e4d738a59c1ab918f067c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e24edcd4b9ca4c6a9afbdd507fa0d0ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ecf7e9dcfc04bc595cedd78ff88d7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd07c7acbca44ae483331638ebf041a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84f9308f674046c3afbadc5142e9a297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4415baacc246488f98db0ebc25837ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fcfa0bf67474a3fbc396385a2e89651",
              "IPY_MODEL_5ba7923642614fec8ac8c240a80db671",
              "IPY_MODEL_d15ac69f523b4e37af387513571e28e0"
            ],
            "layout": "IPY_MODEL_27890ecfed7343d597f0a09da4ac6771"
          }
        },
        "8fcfa0bf67474a3fbc396385a2e89651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ecd1f9128d044c88bd99c9b15e88b23",
            "placeholder": "​",
            "style": "IPY_MODEL_334292415615430c8c7638ed5085fc84",
            "value": "Downloading (…)uct-v0.1.Q4_K_M.gguf: 100%"
          }
        },
        "5ba7923642614fec8ac8c240a80db671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9e94d79f85d44cc984e9943789c20546",
            "max": 4368438944,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4aec83729dab4259a5c7bcef90592172",
            "value": 4368438944
          }
        },
        "d15ac69f523b4e37af387513571e28e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46d24745434a4c78a56b9be2923d4984",
            "placeholder": "​",
            "style": "IPY_MODEL_1ee622b54ce340029b0447b6afb08742",
            "value": " 4.37G/4.37G [00:33&lt;00:00, 53.5MB/s]"
          }
        },
        "27890ecfed7343d597f0a09da4ac6771": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ecd1f9128d044c88bd99c9b15e88b23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "334292415615430c8c7638ed5085fc84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9e94d79f85d44cc984e9943789c20546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aec83729dab4259a5c7bcef90592172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46d24745434a4c78a56b9be2923d4984": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ee622b54ce340029b0447b6afb08742": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}